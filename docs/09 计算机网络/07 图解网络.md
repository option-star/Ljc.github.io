---
title: 计网07:图解网络
date: 2021-12-15
sidebar: 'auto'
categories:
- 计算机网络
isShowComments: true
---



## 一、TCP/IP网络模型

​	对于同⼀台设备上的进程间通信，有很多种⽅式，⽐如有管道、消息队列、共享内存、信号等⽅式，⽽对于不同设备上的进程间通信，就需要⽹络通信，⽽设备是多样性的，所以要兼容多种多样的设备，就协商出了⼀套 **通用的网络协议**。这个⽹络协议是分层的，每⼀层都有各⾃的作⽤和职责，接下来就分别对每⼀层进⾏介绍。

### 1.1 应用层

​	最上层的，也是我们能直接接触到的就是**应⽤层**（*Application Layer*），我们电脑或⼿机使⽤的应⽤软件都是在应⽤层实现。那么，当两个不同设备的应⽤需要通信的时候，应⽤就把应⽤数据传给下⼀层，也就是传输层。所以，应⽤层只需要专注于为⽤户提供应⽤功能，不⽤去关⼼数据是如何传输的，就类似于，我们寄快递的时候，只需要把包裹交给快递员，由他负责运输快递，我们不需要关⼼快速是如何被运输的。⽽且应⽤层是⼯作在操作系统中的⽤户态，传输层及以下则⼯作在内核态。

### 1.2 传输层



![img](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101950683.png)

​	应⽤层的数据包会传给传输层，**传输层**（**Transport Layer**）是为应⽤层提供⽹络⽀持的。在传输层会有两个传输协议，分别是 TCP 和 UDP。

- **TCP** 的全称叫传输层控制协议(**Transmission Control Protocol**)，⼤部分应⽤使⽤的正是 TCP 传输层协议，⽐ 如 HTTP 应⽤层协议。TCP 相⽐ UDP 多了很多特性，⽐如流量控制、超时重传、拥塞控制等，这些都是为了保证 数据包能可靠地传输给对⽅。
- **UDP**就相对很简单，简单到只负责发送数据包，不保证数据包是否能抵达对⽅，但它实时性相对更好，传输效率 也⾼。当然，UDP 也可以实现可靠传输，把 TCP 的特性在应⽤层上实现就可以，不过要实现⼀个商⽤的可靠 UDP 传输协议，也不是⼀件简单的事情。

​	应⽤需要传输的数据可能会⾮常⼤，如果直接传输就不好控制，因此当传输层的数据包⼤⼩超过 MSS（TCP 最⼤ 报⽂段⻓度） ，就要将数据包分块，这样即使中途有⼀个分块丢失或损坏了，只需要重新这⼀个分块，⽽不⽤重新 发送整个数据包。在 TCP 协议中，我们把每个分块称为⼀个 **TCP段（TCP Segment）**。

![image-20211210195507389](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101955437.png)

​	当设备作为接收⽅时，传输层则要负责把数据包传给应⽤，但是⼀台设备上可能会有很多应⽤在接收或者传输数 据，因此需要⽤⼀个编号将应⽤区分开来，这个编号就是 **端口**。⽐如 80 端⼝通常是 Web 服务器⽤的，22 端⼝通常是远程登录服务器⽤的。⽽对于浏览器（客户端）中的每个标 签栏都是⼀个独⽴的进程，操作系统会为这些进程分配临时的端⼝号。由于传输层的报⽂中会携带端⼝号，因此接收⽅可以识别出该报⽂是发送给哪个应⽤。

### 1.3 网络层

![image-20211210195801136](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101958183.png)

​	传输层可能⼤家刚接触的时候，会认为它负责将数据从⼀个设备传输到另⼀个设备，事实上它并不负责。实际场景中的⽹络环节是错综复杂的，中间有各种各样的线路和分叉路⼝，如果⼀个设备的数据要传输给另⼀个设 备，就需要在各种各样的路径和节点进⾏选择，⽽传输层的设计理念是简单、⾼效、专注，如果传输层还负责这⼀ 块功能就有点违背设计原则了。也就是说，我们不希望传输层协议处理太多的事情，只需要服务好应⽤即可，让其作为应⽤间数据传输的媒介，帮 助实现应⽤到应⽤的通信，⽽实际的传输功能就交给下⼀层，也就是⽹络层（Internet    Layer）。

​	⽹络层最常使⽤的是 **IP 协议（Internet Protocol）**，IP 协议会将传输层的报⽂作为数据部分，再加上 IP 包头组装 成 IP 报⽂，如果 IP 报⽂⼤⼩超过 MTU（以太⽹中⼀般为 1500 字节）就会再次进⾏分⽚，得到⼀个即将发送到⽹ 络的 IP 报⽂。

![image-20211210195853065](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101958133.png)



 	⽹络层负责将数据从⼀个设备传输到另⼀个设备，世界上那么多设备，⼜该如何找到对⽅呢？因此，⽹络层需要有 区分设备的编号。我们⼀般⽤ IP 地址给设备进⾏编号，对于 IPv4 协议， IP 地址共 32 位，分成了四段，每段是 8 位。只有⼀个单纯 的 IP 地址虽然做到了区分设备，但是寻址起来就特别麻烦，全世界那么多台设备，难道⼀个⼀个去匹配？这显然 不科学。

因此，需要将 IP 地址分成两种意义：

- **网络号**： 负责表示该IP地址是属于哪个子网的
- **主机号**： 负责标识同一子网下的不同主机

​	怎么分的呢？这需要配合**⼦⽹掩码**才能算出 IP 地址 的⽹络号和主机号。那么在寻址的过程中，先匹配到相同的⽹ 络号，才会去找对应的主机。除了寻址能⼒，   IP   协议还有另⼀个重要的能⼒就是路由。实际场景中，两台设备并不是⽤⼀条⽹线连接起来的，⽽是通过很多⽹关、路由器、交换机等众多⽹络设备连接起来的，那么就会形成很多条⽹络的路径，因此当数据包 到达⼀个⽹络节点，就需要通过算法决定下⼀步⾛哪条路径。所以**，IP 协议的寻址作⽤是告诉我们去往下⼀个⽬的地该朝哪个⽅向⾛，路由则是根据「下⼀个⽬的地」选择路 径**。寻址更像在导航，路由更像在操作⽅向盘。



### 1.4 数据链路层

![image-20211210200145494](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112102001554.png)

​	实际场景中，⽹络并不是⼀个整体，⽐如你家和我家就不属于⼀个⽹络，所以数据不仅可以在同⼀个⽹络中设备间 进⾏传输，也可以跨⽹络进⾏传输。⼀旦数据需要跨⽹络传输，就需要有⼀个设备同时在两个⽹络当中，这个设备⼀般是路由器，路由器可以通过路由表计算出下⼀个要去的 IP 地址。那问题来了，路由器怎么知道这个 IP 地址是哪个设备的呢？于是，就需要有⼀个专⻔的层来标识⽹络中的设备，让数据在⼀个链路中传输，这就是**数据链路层（Data Link Layer）**，它主要为⽹络层提供链路级别传输的服务。每⼀台设备的⽹卡都会有⼀个 MAC 地址，它就是⽤来唯⼀标识设备的。路由器计算出了下⼀个⽬的地 IP 地址，再 通过 ARP 协议找到该⽬的地的 MAC 地址，这样就知道这个 IP 地址是哪个设备的了。

### 1.5 物理层

![image-20211210200511732](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112102005801.png)

​	当数据准备要从设备发送到⽹络时，需要把数据包转换成电信号，让其可以在物理介质中传输，这⼀层就是**物理层（Physical   Layer）**，它主要是为数据链路层提供⼆进制传输的服务。



## 二、HTTP篇

### 2.1 HTTP的基本概念



#### 2.1.1 HTTP是什么？

> HTTP是什么？

​	HTTP是超文本协议，也是`Hyper Text Transfer Protocol`。

> 能否详细解析超文本传输协议？

![image-20211210200854290](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112102008346.png)

​	HTTP的名字「超⽂本协议传输」，它可以拆成三个部分：

- 超文本
- 传输
- 协议

##### 2.1.1.1 协议

协议的特点:

- **协**: 代表的意思是必须有**两个以上的参与者**。
- **议**: 代表的意思是对参与者的⼀种**⾏为约定和规范**。

​	HTTP 是⼀个⽤在计算机世界⾥的**协议**。它使⽤计算机能够理解的语⾔确⽴了⼀种计算机之间交流通信的规范（**两 个以上的参与者**），以及相关的各种控制和错误处理⽅式（**⾏为约定和规范**）。

##### 2.1.1.2 传输

​	所谓的「传输」，很好理解，就是把⼀堆东⻄从 A 点搬到 B 点，或者从 B 点 搬到 A 点。别轻视了这个简单的动作，它⾄少包含两项重要的信息。

​	HTTP 协议是⼀个**双向协议**。我们在上⽹冲浪时，浏览器是请求⽅ A ，百度⽹站就是应答⽅ B。双⽅约定⽤ HTTP 协议来通信，于是浏览器把请求数据发送给⽹站，⽹站再把⼀些数据返回给浏览器，最后由浏览器渲染在屏幕，就可以看到图⽚、视频了。

![image-20211210201656755](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112102016808.png)

​	 数据虽然是在 A 和 B 之间传输，但允许中间有**中转或接⼒**。就好像第⼀排的同学想传递纸条给最后⼀排的同学，那么传递的过程中就需要经过好多个同学（中间⼈），这样的传输⽅式就从「A < --- > B」，变成了「A <-> N <-> M <-> B」。⽽在 HTTP ⾥，需要中间⼈遵从 HTTP 协议，只要不打扰基本的数据传输，就可以添加任意额外的东⻄。针对**传输**，我们可以进⼀步理解了 HTTP。HTTP 是⼀个在计算机世界⾥专⻔⽤来在**两点之间传输数据**的约定和规范。

##### 2.1.1.3 超文本

​	HTTP 传输的内容是「超⽂本」。我们先来理解「⽂本」，在互联⽹早期的时候只是简单的字符⽂字，但现在「⽂本」的涵义已经可以扩展为图⽚、视频、压缩包等，在 HTTP 眼⾥这些都算作「⽂本」。再来理解「超⽂本」，它就是**超越了普通⽂本的⽂本**，它是⽂字、图⽚、视频等的混合体，**最关键有超链接**，能从⼀个超⽂本跳转到另外⼀个超⽂本。HTML 就是最常⻅的超⽂本了，它本身只是纯⽂字⽂件，但内部⽤很多标签定义了图⽚、视频等的链接，再经过浏览器的解释，呈现给我们的就是⼀个⽂字、有画⾯的⽹⻚了。

##### 2.1.1.4 总结

​	**HTTP是一个在计算机世界里专门在“两点”之间“传输”文字、图片、音频、视频等“超文本”数据的“约定和规范”。**

#### 2.1.2 HTTP状态码

![image-20211210202335619](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112102023699.png)

##### 2.1.2.1 1xx

​	`1xx`类状态码属于**提示**信息，是协议处理中的一种**中间**状态，实际用到的比较少



##### 2.1.2.2 2xx

​	`2xx`类状态码表示服务器**成功**处理了客户端的请求。

- `200 OK`： 表示一切正常。如果非`HEAD`请求，服务器返回的响应头都会有body。
- `204 No Content`： 和200OK基本相同，但响应头没有body数据。
- `206 Partial Content`:  表示响应返回的body数据并不是资源的全部，而是其中的一部分。一般应用于HTTP分块下载或断点续存。



##### 2.1.2.3 3xx

​	`3xx`类状态码表示客户端请求的资源发生了变动，需要客户端用新的`URL`重新发送请求获取数据，也就是 **重定向**。

- `301 Moved Permanently`: 表示永久重定向，说明请求的资源已经不存在了，需要改用新的URL再次访问。
- `302 Found`： 表示临时重定向，说明请求的资源还在，但暂时需要用另一个`URL`来访问。

> 301 和 302 都会在响应头⾥使⽤字段`Location`，指明后续要跳转的URL，浏览器会自动重定向新的URL。

- `304 Not Modified`： 不具备跳转的含义，表示资源为修改，重定向已存在的缓存文件，也称缓存重定向，用于缓存控制。

##### 2.1.2.4 4xx

`4xx`类状态码表示客户端发送的 **报文有误**，服务器无法处理，也就是错误码的含义。

- `400 Bad Request`: 表示客户端请求的报文有错误，但只是个笼统的错误。
- `403 Forbidden`:  表示服务器禁止访问资源，但不是客户端出错。
- `404 Not Found`: 表示请求的资源在服务器上不存在或未找到，所以无法提供给客户端。



##### 2.1.2.5 5xx

`5xx`类状态码表示客户端请求报文正确，但 **服务器处理时内部发生了错误**，属于服务器端的错误码。

- `500 Internal Server Error`： 一个笼统通用的错误码，服务器发生了什么错误，我们并不知道。
- `501 Not Implemented`: 表示客户端请求的功能还不支持，类似“即将开业，敬请期待”的意思
- `502 Bad Gateway`：通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，返回后端服务器发生了错误。
- `503 Service Unavailable`： 表示服务器当前很忙，暂时无法响应服务器，类似“网络服务正忙，请稍后重试”的意思。

#### 2.1.3 HTTP常见字段有哪些？

##### 1. Host

​	客户端发送请求时，用来指定服务器的域名。有了 `Host` 字段，就可以将请求发往「同一台」服务器上的不同网站。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112150840131.webp)

```js
Host : WWW.A.com
```

##### 2. Content-Length 

​	![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112150842456.webp)

```js
Content-Length: 1000
```

​	服务器在返回数据时，会有 `Content-Length` 字段，表明本次回应的数据长度。如上面则是告诉浏览器，本次服务器回应的数据长度是 1000 个字节，后面的字节就属于下一个回应了。

##### 3. Connection

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112150844266.webp)

```js
Connection: keep-alive
```



​	`Connection`字段最常用于客户端要求服务器使用 TCP 持久连接，以便其他请求复用。HTTP/1.1 版本的默认连接都是持久连接，但为了兼容老版本的 HTTP，需要指定 `Connection` 首部字段的值为 `Keep-Alive`。一个可以复用的 TCP 连接就建立了，直到客户端或服务器主动关闭连接。但是，这不是标准字段。

##### 4. Content-Type

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112150845631.webp)

```js
Content-Type: text/html; charset=utf-8
```

​	`Content-Type` 字段用于服务器回应时，告诉客户端，本次数据是什么格式。上面的类型表明，发送的是网页，而且编码是UTF-8。

​	客户端请求的时候，可以使用 `Accept` 字段声明自己可以接受哪些数据格式。

```js
Accept: */*
```

上面代码中，客户端声明自己可以接受任何格式的数据。

##### 5. Content-Encoding

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112150847176.webp)

```js
Content-Encoding: gzip
```

​	`Content-Encoding` 字段说明数据的压缩方法。表示服务器返回的数据使用了什么压缩格式。上面表示服务器返回的数据采用了 gzip 方式压缩，告知客户端需要用此方式解压。

​	客户端在请求时，用 `Accept-Encoding` 字段说明自己可以接受哪些压缩方法。

```js
Accept-Encoding: gzip, deflate
```



### 2.2 GET和POST的区别

#### 2.2.1 说一下GET和POST的区别？

​	`Get` 方法的含义是请求**从服务器获取资源**，这个资源可以是静态的文本、页面、图片视频等。

比如，你打开我的文章，浏览器就会发送 GET 请求给服务器，服务器就会返回文章的所有文字及资源。

![GET请求](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112150850543.webp)

​	而`POST` 方法则是相反操作，它向 `URI` 指定的资源提交数据，数据就放在报文的 body 里。

​	比如，你在我文章底部，敲入了留言后点击「提交」（**暗示你们留言**），浏览器就会执行一次 POST 请求，把你的留言文字放进了报文 body 里，然后拼接好 POST 请求头，通过 TCP 协议发送给服务器。

![POST请求](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112150851227.webp)

#### 2.2.2 GET和POST方法都是安全和幂等的吗？

先说明下安全和幂等的概念：

- 在 HTTP 协议里，所谓的「安全」是指请求方法不会「破坏」服务器上的资源。
- 所谓的「幂等」，意思是多次执行相同的操作，结果都是「相同」的。

那么很明显 **GET 方法就是安全且幂等的**，因为它是「只读」操作，无论操作多少次，服务器上的数据都是安全的，且每次的结果都是相同的。

**POST** 因为是「新增或提交数据」的操作，会修改服务器上的资源，所以是**不安全**的，且多次提交数据就会创建多个资源，所以**不是幂等**的。



### 2.3 HTTP特性

#### 2.3.1 HTTP优点

HTTP 最凸出的优点是「简单、灵活和易于扩展、应用广泛和跨平台」。

###### 1. 简单

​	HTTP 基本的报文格式就是 `header + body`，头部信息也是 `key-value` 简单文本的形式，**易于理解**，降低了学习和使用的门槛。



###### 2. 灵活和易于扩展

​	HTTP协议里的各类请求方法、URI/URL、状态码、头字段等每个组成要求都没有被固定死，都允许开发人员**自定义和扩充**。

同时 HTTP 由于是工作在应用层（ `OSI` 第七层），则它**下层可以随意变化**。

HTTPS 也就是在 HTTP 与 TCP 层之间增加了 SSL/TLS 安全传输层，HTTP/3 甚至把 TCP 层换成了基于 UDP 的 QUIC。

###### 3. 应用广泛和跨平台

​	互联网发展至今，HTTP 的应用范围非常的广泛，从台式机的浏览器到手机上的各种 APP，从看新闻、刷贴吧到购物、理财、吃鸡，HTTP 的应用**片地开花**，同时天然具有**跨平台**的优越性。



#### 2.3.2 HTTP缺点

​	HTTP 协议里有优缺点一体的**双刃剑**，分别是「无状态、明文传输」，同时还有一大缺点「不安全」。

##### 1. 无状态双刃剑

无状态的**好处**，因为服务器不会去记忆 HTTP 的状态，所以不需要额外的资源来记录状态信息，这能减轻服务器的负担，能够把更多的 CPU 和内存用来对外提供服务。

无状态的**坏处**，既然服务器没有记忆能力，它在完成有关联性的操作时会非常麻烦。

例如登录->添加购物车->下单->结算->支付，这系列操作都要知道用户的身份才行。但服务器不知道这些请求是有关联的，每次都要问一遍身份信息。

这样每操作一次，都要验证信息，这样的购物体验还能愉快吗？别问，问就是**酸爽**！

对于无状态的问题，解法方案有很多种，其中比较简单的方式用 **Cookie** 技术。

`Cookie` 通过在请求和响应报文中写入 Cookie 信息来控制客户端的状态。

相当于，**在客户端第一次请求后，服务器会下发一个装有客户信息的「小贴纸」，后续客户端请求服务器的时候，带上「小贴纸」，服务器就能认得了了**，

![Cookie技术](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151004110.webp)

##### 2. 明文传输双刃剑

​	明文意味着在传输过程中的信息，是可方便阅读的，通过浏览器的 F12 控制台或 Wireshark 抓包都可以直接肉眼查看，为我们调试工作带了极大的便利性。

​	但是这正是这样，HTTP 的所有信息都暴露在了光天化日下，相当于**信息裸奔**。在传输的漫长的过程中，信息的内容都毫无隐私可言，很容易就能被窃取，如果里面有你的账号密码信息，那**你号没了**。

##### 3. 不安全

HTTP 比较严重的缺点就是不安全：

- 通信使用明文（不加密），内容可能会被窃听。比如，**账号信息容易泄漏，那你号没了。**
- 不验证通信方的身份，因此有可能遭遇伪装。比如，**访问假的淘宝、拼多多，那你钱没了。**
- 无法证明报文的完整性，所以有可能已遭篡改。比如，**网页上植入垃圾广告，视觉污染，眼没了。**

HTTP 的安全问题，可以用 HTTPS 的方式解决，也就是通过引入 SSL/TLS 层，使得在安全上达到了极致。

#### 2.3.3 HTTP/1.1性能

HTTP 协议是基于 **TCP/IP**，并且使⽤了「**请求** **-** **应答**」的通信模式，所以性能的关键就在这**两点**⾥。

##### 1. 长连接

​	早期 HTTP/1.0 性能上的一个很大的问题，那就是每发起一个请求，都要新建一次 TCP 连接（三次握手），而且是串行请求，做了无畏的 TCP 连接建立和断开，增加了通信开销。

​	为了解决上述 TCP 连接问题，HTTP/1.1 提出了**长连接**的通信方式，也叫持久连接。这种方式的好处在于减少了 TCP 连接的重复建立和断开所造成的额外开销，减轻了服务器端的负载。

持久连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。

![短连接与长连接](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151011604.webp)

##### 2. 管道网络传输

​	HTTP/1.1 采用了长连接的方式，这使得管道（pipeline）网络传输成为了可能。即可在同一个 TCP 连接里面，客户端可以发起多个请求，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以**减少整体的响应时间。**

​	举例来说，客户端需要请求两个资源。以前的做法是，在同一个TCP连接里面，先发送 A 请求，然后等待服务器做出回应，收到后再发出 B 请求。管道机制则是允许浏览器同时发出 A 请求和 B 请求。

![管道网络传输](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151013324.webp)

​	但是服务器还是按照**顺序**，先回应 A 请求，完成后再回应 B 请求。要是 前面的回应特别慢，后面就会有许多请求排队等着。这称为「队头堵塞」。



##### 3. 对头阻塞

​	「请求 - 应答」的模式加剧了 HTTP 的性能问题。因为当顺序发送的请求序列中的一个请求因为某种原因被阻塞时，在后面排队的所有请求也一同被阻塞了，会招致客户端一直请求不到数据，这也就是「**队头阻塞**」。**好比上班的路上塞车**。

![对头阻塞](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151014186.webp)

总之 HTTP/1.1 的性能一般般，后续的 HTTP/2 和 HTTP/3 就是在优化 HTTP 的性能。

### 2.4 HTTP与HTTPS

#### 2.4.1 HTTP与HTTPS有哪些区别？

1. HTTP 是超文本传输协议，信息是明文传输，存在安全风险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。
2. HTTP 连接建立相对简单， TCP 三次握手之后便可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后，还需进行 SSL/TLS 的握手过程，才可进入加密报文传输。
3. HTTP 的端口号是 80，HTTPS 的端口号是 443。
4. HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。

#### 2.4.2 HTTPS解决了HTTP的哪些问题？

HTTP 由于是明文传输，所以安全上存在以下三个风险：

- **窃听风险**，比如通信链路上可以获取通信内容，用户号容易没。
- **篡改风险**，比如强制入垃圾广告，视觉污染，用户眼容易瞎。
- **冒充风险**，比如冒充淘宝网站，用户钱容易没。

HTTP**S** 在 HTTP 与 TCP 层之间加入了 `SSL/TLS` 协议。

![HTTP与HTTPS](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151017030.webp)

可以很好的解决了上述的风险：

- **信息加密**：交互信息无法被窃取，但你的号会因为「自身忘记」账号而没。
- **校验机制**：无法篡改通信内容，篡改了就不能正常显示，但百度「竞价排名」依然可以搜索垃圾广告。
- **身份证书**：证明淘宝是真的淘宝网，但你的钱还是会因为「剁手」而没。

可见，只要自身不做「恶」，SSL/TLS 协议是能保证通信是安全的。

#### 2.4.3 HTTPS是如何解决上面的三个风险？

- **混合加密**的方式实现信息的**机密性**，解决了窃听的风险。
- **摘要算法**的方式来实现**完整性**，它能够为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决了篡改的风险。
- 将服务器公钥放入到**数字证书**中，解决了冒充的风险。

##### （1）混合加密

​	通过**混合加密**的方式可以保证信息的**机密性**，解决了窃听的风险。

![混合加密](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151020701.webp)

HTTPS 采用的是**对称加密**和**非对称加密**结合的「混合加密」方式：

- 在通信建立前采用**非对称加密**的方式交换「会话秘钥」，后续就不再使用非对称加密。
- 在通信过程中全部使用**对称加密**的「会话秘钥」的方式加密明文数据。

采用「混合加密」的方式的原因：

- **对称加密**只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。
- **非对称加密**使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。

##### （2）摘要算法

​	**摘要算法**用来实现**完整性**，能够为数据生成独一无二的「指纹」，用于校验数据的完整性，解决了篡改的风险。

![校验完整性](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151023533.webp)

​	客户端在发送明文之前会通过摘要算法算出明文的「指纹」，发送的时候把「指纹 + 明文」一同
加密成密文后，发送给服务器，服务器解密后，用相同的摘要算法算出发送过来的明文，通过比较客户端携带的「指纹」和当前算出的「指纹」做比较，若「指纹」相同，说明数据是完整的。

##### （3）数字证书

​	客户端先向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。这就存在些问题，如何保证公钥不被篡改和信任度？所以这里就需要借助第三方权威机构 `CA` （数字证书认证机构），将**服务器公钥放在数字证书**（由数字证书认证机构颁发）中，只要证书是可信的，公钥就是可信的。

![数字证书工作流程](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151025186.webp)



通过数字证书的方式保证服务器公钥的身份，解决冒充的风险。

#### 2.4.4 HTTPS是如何建立连接的？期间交互了什么？

SSL/TLS 协议基本流程：

- 客户端向服务器索要并验证服务器的公钥。
- 双方协商生产「会话秘钥」。
- 双方采用「会话秘钥」进行加密通信。

前两步也就是 SSL/TLS 的建立过程，也就是握手阶段。

SSL/TLS 的「握手阶段」涉及**四次**通信，可见下图：

![HTTPS连接建立过程](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151031900.webp)

SSL/TLS 协议建立的详细流程：

##### *1. ClientHello*

首先，由客户端向服务器发起加密通信请求，也就是 `ClientHello` 请求。

在这一步，客户端主要向服务器发送以下信息：

（1）客户端支持的 SSL/TLS 协议版本，如 TLS 1.2 版本。

（2）客户端生产的随机数（`Client Random`），后面用于生产「会话秘钥」。

（3）客户端支持的密码套件列表，如 RSA 加密算法。

##### *2. SeverHello*

服务器收到客户端请求后，向客户端发出响应，也就是 `SeverHello`。服务器回应的内容有如下内容：

（1）确认 SSL/ TLS 协议版本，如果浏览器不支持，则关闭加密通信。

（2）服务器生产的随机数（`Server Random`），后面用于生产「会话秘钥」。

（3）确认的密码套件列表，如 RSA 加密算法。

（4）服务器的数字证书。

##### *3.客户端回应*

​	客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。如果证书没有问题，客户端会从数字证书中取出服务器的公钥，然后使用它加密报文，向服务器发送如下信息：

（1）一个随机数（`pre-master key`）。该随机数会被服务器公钥加密。

（2）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。

（3）客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供服务端校验。

​	上面第一项的随机数是整个握手阶段的第三个随机数，这样服务器和客户端就同时有三个随机数，接着就用双方协商的加密算法，**各自生成**本次通信的「会话秘钥」。

##### *4. 服务器的最后回应*

​	服务器收到客户端的第三个随机数（`pre-master key`）之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。然后，向客户端发生最后的信息：

（1）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。

（2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供客户端校验。

​	至此，整个 SSL/TLS 的握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的 HTTP 协议，只不过用「会话秘钥」加密内容。

### 2.5 HTTP/1.1、HTTP/2、HTTP/3演变

#### 2.5.1 说说 HTTP/1.1 相比 HTTP/1.0 提高了什么性能？

HTTP/1.1 相比 HTTP/1.0 性能上的改进：

- 支持 管道（pipeline）网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。
- 使用 TCP 长连接的方式改善了 HTTP/1.0 短连接造成的性能开销。

但 HTTP/1.1 还是有性能瓶颈：

- 服务器是按请求的顺序响应的，如果服务器响应慢，会招致客户端一直请求不到数据，也就是队头阻塞；
- 没有请求优先级控制；
- 请求只能从客户端开始，服务器只能被动响应。
- 请求 / 响应头部（Header）未经压缩就发送，首部信息越多延迟越大。只能压缩 `Body` 的部分；
- 发送冗长的首部。每次互相发送相同的首部造成的浪费较多；

#### 2.5.2 那上面的HTTP/1.1的性能瓶颈，HTTP/2做了什么优化？

HTTP/2 协议是基于 HTTPS 的，所以 HTTP/2 的安全性也是有保障的。

那 HTTP/2 相比 HTTP/1.1 性能上的改进：

##### （1）头部压缩

​	HTTP/2 会**压缩头部**（Header）如果你同时发出多个请求，他们的头是一样的或是相似的，那么，协议会帮你**消除重复的部分**。

​	这就是所谓的 `HPACK` 算法：在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就**提高速度**了。

##### （2）二进制格式

​	HTTP/2 不再像 HTTP/1.1 里的纯文本形式的报文，而是全面采用了**二进制格式。**头信息和数据体都是二进制，并且统称为帧（frame）：**头信息帧和数据帧**。

![报文区别](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151425517.webp)

​	这样虽然对人不友好，但是对计算机非常友好，因为计算机只懂二进制，那么收到报文后，无需再将明文的报文转成二进制，而是直接解析二进制报文，这**增加了数据传输的效率**。

##### （3）数据流

​	HTTP/2 的数据包不是按顺序发送的，同一个连接里面连续的数据包，可能属于不同的回应。因此，必须要对数据包做标记，指出它属于哪个回应。每个请求或回应的所有数据包，称为一个数据流（`Stream`）。每个数据流都标记着一个独一无二的编号，其中规定客户端发出的数据流编号为奇数， 服务器发出的数据流编号为偶数。客户端还可以**指定数据流的优先级**。优先级高的请求，服务器就先响应该请求。



![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151427907.webp)



##### （4）多路复用

​	HTTP/2 是可以在**一个连接中并发多个请求或回应，而不用按照顺序一一对应**。移除了 HTTP/1.1 中的串行请求，不需要排队等待，也就不会再出现「队头阻塞」问题，**降低了延迟，大幅度提高了连接的利用率**。举例来说，在一个 TCP 连接里，服务器收到了客户端 A 和 B 的两个请求，如果发现 A 处理过程非常耗时，于是就回应 A 请求已经处理好的部分，接着回应 B 请求，完成后，再回应 A 请求剩下的部分。

![多路复用](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151431284.webp)

##### （5）服务器推送

​	HTTP/2 还在一定程度上改善了传统的「请求 - 应答」工作模式，服务不再是被动地响应，也可以**主动**向客户端发送消息。举例来说，在浏览器刚请求 HTML 的时候，就提前把可能会用到的 JS、CSS 文件等静态资源主动发给客户端，**减少延时的等待**，也就是服务器推送（Server Push，也叫 Cache Push）。



#### 2.5.3 HTTP/2有哪些缺陷？HTTP/3做了哪些优化？

​	HTTP/2 主要的问题在于：多个 HTTP 请求在复用一个 TCP 连接，下层的 TCP 协议是不知道有多少个 HTTP 请求的。所以一旦发生了丢包现象，就会触发 TCP 的重传机制，这样在一个 TCP 连接中的**所有的 HTTP 请求都必须等待这个丢了的包被重传回来**。

- HTTP/1.1 中的管道（ pipeline）传输中如果有一个请求阻塞了，那么队列后请求也统统被阻塞住了
- HTTP/2 多请求复用一个TCP连接，一旦发生丢包，就会阻塞住所有的 HTTP 请求。

这都是基于 TCP 传输层的问题，所以 **HTTP/3 把 HTTP 下层的 TCP 协议改成了 UDP！**

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151437476.webp)

大家都知道 UDP 是不可靠传输的，但基于 UDP 的 **QUIC 协议** 可以实现类似 TCP 的可靠性传输。

- QUIC 有自己的一套机制可以保证传输的可靠性的。当某个流发生丢包时，只会阻塞这个流，**其他流不会受到影响**。
- TL3 升级成了最新的 `1.3` 版本，头部压缩算法也升级成了 `QPack`。
- HTTPS 要建立一个连接，要花费 6 次交互，先是建立三次握手，然后是 `TLS/1.3` 的三次握手。QUIC 直接把以往的 TCP 和 `TLS/1.3` 的 6 次交互**合并成了 3 次，减少了交互次数**。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151438285.webp)

所以， QUIC 是一个在 UDP 之上的**伪** TCP + TLS + HTTP/2 的多路复用的协议。QUIC 是新协议，对于很多网络设备，根本不知道什么是 QUIC，只会当做 UDP，这样会出现新的问题。所以 HTTP/3 现在普及的进度非常的缓慢，不知道未来 UDP 是否能够逆袭 TCP。



### 2.6 HTTP/1.1如何优化

### 2.7HTTPS RSA握手解析

### 2.8 HTTPS ECDHE握手解析

### 2.9 HTTPS如何优化

### 2.10 HTTP/2

#### 2.10.1 HTTP/1.1协议的性能问题

我们得先要了解下 HTTP/1.1 协议存在的性能问题，因为 HTTP/2 协议就是把这些性能问题逐个攻破了。

现在的站点相比以前变化太多了，比如：

- ***消息的大小变大了***，从几 KB 大小的消息，到几 MB 大小的消息；
- ***页面资源变多了***，从每个页面不到 10 个的资源，到每页超 100 多个资源；
- ***内容形式变多样了***，从单纯到文本内容，到图片、视频、音频等内容；
- ***实时性要求变高了***，对页面的实时性要求的应用越来越多；

​	这些变化带来的最大性能问题就是 **HTTP/1.1  的高延迟**，延迟高必然影响的就是用户体验。主要原因如下几个：

- ***延迟难以下降***，虽然现在网络的「带宽」相比以前变多了，但是延迟降到一定幅度后，就很难再下降了，说白了就是到达了延迟的下限；
- ***并发连接有限***，谷歌浏览器最大并发连接数是 6 个，而且每一个连接都要经过 TCP 和 TLS 握手耗时，以及 TCP 慢启动过程给流量带来的影响；
- ***队头阻塞问题***，同一连接只能在完成一个 HTTP 事务（请求和响应）后，才能处理下一个事务；
- ***HTTP 头部巨大且重复***，由于 HTTP 协议是无状态的，每一个请求都得携带 HTTP 头部，特别是对于有携带 cookie 的头部，而 cookie 的大小通常很大；
- ***不支持服务器推送消息***，因此当客户端需要获取通知时，只能通过定时器不断地拉取消息，这无疑浪费大量了带宽和服务器资源。

几个常见的优化手段：

- 将多张小图合并成一张大图供浏览器 JavaScript 来切割使用，这样可以将多个请求合并成一个请求，但是带来了新的问题，当某张小图片更新了，那么需要重新请求大图片，浪费了大量的网络带宽；
- 将图片的二进制数据通过 base64 编码后，把编码数据嵌入到 HTML 或  CSS 文件中，以此来减少网络请求次数；
- 将多个体积较小的 JavaScript 文件使用 webpack 等工具打包成一个体积更大的 JavaScript 文件，以一个请求替代了很多个请求，但是带来的问题，当某个 js 文件变化了，需要重新请求同一个包里的所有 js 文件；
- 将同一个页面的资源分散到不同域名，提升并发连接上限，因为浏览器通常对同一域名的 HTTP 连接最大只能是 6 个；

​	尽管对 HTTP/1.1 协议的优化手段如此之多，但是效果还是不尽人意，因为这些手段都是对 HTTP/1.1  协议的“外部”做优化，**而一些关键的地方是没办法优化的，比如请求-响应模型、头部巨大且重复、并发连接耗时、服务器不能主动推送等，要改变这些必须重新设计 HTTP 协议，于是 HTTP/2 就出来了！**

#### 2.10.2 兼容HTTP/1.1

​	HTTP/2 出来的目的是为了改善 HTTP 的性能。协议升级有一个很重要的地方，就是要**兼容**老版本的协议，否则新协议推广起来就相当困难，所幸 HTTP/2 做到了兼容 HTTP/1.1 。

那么，HTTP/2 是怎么做的呢？

1. HTTP/2 没有在 URI 里引入新的协议名，仍然用「http://」表示明文协议，用「https://」表示加密协议，于是只需要浏览器和服务器在背后自动升级协议，这样可以让用户意识不到协议的升级，很好的实现了协议的平滑升级。
2. 只在应用层做了改变，还是基于 TCP 协议传输，应用层方面为了保持功能上的兼容，HTTP/2 把 HTTP 分解成了「语义」和「语法」两个部分，「语义」层不做改动，与 HTTP/1.1 完全一致，比如请求方法、状态码、头字段等规则保留不变。

但是，HTTP/2 在「语法」层面做了很多改造，基本改变了 HTTP 报文的传输格式。



#### 2.10.3 头部压缩

​	HTTP 协议的报文是由「Header + Body」构成的，对于 Body  部分，HTTP/1.1 协议可以使用头字段 「Content-Encoding」指定 Body 的压缩方式，比如用 gzip 压缩，这样可以节约带宽，但报文中的另外一部分 Header，是没有针对它的优化手段。

HTTP/1.1 报文中 Header 部分存在的问题：

- 含很多固定的字段，比如Cookie、User Agent、Accept 等，这些字段加起来也高达几百字节甚至上千字节，所以有必要**压缩**；
- 大量的请求和响应的报文里有很多字段值都是重复的，这样会使得大量带宽被这些冗余的数据占用了，所以有必须要**避免重复性**；
- 字段是 ASCII 编码的，虽然易于人类观察，但效率低，所以有必要改成**二进制编码**；

​	HTTP/2 对 Header 部分做了大改造，把以上的问题都解决了。HTTP/2 没使用常见的 gzip 压缩方式来压缩头部，而是开发了 **HPACK** 算法，HPACK 算法主要包含三个组成部分：

- 静态字典；
- 动态字典；
- Huffman 编码（压缩算法）；

​	客户端和服务器两端都会建立和维护「**字典**」，用长度较小的索引号表示重复的字符串，再用 Huffman 编码压缩数据，**可达到 50%~90% 的高压缩率**。

##### （1）静态表编码

​	HTTP/2 为高频出现在头部的字符串和字段建立了一张**静态表**，它是写入到 HTTP/2 客户端与服务器的代码中的，不会变化的，静态表里共有 `61` 组，如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151506009.webp)

​	表中的 `Index` 表示索引（Key），`Header Value` 表示索引对应的  Value，`Header Name` 表示字段的名字，比如 Index 为 2 代表 GET，Index 为 8 代表状态码 200。你可能注意到，表中有的 Index 没有对应的 Header Value，这是因为这些 Value 并不是固定的而是变化的，这些 Value 都会经过 Huffman 编码后，才会发送出去。这么说有点抽象，我们来看个具体的例子，下面这个 `server` 头部字段，在 HTTP/1.1 的形式如下：

```
server: nghttpx\r\n
```

​	算上冒号空格和末尾的\r\n，共占用了 17 字节，**而使用了静态表和 Huffman 编码，可以将它压缩成 8 字节，压缩率大概 47 %**。我抓了个 HTTP/2 协议的网络包，你可以从下图看到，高亮部分就是 `server` 头部字段，只用了 8 个字节来表示 `server` 头部数据。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151524047.webp)

​	根据 RFC7541 规范，如果头部字段属于静态表范围，并且 Value 是变化，那么它的 HTTP/2 头部前 2 位固定为 `01`，所以整个头部格式如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151506054.webp)

​	HTTP/2 头部由于基于**二进制编码**，就不需要冒号空格和末尾的\r\n作为分隔符，于是改用表示字符串长度（Value Length）来分割 Index 和 Value。接下来，根据这个头部格式来分析上面抓包的 `server` 头部的二进制数据。

​	首先，从静态表中能查到 `server` 头部字段的 Index 为 54，二进制为 110110，再加上固定 01，头部格式第 1 个字节就是 `01110110`，这正是上面抓包标注的红色部分的二进制数据。

​	然后，第二个字节的首个比特位表示 Value 是否经过 Huffman 编码，剩余的 7 位表示 Value 的长度，比如这次例子的第二个字节为 `10000110`，首位比特位为 1 就代表 Value 字符串是经过 Huffman 编码的，经过 Huffman 编码的 Value 长度为 6。

​	最后，字符串 `nghttpx` 经过 Huffman 编码后压缩成了 6 个字节，Huffman 编码的原理是将高频出现的信息用「较短」的编码表示，从而缩减字符串长度。

​	于是，在统计大量的 HTTP 头部后，HTTP/2 根据出现频率将 ASCII 码编码为了 Huffman 编码表，可以在 RFC7541 文档找到这张**静态 Huffman 表**，我就不把表的全部内容列出来了，我只列出字符串 `nghttpx` 中每个字符对应的 Huffman 编码，如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151506328.webp)

​	通过查表后，字符串 `nghttpx` 的 Huffman 编码在下图看到，共 6 个字节，每一个字符的 Huffman 编码，我用相同的颜色将他们对应起来了，最后的 7 位是补位的。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151506016.webp)

最终，`server` 头部的二进制数据对应的静态头部格式如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151523767.webp)

##### （2）动态表编码

​	静态表只包含了 61 种高频出现在头部的字符串，不在静态表范围内的头部字符串就要自行构建**动态表**，它的 Index 从 `62` 起步，会在编码解码的时候随时更新。

​	比如，第一次发送时头部中的「`user-agent` 」字段数据有上百个字节，经过 Huffman 编码发送出去后，客户端和服务器双方都会更新自己的动态表，添加一个新的 Index 号 62。**那么在下一次发送的时候，就不用重复发这个字段的数据了，只用发 1 个字节的 Index 号就好了，因为双方都可以根据自己的动态表获取到字段的数据**。

​	而且，随着在同一 HTTP/2 连接上发送的报文越来越多，客户端和服务器双方的「字典」积累的越来越多，理论上最终每个头部字段都会变成 1 个字节的 Index，这样便避免了大量的冗余数据的传输，大大节约了带宽。

​	理想很美好，现实很骨感。动态表越大，占用的内存也就越大，如果占用了太多内存，是会影响服务器性能的，因此 Web 服务器都会提供类似 `http2_max_requests` 的配置，用于限制一个连接上能够传输的请求数量，避免动态表无限增大，请求数量到达上限后，就会关闭 HTTP/2 连接来释放内存。

综上，HTTP/2 头部的编码通过「静态表、动态表、Huffman 编码」共同完成的。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151521708.webp)

#### 2.10.4 二进制帧

​	HTTP/2 厉害的地方在于将 HTTP/1 的文本格式改成二进制格式传输数据，极大提高了 HTTP 传输效率，而且二进制数据使用位运算能高效解析。

你可以从下图看到，HTTP/1.1 的响应 和 HTTP/2 的区别：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151523484.webp)

​	HTTP/2 把响应报文划分成了两个**帧（Frame）**，图中的 HEADERS（首部）和 DATA（消息负载） 是帧的类型，也就是说一条 HTTP 响应，划分成了两个帧来传输，并且采用二进制来编码。

![二进制帧](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151525498.webp)

​	帧头（Fream Header）很小，只有 9 个字节，帧开头的前 3 个字节表示帧数据（Fream Playload）的**长度**。帧长度后面的一个字节是表示**帧的类型**，HTTP/2 总共定义了 10 种类型的帧，一般分为**数据帧**和**控制帧**两类，如下表格：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151526253.webp)

帧类型后面的一个字节是**标志位**，可以保存 8 个标志位，用于携带简单的控制信息，比如：

- **END_HEADERS** 表示头数据结束标志，相当于 HTTP/1 里头后的空行（“\r\n”）；
- **END_STREAM** 表示单方向数据发送结束，后续不会再有数据帧。
- **PRIORITY** 表示流的优先级；

​	帧头的最后 4 个字节是**流标识符**（Stream ID），但最高位被保留不用，只有 31 位可以使用，因此流标识符的最大值是 2^31，大约是 21 亿，它的作用是用来标识该 Fream 属于哪个 Stream，接收方可以根据这个信息从乱序的帧里找到相同 Stream ID 的帧，从而有序组装信息。最后面就是**帧数据**了，它存放的是通过 **HPACK  算法**压缩过的 HTTP 头部和包体。

#### 2.10.5 并发传输

​	知道了 HTTP/2 的帧结构后，我们再来看看它是如何实现**并发传输**的。我们都知道 HTTP/1.1 的实现是基于请求-响应模型的。同一个连接中，HTTP 完成一个事务（请求与响应），才能处理下一个事务，也就是说在发出请求等待响应的过程中，是没办法做其他事情的，如果响应迟迟不来，那么后续的请求是无法发送的，也造成了**队头阻塞**的问题。而 HTTP/2 就很牛逼了，通过 Stream 这个设计，**多个 Stream 复用一条 TCP 连接，达到并发的效果**，解决了 HTTP/1.1 队头阻塞的问题，提高了 HTTP 传输的吞吐量。

​	为了理解 HTTP/2 的并发是怎样实现的，我们先来理解 HTTP/2 中的 Stream、Message、Frame 这 3 个概念。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151529268.webp)

你可以从上图中看到：

- 1 个 TCP 连接包含一个或者多个 Stream，Stream 是 HTTP/2 并发的关键技术；
- Stream 里可以包含 1 个或多个 Message，Message 对应 HTTP/1 中的请求或响应，由 HTTP 头部和包体构成；
- Message 里包含一条或者多个 Frame，Frame 是 HTTP/2 最小单位，以二进制压缩格式存放 HTTP/1 中的内容（头部和包体）；

​	因此，我们可以得出 2 个结论：HTTP 消息可以由多个 Frame 构成，以及 1 个 Frame 可以由多个 TCP 报文构成。在 HTTP/2 连接上，**不同 Stream 的帧是可以乱序发送的（因此可以并发不同的 Stream ）**，因为每个帧的头部会携带 Stream ID 信息，所以接收端可以通过 Stream ID 有序组装成 HTTP 消息，而**同一 Stream 内部的帧必须是严格有序的**。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151531770.webp)

​	客户端和服务器**双方都可以建立 Stream**， Stream ID 也是有区别的，客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号。同一个连接中的 Stream ID 是不能复用的，只能顺序递增，所以当 Stream ID 耗尽时，需要发一个控制帧 `GOAWAY`，用来关闭 TCP 连接。

​	在 Nginx 中，可以通过 `http2_max_concurrent_streams` 配置来设置 Stream 的上限，默认是 128 个。HTTP/2 通过 Stream 实现的并发，比 HTTP/1.1 通过 TCP 连接实现并发要牛逼的多，**因为当 HTTP/2 实现 100 个并发 Stream 时，只需要建立一次 TCP 连接，而  HTTP/1.1 需要建立 100 个 TCP 连接，每个 TCP 连接都要经过TCP 握手、慢启动以及 TLS 握手过程，这些都是很耗时的。**

​	HTTP/2 还可以对每个 Stream 设置不同**优先级**，帧头中的「标志位」可以设置优先级，比如客户端访问 HTML/CSS 和图片资源时，希望服务器先传递 HTML/CSS，再传图片，那么就可以通过设置 Stream 的优先级来实现，以此提高用户体验。

#### 2.10.6 服务器主动推送

​	HTTP/1.1 不支持服务器主动推送资源给客户端，都是由客户端向服务器发起请求后，才能获取到服务器响应的资源。比如，客户端通过  HTTP/1.1 请求从服务器那获取到了 HTML 文件，而 HTML 可能还需要依赖 CSS 来渲染页面，这时客户端还要再发起获取 CSS 文件的请求，需要两次消息往返，如下图左边部分：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151535959.webp)



​	如上图右边部分，在 HTTP/2 中，客户端在访问 HTML 时，服务器可以直接主动推送 CSS 文件，减少了消息传递的次数。

在 Nginx 中，如果你希望客户端访问 /test.html 时，服务器直接推送 /test.css，那么可以这么配置：

```js
location /test.html { 
  http2_push /test.css; 
}
```

那 HTTP/2 的推送是怎么实现的？

​	客户端发起的请求，必须使用的是奇数号 Stream，服务器主动的推送，使用的是偶数号 Stream。服务器在推送资源时，会通过 `PUSH_PROMISE` 帧传输 HTTP 头部，并通过帧中的 `Promised Stream ID` 字段告知客户端，接下来会在哪个偶数号 Stream 中发送包体。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151536789.webp)

​	如上图，在 Stream 1 中通知客户端 CSS 资源即将到来，然后在 Stream 2 中发送 CSS 资源，注意 Stream 1 和 2 是可以**并发**的。

#### 2.10.7 总结

​	HTTP/2 协议其实还有很多内容，比如流控制、流状态、依赖关系等等。这次主要介绍了关于 HTTP/2 是如何提示性能的几个方向，它相比 HTTP/1 大大提高了传输效率、吞吐能力。

​	第一点，对于常见的 HTTP 头部通过**静态表和 Huffman 编码**的方式，将体积压缩了近一半，而且针对后续的请求头部，还可以建立**动态表**，将体积压缩近 90%，大大提高了编码效率，同时节约了带宽资源。不过，动态表并非可以无限增大， 因为动态表是会占用内存的，动态表越大，内存也越大，容易影响服务器总体的并发能力，因此服务器需要限制 HTTP/2 连接时长或者请求次数。

​	第二点，**HTTP/2 实现了 Stream 并发**，多个 Stream 只需复用 1 个 TCP 连接，节约了 TCP 和 TLS 握手时间，以及减少了 TCP 慢启动阶段对流量的影响。不同的 Stream ID 才可以并发，即时乱序发送帧也没问题，但是同一个 Stream 里的帧必须严格有序。另外，可以根据资源的渲染顺序来设置 Stream 的**优先级**，从而提高用户体验。

​	第三点，**服务器支持主动推送资源**，大大提升了消息的传输性能，服务器推送资源时，会先发送 PUSH_PROMISE 帧，告诉客户端接下来在哪个 Stream 发送资源，然后用偶数号 Stream 发送资源给客户端。

​	HTTP/2 通过 Stream 的并发能力，解决了 HTTP/1 队头阻塞的问题，看似很完美了，但是 HTTP/2 还是存在“队头阻塞”的问题，只不过问题不是在 HTTP 这一层面，而是在 TCP 这一层。**HTTP/2 是基于 TCP 协议来传输数据的，TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且连续的，这样内核才会将缓冲区里的数据返回给 HTTP 应用，那么当「前 1 个字节数据」没有到达时，后收到的字节数据只能存放在内核缓冲区里，只有等到这 1 个字节数据到达时，HTTP/2 应用层才能从内核中拿到数据，这就是 HTTP/2 队头阻塞问题。**

​	有没有什么解决方案呢？既然是 TCP 协议自身的问题，那干脆放弃 TCP 协议，转而使用 UDP 协议作为传输层协议，这个大胆的决定， HTTP/3 协议做了！

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112151537394.webp)



### 2.11 HTTP/3





## 三、TCP篇



### 3.1 TCP三次握手与四次挥手



#### 3.1.1 TCP基本认识



##### 3.1.1.1 TCP头格式

![TCP头格式](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112102053576.png)

- **序列号**： 在建⽴连接时由计算机⽣成的随机数作为其初始值，通过   SYN   包传给接收端主机，每发送⼀次数据，就「累加」⼀次该「数据字节数」的⼤⼩。**用来解决网络包乱序问题**。
- **确认应答号**: 指下⼀次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数据都已经被正常接收。 **用来解决不丢包的问题**
- **控制位**：
  - **ACK**: 该位为1时，「确认应答」的字段变为有效，TCP   规定除了最初建立连接时的`SYN`包之外该位必须设置为1。
  - **RST**：该位为1时，表示 TCP 连接中出现异常必须强制断开连接。
  - **SYN**： 该位为 1 时，表示希望建⽴连接，并在其「序列号」的字段进⾏序列号初始值的设定。
  - **FIN**： 该位为 1 时，表示今后不会再有数据发送，希望断开连接。当通信结束希望断开连接时，通信双⽅的主机之间就可以相互交换 FIN 位为 1 的 TCP 段。



##### 3.1.1.2 为什么需要TCP协议？

​	IP 层是「不可靠」的，它不保证⽹络包的交付、不保证⽹络包的按序交付、也不保证⽹络包中的数据的完整性。如果需要保障⽹络数据包的可靠性，那么就需要由上层（传输层）的 `TCP `协议来负责。因为TCP是一个工作在 **传输层**的 **可靠**数据传输的服务，它能确保接收端接受的网络包是 **无损坏、无间隔、非冗余和按序的。**

##### 3.1.1.3 什么是TCP？

![image-20211210210630353](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112102106417.png)

​	TCP 是**⾯向连接的、可靠的、基于字节流**的传输层通信协议。

- **面向连接**：⼀定是「⼀对⼀」才能连接，不能像 UDP 协议可以⼀个主机同时向多个主机发送消息，也就是⼀对多是⽆法做到的；
- **可靠的**：⽆论的⽹络链路中出现了怎样的链路变化，TCP 都可以保证⼀个报⽂⼀定能够到达接收端
- **基于字节流**：消息是「没有边界」的，所以⽆论我们消息有多⼤都可以进⾏传输。并且消息是「有序的」，当「前⼀个」消息没有收到的时候，即使它先收到了后⾯的字节，那么也不能扔给应⽤层去处理，同时对「重复」的报⽂会⾃动丢弃

##### 3.1.1.4 什么是TCP连接？

我们来看看 RFC 793 是如何定义「连接」的：

*Connections:*

*The reliability and flow control mechanisms described above require*

*that TCPs initialize and maintain certain status information for*

*each data stream. The combination of this information, including*

*sockets, sequence numbers, and window sizes, is called a connection.*

简单来说，**用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括Socket、序列号和 窗口大小称之为连接。**

![image-20211210211104306](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112102111369.png)

所以我们可以知道，建⽴⼀个 TCP 连接是需要客户端与服务器端达成上述三个信息的共识。

- **Socket**： 由IP地址和端口号组成
- **序列号**： 用来解决乱序问题等
- **窗口大小**： 用来做流量控制

##### 3.1.1.5 如何确定一个TCP连接呢？

TCP 四元组可以唯⼀的确定⼀个连接，四元组包括如下： 

- **源地址**
- **源端口**
- **目的地址**
- **目的端口**

​	源地址和⽬的地址的字段（32位）是在 IP 头部中，作⽤是通过 IP 协议发送报⽂给对⽅主机。源端⼝和⽬的端⼝的字段（16位）是在 TCP 头部中作⽤是告诉 TCP 协议应该把报⽂发给哪个进程。



##### 3.1.1.6 有一个IP的服务器监听了一个端口，它的TCP的最大连接数是多少？

​	服务器通常固定在某个本地端⼝上监听，等待客户端的连接请求。因此，客户端 IP 和 端⼝是可变的，其理论值计算公式如下：

![image-20211211193443911](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112111934994.png)

​	对 IPv4，客户端的 IP 数最多为 2 的 32 次⽅，客户端的端⼝数最多为 2 的 16 次⽅，也就是服务端单机最⼤ TCP 连接数，约为 2 的 48 次⽅。

当然，服务端最⼤并发 TCP 连接数远不能达到理论上限。

- 首先主要是 **文件描述符限制**，Socket都是文件，所以首先要通过`ulimit`配置文件描述符的数目；
- 另一个是**内存限制**，每个 TCP 连接都要占⽤⼀定内存，操作系统的内存是有限的。

##### 3.1.1.7 UDP和TCP有什么区别呢？分别的应用场景是？

​	UDP 不提供复杂的控制机制，利⽤ IP 提供⾯向「⽆连接」的通信服务。UDP 协议真的⾮常简单，头部只有 8 个字节（ 64 位），UDP 的头部格式如下：

- **目标和源端口**： 主要告诉UDP协议应该把报文发给哪个进程。
- **包长度**： 该字段保存了UDP首部的长度跟数据的长度之和
- **校验和**： 校验和是为了提供可靠的UDP首部和数据设计的

![image-20211211193841270](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112111938355.png)

> TCP和UDP的区别：

1. **连接**
   - TCP是⾯向连接的传输层协议，传输数据前先要建⽴连接。
   - UDP是不需要连接，即刻传输数据。
2. **服务对象**
   - TCP：  是⼀对⼀的两点服务，即⼀条连接只有两个端点。
   - UDP:  ⽀持⼀对⼀、⼀对多、多对多的交互通信.
3. **可靠性**
   - TCP ： 是可靠交付数据的，数据可以⽆差错、不丢失、不重复、按需到达。
   - UDP：  是尽最⼤努⼒交付，不保证可靠交付数据。
4. **拥塞控制、流量控制**
   - TCP： 有拥塞控制和流量控制机制，保证数据传输的安全性。
   - UDP:   没有，即使⽹络⾮常拥堵了，也不会影响 UDP 的发送速率。
5. **首部开销**
   - TCP：⾸部⻓度较⻓，会有⼀定的开销，⾸部在没有使⽤「选项」字段时是 20个字节，如果使用了选项字段则会变长
   - UDP: ⾸部只有 8 个字节，并且是固定不变的，开销较⼩。
6. **传输方式**
   - TCP ： 是流式传输，没有边界，但保证顺序和可靠。
   - UDP：  是⼀个包⼀个包的发送，是有边界的，但可能会丢包和乱序。
7. **分片不同**
   - TCP: 数据⼤⼩如果⼤于 MSS ⼤⼩，则会在传输层进⾏分⽚，⽬标主机收到后，也同样在传输层组装 TCP数据包，如果中途丢失了⼀个分⽚，只需要传输丢失的这个分⽚。
   - UDP: 数据⼤⼩如果⼤于 MTU ⼤⼩，则会在 IP 层进⾏分⽚，⽬标主机收到后，在 IP 层组装完数据，接着 再传给传输层，但是如果中途丢了⼀个分⽚，在实现可靠传输的 UDP 时则就需要重传所有的数据包，这样 传输效率⾮常差，所以通常 UDP 的报⽂应该⼩于 MTU。

> TCP和UDP应用场景：

由于 TCP 是⾯向连接，能保证数据的可靠性交付，因此经常⽤于：

- FTP文件传输
- HTTP/HTTPS

由于UDP面向无连接，它可以随时发送数据，再加上UDP本身的处理既简单又高效，因此经常用于：

- 包总量较少的通信，如DNS、SNMP等
- 视频、音频等多媒体通信
- 广播通信

##### 3.1.1.8 为什么 UDP 头部没有「⾸部⻓度」字段，⽽ TCP  头部有「⾸部⻓度」字段呢？

​	原因是 TCP 有可变⻓的「选项」字段，⽽ UDP 头部⻓度则是不会变化的，⽆需多⼀个字段去记录 UDP 的⾸部⻓ 度。

##### 3.1.1.5 为什么 UDP 头部有「包⻓度」字段，⽽ TCP  头部则没有「包⻓度」字段呢？

先说说TCP是如何计算负载数据长度：

![image-20211211195314232](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112111953291.png)

​	其中 IP 总⻓度 和 IP ⾸部⻓度，在 IP ⾸部格式是已知的。TCP ⾸部⻓度，则是在 TCP ⾸部格式已知的，所以就 可以求得 TCP 数据的⻓度。⼤家这时就奇怪了问：“ UDP 也是基于 IP 层的呀，那 UDP 的数据⻓度也可以通过这个公式计算呀？ 为何还要有「包⻓度」呢？”

 **因为为了⽹络设备硬件设计和处理⽅便，⾸部⻓度需要是4字节的整数倍。**

#### 3.1.2 TCP连接建立

##### 3.1.2.1 TCP三次握手过程和状态变迁

​	TCP 是⾯向连接的协议，所以使⽤ TCP 前必须先建⽴连接，⽽**建⽴连接是通过三次握⼿来进⾏的。**

![image-20211211195746454](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112111957563.png)

1. 一开始，客户端和服务端都处于`CLOSE`状态。先是服务器主动监听某个端口，处于`LISTEN`状态。

![image-20211211195913014](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112111959095.png)

2. 客户端会随机初始化序号（ `client_isn` ），将此序号置于 TCP ⾸部的「序号」字段中，同时把 `SYN `标志位置为 1 ，表示 `SYN `报⽂。接着把第⼀个 `SYN `报⽂发送给服务端，表示向服务端发起连接，该报⽂不包含应⽤层数据，之后客户端处于 `SYN-SENT` 状态。

![image-20211211200058529](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112000625.png)

2. 服务端收到客户端的`SYN`报文后，首先服务端也随机初始化自己的序号（`server_isn`），将此序号填入TCP首部的[序号]字段中，其次把TCP首部的[ 确认应答号 ] 填入 `client_isn + 1`，接着把`SYN`和`ACK`标志位置为1。最后把该报文发给客户端，该报文也不包含应用数据，之后服务端处于`SYN-REVD`状态。

![image-20211211200448035](C:/Users/27135/AppData/Roaming/Typora/typora-user-images/image-20211211200448035.png)

3. 客户端收到服务端报文后，还向服务端回应最后一个应答报文，首先该报文TCP首部`ACK`标志位置为1其次[ 确认应答号 ] 字段填入`server_isn + 1`，最后把报文发送给服务端，这次报文可以携带客户端到服务端的数据，之后客户端处于`ESTABLSHED`状态。
4. 服务器收到客户端的应答报文后，也进入`ESTABLISHED`状态。



##### 3.1.2.2 为什么第三次握手可以携带数据，前两次握手不可以携带数据？

​	一旦完成三次握手，双方都处于`ESTABLISHED`状态，此时连接就已建立，客户端和服务端就可以发送数据。

##### 3.1.2.3 如何在 Linux 系统中查看 TCP 状态？

TCP 的连接状态查看，在 Linux 可以通过` netstat -napt` 命令查看。

![image-20211211201132305](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112011386.png)

##### 3.1.2.4 为什么是三次握手？不是两次、四次？

在前⾯我们知道了什么是 **TCP** **连接**：⽤于保证可靠性和流ᰁ控制维护的某些状态信息，这些信息的组合，包括 **Socket、序列号和窗口大小**称为连接。所以重要的是 **为什么三次握手才可以初始化Socket、序列号和窗口大小并建立TCP连接**。

接下来以三个⽅⾯分析三次握⼿的原因：

- 三次握⼿才可以阻⽌重复历史连接的初始化（主要原因）
- 三次握⼿才可以同步双⽅的初始序列号
- 三次握⼿才可以避免资源浪费

###### 原因一：避免历史连接

我们来看看 RFC 793 指出的 TCP 连接使⽤三次握⼿的**⾸要原因**：

*The principle reason for the three-way handshake is to prevent old duplicate connection initiations from causing confusion*

简单来说，三次握手的 **首要原因是为了防止旧的重复连接初始化造成混乱**。

​	⽹络环境是错综复杂的，往往并不是如我们期望的⼀样，先发送的数据包，就先到达⽬标主机，反⽽它很骚，可能会由于⽹络拥堵等乱七⼋糟的原因，会使得旧的数据包，先到达⽬标主机，那么这种情况下 TCP 三次握⼿是如何避免的呢？

![image-20211211202155637](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112021703.png)

![image-20211211202201750](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112022856.png)

客户端连续发送多次 SYN 建⽴连接的报⽂，在**⽹络拥堵**情况下：

- 一个 [ 旧SYN报文 ] 比 [ 最新的SYN ] 报文早到达了服务端；
- 那么此时服务端就会回一个 `SYN + ACK`报文给客户端；
-   客户端收到后可以根据⾃身的上下⽂，判断这是⼀个历史连接（序列号过期或超时），那么客户端就会发送`RST`报文给服务端，表示中止这一次的连接。

​	如果是两次握⼿连接，就不能判断当前连接是否是历史连接，三次握⼿则可以在客户端（发送⽅）准备发送第三次报⽂时，客户端因有⾜够的上下⽂来判断当前连接是否是历史连接：

- 如果是历史连接（序列号过期或超时），则第三次握⼿发送的报⽂是 RST 报⽂，以此中⽌历史连接；
- 如果不是历史连接，则第三次发送的报⽂是 ACK 报⽂，通信双⽅就会成功建⽴连接；

所以，TCP使用三次握手建立连接的最主要的原因是 **防止历史连接初始化了连接**。



###### 原因二： 同步双方初始序列号

​	TCP 协议的通信双⽅， 都必须维护⼀个「序列号」， 序列号是可靠传输的⼀个关键因素，它的作⽤：

- 接收⽅可以去除重复的数据；
- 接收⽅可以根据数据包的序列号按序接收；
- 可以标识发送出去的数据包中， 哪些是已经被对⽅收到的；

​	可⻅，序列号在 TCP 连接中占据着⾮常重要的作⽤，所以当客户端发送携带「初始序列号」的`SYN`报文的时候，需要服务端回⼀个 ACK 应答报⽂，表示客户端的 SYN 报⽂已被服务端成功接收，那当服务端发送「初始序列号」给客户端的时候，依然也要得到客户端的应答回应， **这样一来一回，才能确保双方的初始化序号能被可靠的同步。**

![image-20211211203007586](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112030669.png)

​	四次握⼿其实也能够可靠的同步双⽅的初始化序号，但由于 **第二步和第三步可以优化成一步**，所以就成了 **三次握手**。而两次握⼿只保证了⼀⽅的初始序列号能被对⽅成功接收，没办法保证双⽅的初始序列号都能被确认接收。



###### 原因三： 避免资源浪费

​	如果只有「两次握⼿」，当客户端的 SYN 请求连接在⽹络中阻塞，客户端没有接收到 ACK 报⽂，就会重新发送 SYN ，由于没有第三次握⼿，服务器不清楚客户端是否收到了⾃⼰发送的建⽴连接的 ACK 确认信号，所以每收到⼀个 SYN 就只能先主动建⽴⼀个连接，这会造成什么情况呢？

​	如果客户端的 SYN 阻塞了，重复发送多次 SYN 报⽂，那么服务器在收到请求后就会 **建立多个冗余的无效链接，造成不必要的资源浪费**。

![image-20211211203334076](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112033197.png)![image-20211211203351547](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112033632.png)

​	即两次握⼿会造成消息滞留情况下，服务器重复接受⽆⽤的连接请求`SYN`报文，而造成重复分配资源。

###### 小结

​	TCP 建⽴连接时，通过三次握⼿**能防⽌历史连接的建⽴，能减少双⽅不必要的资源开销，能帮助双⽅同步初始化序列号**。序列号能够保证数据包不重复、不丢弃和按序传输。

不适用 [ 两次握手 ] 和 [ 四次握手 ] 原因：

- 「两次握⼿」：⽆法防⽌历史连接的建⽴，会造成双⽅资源的浪费，也⽆法可靠的同步双⽅序列号；
- 「四次握⼿」：三次握⼿就已经理论上最少可靠连接建⽴，所以不需要使⽤更多的通信次数。



##### 3.1.2.5 为什么客户端和服务端的初始序列号 ISN 是不相同的？

​	如果⼀个已经失效的连接被重⽤了，但是该旧连接的历史报⽂还残留在⽹络中，如果序列号相同，那么就⽆法分辨出该报⽂是不是历史报⽂，如果历史报⽂被新的连接接收了，则会产⽣数据错乱。所以，每次建⽴连接前重新初始化⼀个序列号主要是为了通信双⽅能够根据序号将不属于本连接的报⽂段丢弃。另⼀⽅⾯是为了安全性，防⽌⿊客伪造的相同序列号的 TCP 报⽂被对⽅接收。



##### 3.1.2.6 初始序列号ISN是如何随机产生的？

​	起始 ISN 是基于时钟的，每 4 毫秒 + 1，转⼀圈要 4.55 个⼩时。RFC1948 中提出了⼀个较好的初始化序列号 ISN 随机⽣成算法。

​	*ISN = M + F (localhost, localport, remotehost, remoteport)*

- **M**： 是一个计时器，这个计时器每隔4毫秒加1
- **F**： 是一个Hash算法，根据源IP、目的IP、源端口、目的端口生成一个随机数值。要保证Hash算法不能被轻易推算出，用MD5算法是一个比较好的选择



##### 3.1.2.7 既然IP会分片，为什么TCP层还需要MSS呢？

![image-20211211205252234](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112052314.png)

- `MTU`: 一个网络包的最大长度，以太网中一般为1500字节；
- `MSS`: 除去 IP 和 TCP 头部之后，⼀个⽹络包所能容纳的 TCP 数据的最⼤⻓度； 

如果在 TCP 的整个报⽂（头部 + 数据）交给 IP 层进⾏分⽚，会有什么异常呢？

​	当 IP 层有⼀个超过 MTU ⼤⼩的数据（TCP 头部 + TCP 数据）要发送，那么 IP 层就要进⾏分⽚，把数据分⽚成若⼲⽚，保证每⼀个分⽚都⼩于 MTU。把⼀份 IP 数据报进⾏分⽚以后，由⽬标主机的 IP 层来进⾏重新组装后，再交给上⼀层 TCP 传输层。这看起来井然有序，但这存在隐患的，**那么当如果⼀个** **IP** **分⽚丢失，整个IP报⽂的所有分⽚都得重传**

​	因为 IP 层本身没有超时重传机制，它由传输层的 TCP 来负责超时和重传。当接收⽅发现 TCP 报⽂（头部 + 数据）的某⼀⽚丢失后，则不会响应 ACK 给对⽅，那么发送⽅的 TCP 在超时后，就会重发「整个 TCP 报⽂（头部 + 数据）」。因此，可以得知由 IP 层进⾏分⽚传输，是⾮常没有效率的。

​	所以，为了达到最佳的传输效能 TCP 协议在**建⽴连接的时候通常要协商双⽅的MSS** **值**，当 TCP 层发现数据超过MSS 时，则就先会进⾏分⽚，当然由它形成的 IP 包的⻓度也就不会⼤于 MTU ，⾃然也就不⽤ IP 分⽚了。

![image-20211211205739073](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112057312.png)

​	经过 TCP 层分⽚后，如果⼀个 TCP 分⽚丢失后，进行重发时也是以MSS为单位，⽽不⽤重传所有的分⽚，⼤⼤ 增加了重传的效率

##### 3.1.2.8 什么是SYN攻击？如何避免SYN攻击？

​	我们都知道 TCP 连接建⽴是需要三次握⼿，假设攻击者短时间伪造不同 IP 地址的 SYN 报⽂，服务端每接收到⼀个 SYN 报⽂，就进⼊ SYN_RCVD 状态，但服务端发送出去的 ACK + SYN 报⽂，⽆法得到未知 IP 主机的ACK 应答，久⽽久之就会**占满服务端的 SYN 接收队列（未连接队列）**，使得服务器不能为正常⽤户服务。

![image-20211211210015588](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112100662.png)

###### 避免方式一

​	其中⼀种解决⽅式是通过修改 Linux 内核参数，控制队列⼤⼩和当队列满时应做什么处理。

- 当⽹卡接收数据包的速度⼤于内核处理的速度时，会有⼀个队列保存这些数据包。控制该队列的最⼤值如下参数:

```shell
net.core.netdev_max_backlog
```

- SYN_RCVD状态连接的最大个数：

```shell
net.ipv4.tcp_max_syn_backlog
```

- 超出处理时，对新的SYN直接返回RST, 丢弃连接：

```shell
net.ipv4.tcp_abort_on_overflow
```



###### 避免方式二

​	我们先来看下 Linux 内核的 `SYN` （未完成连接建⽴）队列与` Accpet` （已完成连接建⽴）队列是如何⼯作的？

![image-20211211210527720](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112105803.png)

正常流程：

1. 当服务端接收到客户端的 SYN 报⽂时，会将其加⼊到内核的「 SYN 队列」；
2. 接着发送 SYN + ACK 给客户端，等待客户端回应 ACK 报⽂；
3. 服务端接收到 ACK 报⽂后，从「 SYN 队列」移除放⼊到「 Accept 队列」；
4. 应用通过调用`accpet()`socket接口，从[ Accept队列 ]取出连接。

![image-20211211210722921](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112108408.png)



应⽤程序过慢：

- 如果应⽤程序过慢时，就会导致「 Accept 队列」被占满。



![image-20211211210842481](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112108556.png)

受到 SYN 攻击：

- 如果不断收到SYN攻击，就会导致 [ SYN队列 ]被占满。

tcp_syncookies 的⽅式可以应对 SYN 攻击的⽅法：

```shell
net.ipv4.tcp_syncookies = 1
```

![image-20211211211001809](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112110926.png)

-  「 SYN 队列」满之后，后续服务器收到 SYN 包，不进⼊「 SYN 队列」；
- 计算出⼀个 cookie 值，再以 SYN + ACK 中的「序列号」返回客户端
- 服务端接收到客户端的应答报⽂时，服务器会检查这个 ACK 包的合法性。如果合法，直接放⼊到「 Accept队列」。
- 最后应⽤通过调⽤ accpet() socket 接⼝，从「 Accept 队列」取出的连接。



#### 3.1.3 TCP连接断开

##### 3.1.3.1 TCP四次挥手过程和状态变迁

​	天下没有不散的宴席，对于 TCP 连接也是这样， TCP 断开连接是通过**四次挥⼿**⽅式。双⽅都可以主动断开连接，断开连接后主机中的「资源」将被释放。

![image-20211211211602262](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112116388.png)

1. 客户端打算关闭连接，此时会发送一个TCP首部`FIN`标志位被置为1的报文，也即FIN报文，之后客户端进入`FIN_WAIT_1`状态。
2. 服务端收到该报文后，就向客户端发送`ACK`应答报文，接着服务端进入`ClOSED_WAIT`状态
3. 客户端收到服务端的`ACK`应答报文后，之后进入`FIN_WAIT_2`的状态。
4. 等待服务端处理完数据后，也向客户端发送`FIN`报文，之后服务端进入`LAST_ACK`状态。
5. 客户端收到服务端的`FIN`报文后，回一个`ACK`应答报文，之后进入`TIME_WAIT`状态。
6. 服务端收到`ACK`应答报文后，就进入`CLOSED`状态，至此服务端已经完成连接的关闭。
7. 客户端在经过`2MSL`一段时间后，自动进入`CLOSED`状态，至此客户端也完成连接的关闭。

你可以看到，每个⽅向都需要**⼀个** **FIN** **和⼀个** **ACK**，因此通常被称为**四次挥⼿**。

注意：**主动关闭连接的，才有** **TIME_WAIT** **状态。**



##### 3.1.3.2 为什么挥手需要四次？

​	再来回顾下四次挥⼿双⽅发 FIN 包的过程，就能理解为什么需要四次了。

- 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示客户端不再发送数据了但是还能接收数据。
- 服务器收到客户端的 FIN 报⽂时，先回⼀个 ACK 应答报⽂，⽽服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 FIN 报⽂给客户端来表示同意现在关闭连接。

​	从上⾯过程可知，服务端通常需要等待完成数据的发送和处理，所以服务端的 ACK 和 FIN ⼀般都会分开发送，从⽽⽐三次握⼿导致多了⼀次。



##### 3.1.3.3 为什么 TIME_WAIT 等待的时间是 2MSL？

​	`MSL` 是 `Maximum Segment Lifetime`，**报⽂最⼤⽣存时间**，它是任何报⽂在⽹络上存在的最⻓时间，超过这个时间报⽂将被丢弃。因为 TCP 报⽂基于是 IP 协议的，⽽ IP 头中有⼀个 TTL 字段，是 IP 数据报可以经过的最⼤路由数，每经过⼀个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报⽂通知源主机。MSL 与 TTL 的区别： MSL 的单位是时间，⽽ TTL 是经过路由跳数。所以 **MSL应该要⼤于等于TTL消耗为0的时间**，以确保报⽂已被⾃然消亡.

​	TIME_WAIT 等待 2 倍的 MSL，⽐较合理的解释是：⽹络中可能存在来⾃发送⽅的数据包，当这些发送⽅的数据包被接收⽅处理后⼜会向对⽅发送响应，所以**⼀来⼀回需要等待2倍的时间**。⽐如如果被动关闭⽅没有收到断开连接的最后的 ACK 报⽂，就会触发超时重发 Fin 报⽂，另⼀⽅接收到 FIN 后，会重发 ACK 给被动关闭⽅， ⼀来⼀去正好 2 个 MSL。

​	2MSL 的时间是从**客户端接收到FIN后发送ACK开始计时的**。如果在 TIME-WAIT 时间内，因为客户端的 ACK没有传输到服务端，客户端⼜接收到了服务端重发的 FIN 报⽂，那么 **2MSL** **时间将重新计时**。



##### 3.1.3.4 为什么需要TIME_WAIT状态？

​	主动发起关闭连接的⼀⽅，才会有 TIME-WAIT 状态。需要 TIME-WAIT 状态，主要是两个原因：

- 防止具有相同 [ 相同四元组 ]的 [ 旧 ] 数据包被收到
- 保证 [ 被动关闭连接 ] 的一方能被正确的关闭，即保证最后的ACK能让被动关闭方接受，从而帮助其正常关闭。

###### 原因一： 防止旧连接的数据包

假设 TIME-WAIT 没有等待时间或时间过短，被延迟的数据包抵达后会发⽣什么呢？

![image-20211211213901598](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112140765.png)

- 如上图⻩⾊框框服务端在关闭连接之前发送的 SEQ = 301 报⽂，被⽹络延迟了。
- 这时有相同端⼝的 TCP 连接被复⽤后，被延迟的 SEQ = 301 抵达了客户端，那么客户端是有可能正常接收这个过期的报⽂，这就会产⽣数据错乱等严重的问题。

​	所以，TCP 就设计出了这么⼀个机制，经过 2MSL 这个时间，**足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再次出现的数据包一定都是新建立连接所产生的**。

###### 原因二： 保证连接的正确关闭

在 RFC 793 指出 TIME-WAIT 另⼀个重要的作⽤是：

*TIME-WAIT - represents waiting for enough time to pass to be sure the remote TCP received the acknowledgment of its connection termination request.*

​	也就是说，TIME-WAIT 作⽤是 **等待足够的时间以确保最后的ACK能让被动关闭方接受，从而帮助其正常关闭。**假设 TIME-WAIT 没有等待时间或时间过短，断开连接会造成什么问题呢？

![image-20211211214527941](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112112145046.png)

- 如上图红⾊框框客户端四次挥⼿的最后⼀个 ACK 报⽂如果在⽹络中被丢失了，此时如果客户端 TIME-WAIT 过短或没有，则就直接进⼊了 CLOSED 状态了，那么服务端则会⼀直处在 LASE_ACK 状态。

- 当客户端发起建⽴连接的 SYN 请求报⽂后，服务端会发送 RST 报⽂给客户端，连接建⽴的过程就会被

  终⽌。

如果 TIME-WAIT 等待⾜够⻓的情况就会遇到两种情况：

- 服务端正常收到四次挥⼿的最后⼀个 ACK 报⽂，则服务端正常关闭连接
- 服务端没有收到四次挥⼿的最后⼀个 ACK 报⽂时，则会᯿发 FIN 关闭连接报⽂并等待新的 ACK 报⽂。

所以客户端在 TIME-WAIT 状态等待 2MSL 时间后，就可以**保证双⽅的连接都可以正常的关闭。**

##### 3.1.3.5 TIME_WAIT 过多有什么危害？

​	如果服务器有处于 TIME-WAIT 状态的 TCP，则说明是由服务器⽅主动发起的断开请求。过多的 TIME-WAIT 状态主要的危害有两种：

1. 内存资源占⽤；
2. 对端⼝资源的占⽤，⼀个 TCP 连接⾄少消耗⼀个本地端⼝；

​	第⼆个危害是会造成严᯿的后果的，要知道，端⼝资源也是有限的，⼀般可以开启的端⼝为 32768～61000 ，也可以通过如下参数设置指定：

```shell
net.ipv4.ip_local_port_range
```

**如果发起连接⼀⽅的** **TIME_WAIT** **状态过多，占满了所有端⼝资源，则会导致⽆法创建新连接。**

客户端受端⼝资源限制：

- 客户端TIME_WAIT过多，就会导致端⼝资源被占⽤，因为端⼝就65536个，被占满就会导致⽆法创建新的连接。

服务端受系统资源限制：

- 由于⼀个四元组表示 TCP 连接，理论上服务端可以建⽴很多连接，服务端确实只监听⼀个端⼝ 但是会把连接扔给处理线程，所以理论上监听的端⼝可以继续监听。但是线程池处理不了那么多⼀直不断的连接了。所以当服务端出现⼤量 TIME_WAIT 时，系统资源被占满时，会导致处理不过来新的连接。

##### 3.1.3.6 如果已经建立了连接，但是客户端突然出现故障了怎么办？

TCP 有⼀个机制是**保活机制**。这个机制的原理是这样的：

​	定义⼀个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作⽤，每隔⼀个时间间隔，发送⼀个探测报⽂，该探测报⽂包含的数据⾮常少，如果连续⼏个探测报⽂都没有得到响应，则认为当前的TCP 连接已经死亡，系统内核将错误信息通知给上层应⽤程序。

如果开启了 TCP 保活，需要考虑以下⼏种情况：

1. 对端程序是正常⼯作的。当 TCP 保活的探测报⽂发送给对端, 对端会正常响应，这样 **TCP** **保活时间会被重置**，等待下⼀个 TCP 保活时间的到来

2. 对端程序崩溃并重启。当 TCP 保活的探测报⽂发送给对端后，对端是可以响应的，但由于没有该连接的有效信息，**会产⽣⼀个** **RST** **报⽂**，这样很快就会发现 TCP 连接已经被重置。

3. 对端程序崩溃，或对端由于其他原因导致报⽂不可达。当 TCP 保活的探测报⽂发送给对端后，⽯沉⼤海，没有响应，连续⼏次，达到保活探测次数后，**TCP** **会报告该** **TCP** **连接已经死亡**。

   

   


### 3.2  TCP重传、滑动窗口、流量控制、拥塞控制

#### 3.2.1 重传机制

​	TCP 实现可靠传输的⽅式之⼀，是通过序列号与确认应答。在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回⼀个确认应答消息，表示已收到消息。

![image-20211212092855484](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112120929837.png)

​	但在错综复杂的⽹络，并不⼀定能如上图那么顺利能正常的数据传输，万⼀数据在传输过程中丢失了呢？所以 TCP 针对数据包丢失的情况，会⽤ **重传机制** 解决。 

常见的重传机制：

- 超时重传
- 快速重传
- SACK
- D-SACK



##### 3.2.1.1 超时重传

​	重传机制的其中⼀个⽅式，就是在发送数据时，设定⼀个定时器，当超过指定的时间后，没有收到对⽅的 ACK确认应答报⽂，就会重发该数据，也就是我们常说的**超时重传**。

TCP 会在以下两种情况发⽣超时重传：

- 数据包丢失
- 确认应答丢失

![image-20211212093242740](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112120932932.png)



> 超时时间应设置为多少呢？

![image-20211212093346764](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112120933925.png)

​		我们先来了解⼀下什么是**`RTT`（Round-Trip Time 往返时延）**，]由上图可知，`RTT` 就是**数据从⽹络⼀端传送到另⼀端所需的时间**，也就是包的往返时间。超时重传时间是以 RTO （Retransmission Timeout 超时重传时间）表示。

假设在重传的情况下，超时时间 RTO 「较⻓或较短」时，会发⽣什么事情呢？

![image-20211212093601288](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112120936431.png)

上图中有两种超时时间不同的情况：

- 当超时时间 **RTO** **较⼤**时，重发就慢，丢了⽼半天才重发，没有效率，性能差；
- 当超时时间 **RTO** **较⼩**时，会导致可能并没有丢就重发，于是重发的就快，会增加⽹络拥塞，导致更多的超时，更多的超时导致更多的重发。

​	精确的测量超时时间 RTO 的值是⾮常重要的，这可让我们的重传机制更⾼效。根据上述的两种情况，我们可以得知，**超时重传时间** **RTO** **的值应该略⼤于报⽂往返** **RTT** **的值**。

![image-20211212093904775](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112120939202.png)

​	⾄此，可能⼤家觉得超时重传时间 RTO 的值计算，也不是很复杂嘛。好像就是在发送端发包时记下 t0 ，然后接收端再把这个 ack 回来时再记⼀个 t1 ，于是 RTT = t1 – t0 。没那么简单，**这只是⼀个采样，不能代表普遍情况**。实际上「报⽂往返 RTT 的值」是经常变化的，因为我们的⽹络也是时常变化的。也就因为「报⽂往返 RTT 的值」是经常波动变化的，所以「超时重传时间 RTO 的值」应该是⼀个**动态变化的值**。

​	我们来看看 Linux 是如何计算 RTO 的呢？估计往返时间，通常需要采样以下两个：

- 需要 TCP 通过采样 RTT 的时间，然后进⾏加权平均，算出⼀个平滑 RTT 的值，⽽且这个值还是要不断变化的，因为⽹络状况不断地变化。
- 除了采样 RTT，还要采样 RTT 的波动范围，这样就避免如果 RTT 有⼀个⼤的波动的话，很难被发现的情况

RFC6289 建议使⽤以下的公式计算 RTO：

![image-20211212094142205](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112120941428.png)

​	其中 SRTT 是计算平滑的RTT ， DevRTR 是计算平滑的RTT 与 最新 RTT 的差距。在 Linux 下，**α = 0.125**，**β = 0.25**，**μ = 1**，**∂ = 4**。别问怎么来的，问就是⼤量实验中调出来的。

​	如果超时重发的数据，再次超时的时候，⼜需要重传的时候，TCP 的策略是**超时时间隔加倍。**也就是 **每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送**。

​	超时触发重传存在的问题是，超时周期可能相对较⻓。那是不是可以有更快的⽅式呢？于是就可以⽤「快速重传」机制来解决超时重发的时间等待。

##### 3.2.1.2 快速重传

​	TCP 还有另外⼀种**快速重传（Fast Retransmit）机制**，它**不以时间为驱动，⽽是以数据驱动重传**。快速重传机制，是如何⼯作的呢？

![image-20211212094717234](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112120947400.png)

在上图，发送⽅发出了 1，2，3，4，5 份数据：

1. 第一份Seq1先送到了，于是就Ack回2；
2. 结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2；
3. 后面的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到；
4. **发送端收到三个Ack=2的确认，知道了Seq2还没收到，就会在定时器过期之前，重传丢失的Seq2**。
5. 最后，收到了Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。 所以，快速重传的⼯作⽅式是当收到三个相同的   ACK   报⽂时，会在定时器过期之前，重传丢失的报⽂段。

​	快速重传机制只解决了⼀个问题，就是超时时间的问题，但是它依然⾯临着另外⼀个问题。就是 **重传的时候，是重传之前的一个，还是重传所有的问题**。

​	⽐如对于上⾯的例⼦，是重传 Seq2 呢？还是重传 Seq2、Seq3、Seq4、Seq5 呢？因为发送端并不清楚这连续的 三个 Ack 2 是谁传回来的。根据 TCP 不同的实现，以上两种情况都是有可能的。可⻅，这是⼀把双刃剑。为了解决不知道该重传哪些 TCP 报⽂，于是就有SACK⽅法。

##### 3.2.1.3 SACK

​	还有⼀种实现重传机制的⽅式叫： SACK （ Selective Acknowledgment 选择性确认）。这种⽅式需要在 TCP 头部「选项」字段⾥加⼀个 SACK 的东⻄，它**可以将缓存的地图发送给发送⽅**，这样发送⽅就可以知道哪些数据收到了，哪些数据没收到，知道了这些信息，就可以**只重传丢失的数据**。

​	如下图，发送⽅收到了三次同样的 ACK 确认报⽂，于是就会触发快速重发机制，通过 SACK 信息发现只有200~299 这段数据丢失，则重发时，就只选择了这个 TCP 段进⾏重发。

![image-20211212095603283](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112120956560.png)

如果要⽀持 SACK ，必须双⽅都要⽀持。在 Linux 下，可以通过 net.ipv4.tcp_sack 参数打开这个功能（Linux2.4 后默认打开）

##### 3.2.1.4 D-SACK

​	Duplicate SACK ⼜称 D-SACK ，其主要**使⽤了** **SACK** **来告诉「发送⽅」有哪些数据被重复接收了。**下⾯举例两个栗⼦，来说明 D-SACK 的作⽤。

###### 例子一： ACK丢包

![image-20211212095828014](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112120958168.png)

- 「接收⽅」发给「发送⽅」的两个 ACK 确认应答都丢失了，所以发送⽅超时后，重传第⼀个数据包（3000 ~3499）
- **于是「接收⽅」发现数据是重复收到的，于是回了⼀个** **SACK = 3000~3500**，告诉「发送⽅」 3000~3500的数据早已被接收了，因为 ACK 都到了 4000 了，已经意味着 4000 之前的所有数据都已收到，所以这个SACK 就代表着 D-SACK 。
- 这样「发送⽅」就知道了，数据没有丢，是「接收⽅」的 ACK 确认报⽂丢了。

###### 例子二：网络延时

![image-20211212100035388](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121000648.png)

- 数据包（1000~1499） 被⽹络延迟了，导致「发送⽅」没有收到 Ack 1500 的确认报⽂。
- ⽽后⾯报⽂到达的三个相同的 ACK 确认报⽂，就触发了快速重传机制，但是在重传后，被延迟的数据包（1000~1499）⼜到了「接收⽅」；
- **所以接收方回了一个SACK=1000~1500, 因为ACK已经到了3000, 所以这个SACK是D-SACK，表示收到了重复的包。**
- 这样发送⽅就知道快速重传触发的原因不是发出去的包丢了，也不是因为回应的 ACK 包丢了，⽽是因为⽹络延迟了。

可⻅， D-SACK 有这么⼏个好处：

1. 可以让「发送⽅」知道，是发出去的包丢了，还是接收⽅回应的 ACK 包丢了;
2.  可以知道是不是「发送⽅」的数据包被⽹络延迟了;
3.  可以知道⽹络中是不是把「发送⽅」的数据包给复制了;

在 Linux 下可以通过 net.ipv4.tcp_dsack 参数开启/关闭这个功能（Linux 2.4 后默认打开）。



#### 3.2.2 滑动窗口

##### 3.2.2.1 引入窗口概念的原因

​	我们都知道 TCP 是每发送⼀个数据，都要进⾏⼀次确认应答。当上⼀个数据包收到了应答了， 再发送下⼀个。这个模式就有点像我和你⾯对⾯聊天，你⼀句我⼀句。但这种⽅式的缺点是效率⽐较低的。如果你说完⼀句话，我在处理其他事情，没有及时回复你，那你不是要⼲等着我做完其他事情后，我回复你，你才能说下⼀句话，很显然这不现实。所以，这样的传输⽅式有⼀个缺点：数据包的**往返时间越⻓，通信的效率就越低**。

![image-20211212111415470](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121114561.png)

​	为解决这个问题，TCP 引⼊了**窗⼝**这个概念。即使在往返时间较⻓的情况下，它也不会降低⽹络通信的效率。那么有了窗⼝，就可以指定窗⼝⼤⼩，窗⼝⼤⼩就是指**⽆需等待确认应答，⽽可以继续发送数据的最⼤值**。窗⼝的实现实际上是操作系统开辟的⼀个缓存空间，发送⽅主机在等到确认应答返回之前，必须在缓冲区中保留已发送的数据。如果按期收到确认应答，此时数据就可以从缓存区清除。

​	假设窗⼝⼤⼩为 3 个 TCP 段，那么发送⽅就可以「连续发送」 3 个 TCP 段，并且中途若有 ACK 丢失，可以通过「下⼀个确认应答进⾏确认」。如下图：

![image-20211212111617262](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121116356.png)

​	图中的 ACK 600 确认应答报⽂丢失，也没关系，因为可以通过下⼀个确认应答进⾏确认，只要发送⽅收到了 ACK700 确认应答，就意味着 700 之前的所有数据「接收⽅」都收到了。这个模式就叫**累计确认**或者**累计应答**。

##### 3.2.2.2 窗口大小由哪一方决定？

​	TCP 头⾥有⼀个字段叫 Window ，也就是窗⼝⼤⼩。**这个字段是接收段告诉发送端自己还有多少缓存区可以接受数据。于是发送端就可以根据这个接收端的处理能来发送数据，而不是导致接受端处理不过来。**所以，通常窗⼝的⼤⼩是由接收⽅的窗⼝⼤⼩来决定的。发送⽅发送的数据⼤⼩不能超过接收⽅的窗⼝⼤⼩，否则接收⽅就⽆法正常接收到数据。

##### 3.2.2.3 发送方的滑动窗口

​	我们先来看看发送⽅的窗⼝，下图就是发送⽅缓存的数据，根据处理的情况分成四个部分，其中深蓝⾊⽅框是发送窗⼝，紫⾊⽅框是可⽤窗⼝：

![image-20211212112113981](https://gitee.com/ljcdzh/my_pic/raw/master/img/20211212112106

部」都⼀下发送出去后，可⽤窗⼝的⼤⼩就为   0   了，表明可⽤窗⼝耗尽，在没收到ACK 确认之前是⽆法继续发送数据了。

![image-20211212112411509](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121124582.png)

​	在下图，当收到之前发送的数据 `32~36 `字节的 ACK 确认应答后，如果发送窗⼝的⼤⼩没有变化，则**滑动窗⼝往右边移动5个字节，因为有 5 个字节的数据被应答确认**，接下来 52~56 字节⼜变成了可⽤窗⼝，那么后续也就可以发送 `52~56 `这 5 个字节的数据了。

![image-20211212112507045](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121125118.png)

##### 3.2.2.4 程序是如何表示发送方的四个部分的呢？

​	TCP 滑动窗⼝⽅案使⽤三个指针来跟踪在四个传输类别中的每⼀个类别中的字节。其中两个指针是绝对指针（指特定的序列号），⼀个是相对指针（需要做偏移）。

![image-20211212112621951](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121126031.png)

- `SND.WND`: 表示发送窗⼝的⼤⼩（⼤⼩是由接收⽅指定的）；

- `SND.UNA`: 是⼀个绝对指针，它指向的是已发送但未收到确认的第⼀个字节的序列号，也就是 #2 的第⼀

  个字节。

- `SND.NXT`: 也是⼀个绝对指针，它指向未发送但可发送范围的第⼀个字节的序列号，也就是 #3 的第⼀个字节。

- 指向 #4 的第⼀个字节是个相对指针，它需要 SND.UNA 指针加上 SND.WND ⼤⼩的偏移量，就可以指向

  \#4 的第⼀个字节了

那么可⽤窗⼝⼤⼩的计算就可以是：

**可用窗口大小 = SND.WND - (SND.NXT - SND.UNA)**

##### 3.2.2.5 接受方的滑动窗口

![image-20211212113051138](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121130215.png)

接下来我们看看接收⽅的窗⼝，接收窗⼝相对简单⼀些，根据处理的情况划分成三个部分：

- `#1 + #2`: 已成功接收并确认的数据（等待应⽤进程读取）；
- `#3`: 未收到数据但可以接收的数据；
- `#4`: 未收到数据并不可以接收的数据；

其中三个接收部分，使⽤两个指针进⾏划分:

- `RCV.WND `：表示接收窗⼝的⼤⼩，它会通告给发送⽅。

- `RCV.NXT` ：是⼀个指针，它指向期望从发送⽅发送来的下⼀个数据字节的序列号，也就是 #3 的第⼀个字节

- 指向 #4 的第⼀个字节是个相对指针，它需要 RCV.NXT 指针加上 RCV.WND ⼤⼩的偏移量，就可以指向

  \#4 的第⼀个字节了

##### 3.2.2.6 接受窗口和发送窗口的大小是相等的吗？

​	并不是完全相等，接收窗⼝的⼤⼩是**约等于**发送窗⼝的⼤⼩的。因为滑动窗⼝并不是⼀成不变的。⽐如，当接收⽅的应⽤进程读取数据的速度⾮常快的话，这样的话接收窗⼝可以很快的就空缺出来。那么新的接收窗⼝⼤⼩，是通过 TCP 报⽂中的 Windows 字段来告诉发送⽅。那么这个传输过程是存在时延的，所以接收窗⼝和发送窗⼝是约等于的关系。

#### 3.2.3 流量控制

​	发送⽅不能⽆脑的发数据给接收⽅，要考虑接收⽅处理能⼒。如果⼀直⽆脑的发数据给对⽅，但对⽅处理不过来，那么就会导致触发重发机制，从⽽导致⽹络流量的⽆端的浪费。为了解决这种现象发⽣，**TCP提供一种可以让[ 发送方 ] 根据 [ 接受方 ]的实际接收能力控制发送的数据量，这就是所谓的流量控制。**

下⾯举个栗⼦，为了简单起⻅，假设以下场景：

- 客户端是接收⽅，服务端是发送⽅
- 假设接收窗⼝和发送窗⼝相同，都为 `200`
- 假设两个设备在整个传输过程中都保持相同的窗⼝⼤⼩，不受外界影响

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121353902.webp)



根据上图的流量控制，说明下每个过程：

1. 客户端向服务端发送请求数据报⽂。这⾥要说明下，本次例⼦是把服务端作为发送⽅，所以没有画出服务端的接收窗⼝。
2. 服务端收到请求报⽂后，发送确认报⽂和 80 字节的数据，于是可⽤窗⼝` Usable `减少为 120 字节，同时`SND.NXT` 指针也向右偏移 80 字节后，指向 321，**这意味着下次发送数据的时候，序列号是321**。
3. 客户端收到 80 字节数据后，于是接收窗⼝往右移动 80 字节， RCV.NXT 也就指向 321，**这意味着客户端期望的下⼀个报⽂的序列号是321**，接着发送确认报⽂给服务端。
4.  服务端再次发送了 120 字节数据，于是可⽤窗⼝耗尽为 0，服务端⽆法再继续发送数据。
5.  客户端收到 120 字节的数据后，于是接收窗⼝往右移动 120 字节， RCV.NXT 也就指向 441，接着发送确认报⽂给服务端。
6. 服务端收到对 80 字节数据的确认报⽂后， SND.UNA 指针往右偏移后指向 321，于是可⽤窗⼝ Usable增⼤到 80。
7.  服务端收到对 120 字节数据的确认报⽂后， SND.UNA 指针往右偏移后指向 441，于是可⽤窗⼝ Usable增⼤到 200。
8.  服务端可以继续发送了，于是发送了 160 字节的数据后， SND.NXT 指向 601，于是可⽤窗⼝ Usable 减少到 40。
9. 客户端收到 160 字节后，接收窗⼝往右移动了 160 字节， RCV.NXT 也就是指向了 601，接着发送确认报⽂给服务端。
10. 服务端收到对 160 字节数据的确认报⽂后，发送窗⼝往右移动了 160 字节，于是 SND.UNA 指针偏移了160 后指向 601，可⽤窗⼝ Usable 也就增⼤⾄了 200。



##### 3.2.3.1 操作系统缓冲区和滑动窗口的关系

前⾯的流量控制例⼦，我们假定了发送窗⼝和接收窗⼝是不变的，但是实际上，发送窗⼝和接收窗⼝中所存放的字节数，都是放在操作系统内存缓冲区中的，⽽操作系统的缓冲区，会**被操作系统调整**。当应⽤进程没办法及时读取缓冲区的内容时，也会对我们的缓冲区造成影响。

> 那操作系统的缓冲区，是如何影响发送窗口和接收窗口呢？

###### 举例一

​	当应⽤程序没有及时读取缓存时，发送窗⼝和接收窗⼝的变化。考虑以下场景：

- 客户端作为发送⽅，服务端作为接收⽅，发送窗⼝和接收窗⼝初始⼤⼩为 360 ；
- 服务端⾮常的繁忙，当收到客户端的数据时，应⽤层不能及时读取数据。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121355412.webp)

根据上图的流量控制，说明下每个过程：

1. 客户端发送 140 字节数据后，可用窗口变为 220 （360 - 140）。
2. 服务端收到 140 字节数据，**但是服务端非常繁忙，应用进程只读取了 40 个字节，还有 100 字节占用着缓冲区，于是接收窗口收缩到了 260 （360 - 100）**，最后发送确认信息时，将窗口大小通过给客户端。
3. 客户端收到确认和窗口通告报文后，发送窗口减少为 260。
4. 客户端发送 180 字节数据，此时可用窗口减少到 80。
5. 服务端收到 180 字节数据，**但是应用程序没有读取任何数据，这 180 字节直接就留在了缓冲区，于是接收窗口收缩到了 80 （260 - 180）**，并在发送确认信息时，通过窗口大小给客户端。
6. 客户端收到确认和窗口通告报文后，发送窗口减少为 80。
7. 客户端发送 80 字节数据后，可用窗口耗尽。
8. 服务端收到 80 字节数据，**但是应用程序依然没有读取任何数据，这 80 字节留在了缓冲区，于是接收窗口收缩到了 0**，并在发送确认信息时，通过窗口大小给客户端。
9. 客户端收到确认和窗口通告报文后，发送窗口减少为 0。

​	可见最后窗口都收缩为 0 了，也就是发生了窗口关闭。当发送方可用窗口变为 0 时，发送方实际上会定时发送窗口探测报文，以便知道接收方的窗口是否发生了改变，这个内容后面会说，这里先简单提一下。



###### 举例二

​	当服务端系统资源非常紧张的时候，操心系统可能会直接减少了接收缓冲区大小，这时应用程序又无法及时读取缓存数据，那么这时候就有严重的事情发生了，会出现数据包丢失的现象。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121400840.webp)

说明每一个过程：

1. 客户端发送 140 字节的数据，于是可用窗口减少到了 220。
2. **服务端因为现在非常的繁忙，操作系统于是就把接收缓存减少了 100 字节，当收到 对 140 数据确认报文后，又因为应用程序没有读取任何数据，所以 140 字节留在了缓冲区中，于是接收窗口大小从 360 收缩成了 100**，最后发送确认信息时，通告窗口大小给对方。
3. 此时客户端因为还没有收到服务端的通告窗口报文，所以不知道此时接收窗口收缩成了 100，客户端只会看自己的可用窗口还有 220，所以客户端就发送了 180 字节数据，于是可用窗口减少到 40。
4. 服务端收到了 180 字节数据时，**发现数据大小超过了接收窗口的大小，于是就把数据包丢失了。**
5. 客户端收到第 2 步时，服务端发送的确认报文和通告窗口报文，尝试减少发送窗口到 100，把窗口的右端向左收缩了 80，此时可用窗口的大小就会出现诡异的负值。

所以，如果发生了先减少缓存，再收缩窗口，就会出现丢包的现象。**为了防止这种情况发生，TCP 规定是不允许同时减少缓存又收缩窗口的，而是采用先收缩窗口，过段时间在减少缓存，这样就可以避免了丢包情况。**

##### 3.2.3.2 窗口关闭

​	在前面我们都看到了，TCP 通过让接收方指明希望从发送方接收的数据大小（窗口大小）来进行流量控制。**如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止，这就是窗口关闭。**

> 窗口关闭潜在的危险

​	接收方向发送方通告窗口大小时，是通过 `ACK` 报文来通告的。那么，当发生窗口关闭时，接收方处理完数据后，会向发送方通告一个窗口非 0 的 ACK 报文，如果这个通告窗口的 ACK 报文在网络中丢失了，那麻烦就大了。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121406314.webp)

​	这会导致发送方一直等待接收方的非 0 窗口通知，接收方也一直等待发送方的数据，如不不采取措施，这种相互等待的过程，会造成了死锁的现象。

> 　TCP 是如何解决窗口关闭时，潜在的死锁现象呢？

​	为了解决这个问题，TCP 为每个连接设有一个持续定时器，**只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。**如果持续计时器超时，就会发送**窗口探测 ( Window probe ) 报文**，而对方在确认这个探测报文时，给出自己现在的接收窗口大小。

![窗口探测](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121408261.webp)

- 如果接收窗口仍然为 0，那么收到这个报文的一方就会重新启动持续计时器；
- 如果接收窗口不是 0，那么死锁的局面就可以被打破了。

​	窗口探查探测的次数一般为 3 此次，每次次大约 30-60 秒（不同的实现可能会不一样）。如果 3 次过后接收窗口还是 0 的话，有的 TCP 实现就会发 `RST` 报文来中断连接。

##### 3.2.3.3 糊涂窗口综合症

​	如果接收方太忙了，来不及取走接收窗口里的数据，那么就会导致发送方的发送窗口越来越小。到最后，**如果接收方腾出几个字节并告诉发送方现在有几个字节的窗口，而发送方会义无反顾地发送这几个字节，这就是糊涂窗口综合症**。要知道，我们的 `TCP + IP` 头有 `40` 个字节，为了传输那几个字节的数据，要达上这么大的开销，这太不经济了。就好像一个可以承载 50 人的大巴车，每次来了一两个人，就直接发车。除非家里有矿的大巴司机，才敢这样玩，不然迟早破产。要解决这个问题也不难，大巴司机等乘客数量超过了 25 个，才认定可以发车。

现举个糊涂窗口综合症的栗子，考虑以下场景：

​	接收方的窗口大小是 360 字节，但接收方由于某些原因陷入困境，假设接收方的应用层读取的能力如下：

- 接收方每接收 3 个字节，应用程序就只能从缓冲区中读取 1 个字节的数据；
- 在下一个发送方的 TCP 段到达之前，应用程序还从缓冲区中读取了 40 个额外的字节；

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121411251.webp)

​	每个过程的窗口大小的变化，在图中都描述的很清楚了，可以发现窗口不断减少了，并且发送的数据都是比较小的了。所以，糊涂窗口综合症的现象是可以发生在发送方和接收方：

- 接收方可以通告一个小的窗口
- 而发送方可以发送小数据

于是，要解决糊涂窗口综合症，就解决上面两个问题就可以了

- 让接收方不通告小窗口给发送方
- 让发送方避免发送小数据

> 怎么让接收方不通告小窗口呢？

接收方通常的策略如下:

​	当「窗口大小」小于 min( MSS，缓存空间/2 ) ，也就是小于 MSS 与 1/2 缓存大小中的最小值时，就会向发送方通告窗口为 `0`，也就阻止了发送方再发数据过来。等到接收方处理了一些数据后，窗口大小 >= MSS，或者接收方缓存空间有一半可以使用，就可以把窗口打开让发送方发送数据过来。

> 怎么让发送方避免发送小数据呢？

发送方通常的策略:

​	使用 Nagle 算法，该算法的思路是延时处理，它满足以下两个条件中的一条才可以发送数据：

- 要等到窗口大小 >= `MSS` 或是 数据大小 >= `MSS`
- 收到之前发送数据的 `ack` 回包

​	只要没满足上面条件中的一条，发送方一直在囤积数据，直到满足上面的发送条件。另外，Nagle 算法默认是打开的，如果对于一些需要小数据包交互的场景的程序，比如，telnet 或 ssh 这样的交互性比较强的程序，则需要关闭 Nagle 算法。

​	可以在 Socket 设置 `TCP_NODELAY` 选项来关闭这个算法（关闭 Nagle 算法没有全局参数，需要根据每个应用自己的特点来关闭）

```shell
setsockopt(sock_fd, IPPROTO_TCP, TCP_NODELAY, (char *)&value, sizeof(int));
```



#### 3.2.4 拥塞控制

> 为什么要有拥塞控制呀，不是有流量控制了吗？

​	前面的流量控制是避免「发送方」的数据填满「接收方」的缓存，但是并不知道网络的中发生了什么。一般来说，计算机网络都处在一个共享的环境。因此也有可能会因为其他主机之间的通信使得网络拥堵。

​	**在网络出现拥堵时，如果继续发送大量数据包，可能会导致数据包时延、丢失等，这时 TCP 就会重传数据，但是一重传就会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，这个情况就会进入恶性循环被不断地放大….**

​	所以，TCP 不能忽略网络上发生的事，它被设计成一个无私的协议，当网络发送拥塞时，TCP 会自我牺牲，降低发送的数据量。

​	于是，就有了**拥塞控制**，控制的目的就是**避免「发送方」的数据填满整个网络。**为了在「发送方」调节所要发送数据的量，定义了一个叫做「**拥塞窗口**」的概念。

> 什么是拥塞窗口？和发送窗口有什么关系呢？

​	**拥塞窗口 cwnd**是发送方维护的一个 的状态变量，它会根据**网络的拥塞程度动态变化的**。我们在前面提到过发送窗口 `swnd` 和接收窗口 `rwnd` 是约等于的关系，那么由于入了拥塞窗口的概念后，此时发送窗口的值是swnd = min(cwnd, rwnd)，也就是拥塞窗口和接收窗口中的最小值。

拥塞窗口 `cwnd` 变化的规则：

- 只要网络中没有出现拥塞，`cwnd` 就会增大；
- 但网络中出现了拥塞，`cwnd` 就减少；

> 那么怎么知道当前网络是否出现了拥塞呢？

​	其实只要「发送方」没有在规定时间内接收到 ACK 应答报文，也就是**发生了超时重传，就会认为网络出现了拥塞。**

> 拥塞控制有哪些控制算法？

拥塞控制主要是四个算法：

- 慢启动
- 拥塞避免
- 拥塞发生
- 快速恢复

##### 3.2.4.1 慢启动

​	TCP 在刚建立连接完成后，首先是有个慢启动的过程，这个慢启动的意思就是一点一点的提高发送数据包的数量，如果一上来就发大量的数据，这不是给网络添堵吗？

​	慢启动的算法记住一个规则就行：**当发送方每收到一个 ACK，就拥塞窗口 cwnd 的大小就会加 1。**

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121421443.webp)

这里假定拥塞窗口 `cwnd` 和发送窗口 `swnd` 相等 :  

- 连接建立完成后，一开始初始化 `cwnd = 1`，表示可以传一个 `MSS` 大小的数据。
- 当收到一个 ACK 确认应答后，cwnd 增加 1，于是一次能够发送 2 个
- 当收到 2 个的 ACK 确认应答后， cwnd 增加 2，于是就可以比之前多发2 个，所以这一次能够发送 4 个
- 当这 4 个的 ACK 确认到来的时候，每个确认 cwnd 增加 1， 4 个确认 cwnd 增加 4，于是就可以比之前多发 4 个，所以这一次能够发送 8 个。

可以看出慢启动算法，发包的个数是**指数性的增长**。

> 那慢启动涨到什么时候是个头呢？

有一个叫慢启动门限  `ssthresh` （slow start threshold）状态变量。

- 当 `cwnd < ssthresh` 时，使用慢启动算法。
- 当 `cwnd >= ssthresh` 时，就会使用「拥塞避免算法」。



##### 3.2.4.2 拥塞避免

​	前面说道，当拥塞窗口 `cwnd` 「超过」慢启动门限 `ssthresh` 就会进入拥塞避免算法。一般来说 `ssthresh` 的大小是 `65535` 字节。那么进入拥塞避免算法后，它的规则是：**每当收到一个 ACK 时，cwnd 增加 1/cwnd。**

接上前面的慢启动的栗子，现假定 `ssthresh` 为 `8`：

​	当 8 个 ACK 应答确认到来时，每个确认增加 1/8，8 个 ACK 确认 cwnd 一共增加 1，于是这一次能够发送 9 个 `MSS` 大小的数据，变成了**线性增长。**

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121425809.webp)

​	所以，我们可以发现，拥塞避免算法就是将原本慢启动算法的指数增长变成了线性增长，还是增长阶段，但是增长速度缓慢了一些。就这么一直增长着后，网络就会慢慢进入了拥塞的状况了，于是就会出现丢包现象，这时就需要对丢失的数据包进行重传。当触发了重传机制，也就进入了「拥塞发生算法」。

##### 3.2.4.3 拥塞发生

​	当网络出现拥塞，也就是会发生数据包重传，重传机制主要有两种：

- 超时重传
- 快速重传

这两种使用的拥塞发送算法是不同的，接下来分别来说说。

> 发生超时重传的拥塞发生算法

当发生了「超时重传」，则就会使用拥塞发生算法。

这个时候，sshresh 和 cwnd 的值会发生变化：

- `ssthresh` 设为 `cwnd/2`，
- `cwnd` 重置为 `1`

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121427731.webp)

​	接着，就重新开始慢启动，慢启动是会突然减少数据流的。这真是一旦「超时重传」，马上回到解放前。但是这种方式太激进了，反应也很强烈，会造成网络卡顿。就好像本来在秋名山高速漂移着，突然来个紧急刹车，轮胎受得了吗。。。

> 发生快速重传的拥塞发生算法

​	还有更好的方式，前面我们讲过「快速重传算法」。当接收方发现丢了一个中间包的时候，发送三次前一个包的 ACK，于是发送端就会快速地重传，不必等待超时再重传。

TCP 认为这种情况不严重，因为大部分没丢，只丢了一小部分，则 `ssthresh` 和 `cwnd` 变化如下：

- `cwnd = cwnd/2` ，也就是设置为原来的一半;
- `ssthresh = cwnd`;
- 进入快速恢复算法



##### 3.2.4.4 快速恢复

​	快速重传和快速恢复算法一般同时使用，快速恢复算法是认为，你还能收到 3 个重复 ACK 说明网络也不那么糟糕，所以没有必要像 `RTO` 超时那么强烈。

​	正如前面所说，进入快速恢复之前，`cwnd` 和 `ssthresh` 已被更新了：

- `cwnd = cwnd/2` ，也就是设置为原来的一半;
- `ssthresh = cwnd`;

然后，进入快速恢复算法如下：

- 拥塞窗口 `cwnd = ssthresh + 3` （ 3 的意思是确认有 3 个数据包被收到了）
- 重传丢失的数据包
- 如果再收到重复的 ACK，那么 cwnd 增加 1
- 如果收到新数据的 ACK 后，设置 cwnd 为 ssthresh，接着就进入了拥塞避免算法

![快速重传和快速恢复](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121430475.webp)

也就是没有像「超时重传」一夜回到解放前，而是还在比较高的值，后续呈线性增长。



## 四、IP篇

### 4.1 IP基本认识

#### 4.1.1 IP的作用

​	IP 在 TCP/IP 参考模型中处于第三层，也就是**网络层**。网络层的主要作用是：**实现主机与主机之间的通信，也叫点对点（end to end）通信。**

![IP作用](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112131026988.webp)

#### 4.1.2 IP和MAC的关系

​	有的小伙伴分不清 IP（网络层） 和 MAC （数据链路层）之间的区别和关系。其实很容易区分，在上面我们知道 IP 的作用是主机之间通信中的，而 **MAC 的作用则是实现「直连」的两个设备之间通信，而 IP 则负责在「没有直连」的两个网络之间进行通信传输。**

​	举个生活的栗子，小林要去一个很远的地方旅行，制定了一个行程表，其间需先后乘坐飞机、地铁、公交车才能抵达目的地，为此小林需要买飞机票，地铁票等。飞机票和地铁票都是去往特定的地点的，每张票只能够在某一限定区间内移动，此处的「区间内」就如同通信网络中数据链路。在区间内移动相当于数据链路层，充当区间内两个节点传输的功能，区间内的出发点好比源 MAC 地址，目标地点好比目的 MAC 地址。整个旅游行程表就相当于网络层，充当远程定位的功能，行程的开始好比源 IP，行程的终点好比目的 IP 地址。

![IP的作用与MAC的作用](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112131038758.webp)

​	如果小林只有行程表而没有车票，就无法搭乘交通工具到达目的地。相反，如果除了车票而没有行程表，恐怕也很难到达目的地。因为小林不知道该坐什么车，也不知道该在哪里换乘。因此，只有两者兼备，既有某个区间的车票又有整个旅行的行程表，才能保证到达目的地。与此类似，**计算机网络中也需要「数据链路层」和「网络层」这个分层才能实现向最终目标地址的通信。**

​	还有重要一点，旅行途中我们虽然不断变化了交通工具，但是旅行行程的起始地址和目的地址始终都没变。其实，在网络中数据包传输中也是如此，**源IP地址和目标IP地址在传输过程中是不会变化的，只有源 MAC 地址和目标 MAC 一直在变化。**



### 4.2 IP的基础知识

#### 4.2.1 IP地址的定义

​	在 TCP/IP 网络通信时，为了保证能正常通信，每个设备都需要配置正确的 IP 地址，否则无法实现正常的通信。IP 地址（IPv4 地址）由 `32` 位正整数来表示，IP 地址在计算机是以二进制的方式处理的。而人类为了方便记忆采用了**点分十进制**的标记方式，也就是将 32 位 IP 地址以每 8 位为组，共分为 `4` 组，每组以「`.`」隔开，再将每组转换成十进制。

![点分十进制](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112131041231.webp)

那么，IP 地址最大值也就是：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112131042615.webp)

也就说，最大允许 43 亿台计算机连接到网络。

​	实际上，IP 地址并不是根据主机台数来配置的，而是以网卡。像服务器、路由器等设备都是有 2 个以上的网卡，也就是它们会有 2 个以上的 IP 地址。

![每块网卡可以分配一个以上的IP地址](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112131042204.webp)

​	因此，让 43 亿台计算机全部连网其实是不可能的，更何况 IP 地址是由「网络标识」和「主机标识」这两个部分组成的，所以实际能够连接到网络的计算机个数更是少了很多。

> 可能有的小伙伴提出了疑问，现在不仅电脑配了 IP， 手机、IPad 等电子设备都配了 IP 呀，照理来说肯定会超过 43 亿啦，那是怎么能够支持这么多 IP 的呢？

​	因为会根据一种可以更换 IP 地址的技术 `NAT`，使得可连接计算机数超过 43 亿台。`NAT` 技术后续会进一步讨论和说明。



#### 4.2.2 IP地址的分类

​	互联网诞生之初，IP 地址显得很充裕，于是计算机科学家们设计了**分类地址**。IP 地址分类成了 5 种类型，分别是 A 类、B 类、C 类、D 类、E 类。

![IP地址分类](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112131049740.webp)



上图中黄色部是分类号，用以区分 IP 地址类别。

> 什么是 A、B、C 类地址？

​	其中对于 A、B、C 类主要分为两个部分，分别是**网络号和主机号**。这很好理解，好比小林是 A 小区 1 栋 101 号，你是 B 小区 1 栋 101 号。我们可以用下面这个表格， 就能很清楚的知道 A、B、C 分类对应的地址范围、最大主机个数。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112131050740.webp)





#### 4.2.3 无分类IP的地址

#### 4.2.4 公有IP地址与私有IP地址

#### 4.2.5 IP地址与路由控制

#### 4.2.6 IP分片与重组

#### 4.2.7 iPv6基本认识

#### 4.2.8 iPv4首部与IPv6的首部

### 4.3 IP协议相关技术

#### 4.3.1 DNS域名解析

#### 4.3.2 ARP与RARP协议

#### 4.3.3 DHCP动态获取IP地址

#### 4.3.4 NAT网络地址转换

#### 4.3.5 ICMP互联网控制报文协议

#### 4.3.6 ICMP因特网组管理协议

### 4.4 ping工作原理





## 五、网络综合篇

> **当键入网址后，到网页显示，其间发生了什么**

![简单网络模型](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121438869.webp)

### 5.1 HTTP

#### 5.1.1 解析URL

​	首先浏览器做的第一步工作就是要对 `URL` 进行解析，从而生发送给 `Web` 服务器的请求信息。

让我们看看一条长长的 URL 里的各个元素的代表什么，见下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121443297.webp)

所以图中的长长的 URL 实际上是请求服务器里的文件资源。

> 要是上图中的蓝色部分 URL 元素都省略了，哪应该是请求哪个文件呢？

当没有路径名时，就代表访问根目录下事先设置的**默认文件**，也就是 `/index.html` 或者 `/default.html` 这些文件，这样就不会发生混乱了。



#### 5.1.1 生产 HTTP 请求信息

​	对 `URL` 进行解析之后，浏览器确定了 Web 服务器和文件名，接下来就是根据这些信息来生成 HTTP 请求消息了。

![HTTP消息格式](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121445132.webp)



### 5.2DNS

​	通过浏览器解析 URL 并生成 HTTP 消息后，需要委托操作系统将消息发送给 `Web` 服务器。但在发送之前，还有一项工作需要完成，那就是**查询服务器域名对于的 IP 地址**，因为委托操作系统发送消息时，必须提供通信对象的 IP 地址。比如我们打电话的时候，必须要知道对方的电话号码，但由于电话号码难以记忆，所以通常我们会将对方电话号 + 姓名保存在通讯录里。所以，有一种服务器就专门保存了 `Web` 服务器域名与 `IP` 的对应关系，它就是 `DNS` 服务器。

#### 5.2.1 域名的层级关系

​	DNS 中的域名都是用**句点**来分隔的，比如 `www.server.com`，这里的句点代表了不同层次之间的**界限**。在域名中，**越靠右**的位置表示其层级**越高**。毕竟域名是外国人发明，所以思维和中国人相反，比如说一个城市地点的时候，外国喜欢从小到大的方式顺序说起（如 XX 街道 XX 区 XX 市 XX 省），而中国则喜欢从大到小的顺序（如 XX 省 XX 市 XX 区 XX 街道）。根域是在最顶层，它的下一层就是 com 顶级域，再下面是 server.com。

所以域名的层级关系类似一个树状结构：

- 根域名DNS服务器
- 顶级域DNS服务器（com）
- 权威 DNS 服务器（server.com）

![DNS树状结构](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121451849.webp)



​	根域的 DNS 服务器信息保存在互联网中所有的 DNS 服务器中。这样一来，任何 DNS 服务器就都可以找到并访问根域 DNS 服务器了。因此，客户端只要能够找到任意一台 DNS 服务器，就可以通过它找到根域 DNS 服务器，然后再一路顺藤摸瓜找到位于下层的某台目标 DNS 服务器。

#### 5.2.2 域名解析的工作流程

![域名解析的工作流程](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZdCwxNydn5YuT0s7aLuqWCv5bBPibRf9nk4wIb6J3jP62L6NEmPk3HicMUgf8VatcBicynP6BKLeT6GQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

1. 客户端首先会发出一个 DNS 请求，问 www.server.com 的 IP 是啥，并发给本地 DNS 服务器（也就是客户端的 TCP/IP 设置中填写的 DNS 服务器地址）。
2. 本地域名服务器收到客户端的请求后，如果缓存里的表格能找到 www.server.com，则它直接返回 IP 地址。如果没有，本地 DNS 会去问它的根域名服务器：“老大， 能告诉我 www.server.com 的 IP 地址吗？” 根域名服务器是最高层次的，它不直接用于域名解析，但能指明一条道路。
3. 根 DNS 收到来自本地 DNS 的请求后，发现后置是 .com，说：“www.server.com 这个域名归 .com 区域管理”，我给你 .com 顶级域名服务器地址给你，你去问问它吧。”
4. 本地 DNS 收到顶级域名服务器的地址后，发起请求问“老二， 你能告诉我 www.server.com  的 IP 地址吗？”
5. 顶级域名服务器说：“我给你负责 www.server.com 区域的权威 DNS 服务器的地址，你去问它应该能问到”。
6. 本地 DNS 于是转向问权威 DNS 服务器：“老三，www.server.com对应的IP是啥呀？” server.com 的权威 DNS 服务器，它是域名解析结果的原出处。为啥叫权威呢？就是我的域名我做主。
7. 权威 DNS 服务器查询后将对应的 IP 地址 X.X.X.X 告诉本地 DNS。
8. 本地 DNS 再将 IP 地址返回客户端，客户端和目标建立连接。



### 5.3 协议栈

​	通过 DNS 获取到 IP 后，就可以把 HTTP 的传输工作交给操作系统中的**协议栈**。协议栈的内部分为几个部分，分别承担不同的工作。上下关系是有一定的规则的，上面的部分会向下面的部分委托工作，下面的部分收到委托的工作并执行。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112121456827.webp)

​	应用程序（浏览器）通过调用 Socket 库，来委托协议栈工作。协议栈的上半部分有两块，分别是负责收发数据的 TCP 和 UDP 协议，它们两会接受应用层的委托执行收发数据的操作。协议栈的下半部分是用 IP 协议控制网络包收发操作，在互联网上传数据时，数据刽被切分成一块块的网络包，而将网络包发送给对方的操作就是由 IP 负责的。

此外 IP 中还包括 `ICMP` 协议和 `ARP` 协议。

- `ICMP` 用于告知网络包传送过程中产生的错误以及各种控制信息。
- `ARP` 用于根据 IP 地址查询相应的以太网 MAC 地址。

​	IP 下面的网卡驱动程序负责控制网卡硬件，而最下面的网卡则负责完成实际的收发操作，也就是对网线中的信号执行发送和接收操作。

### 5.4 TCP

​	HTTP 是基于 TCP 协议传输的，所以在这我们先了解下 TCP 协议。



### 5.5 远程定位-IP



### 5.6 两点之间-MAC



### 5.7 网卡



### 5.8 交换机



### 5.9 路由器



### 5.10 服务器与客户端



