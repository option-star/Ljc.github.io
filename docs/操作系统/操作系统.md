---
title: 图解操作系统
date: 2021-12-14
sidebar: 'auto'
tags:
- 操作系统
categories:
- 操作系统
isShowComments: true
---


## 一、硬件结构



### 1.1 CPU是如何执行程序的？

:::tip

列举一些小问题

1. a = 1 + 2这条代码是怎么被CPU执行的？
2. 软件的32位和64位之间的区别？
3. 32位操作系统可以运行在64位的电脑上吗？
4. 64位的操作系统可以运行在32位的电脑上吗？如果不行，原因是什么？
5. 64位相比32位CPU的优势在哪？
6. 64位CPU的计算性能一定比32位CPU高很多吗？

:::

#### 图灵的工作方式

​	要想知道程序执行的原理，我们可以先从「图灵机」说起，图灵的基本思想是用机器来模拟人们用纸笔进行数学运算的过程，而且还定义了计算机由哪些部分组成，程序又是如何执行的。

![image-20211207144158423](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071442578.png)

图灵机的基本组成如下：

- **纸带**，纸带由⼀个个连续的格子组成，每个格子可以写入字符，纸带就好比内存，而纸带上的格子的字符就好比内存中的数据或程序；
- **读写头**，读写头可以读取纸带上任意格子的字符，也可以把字符写入到纸带的格子；
- 读写头上有⼀些部件，比如存储单元、控制单元以及运算单元：
  1. 存储单元用于存放数据；
  2. 控制单元用于识别字符是数据还是指令，以及控制程序的流程等；
  3. 运算单元用于执行运算指令；

​	知道了图灵机的组成后，我们以简单数学运算的 `1 + 2 `作为例子，来看看它是怎么执行这行代码的。

1. 首先，用读写头把 「1、2、+」这 3 个字符分别写入到纸带上的 3 个格子，然后读写头先停在 1 字符对应的格子上；

![步骤1](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071446658.png)

2. 接着，读写头读入 1 到存储设备中，这个存储设备称为图灵机的状态；

![image-20211207144737532](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071447589.png)

3. 然后读写头向右移动⼀个格，用同样的方式把 2 读入到图灵机的状态，于是现在图灵机的状态中存储着两个连续的数字， 1 和 2；

![image-20211207144813045](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071448104.png)

4. 读写头再往右移动⼀个格，就会碰到 + 号，读写头读到 + 号后，将 + 号传输给「控制单元」，控制单元发现是⼀个 + 号而不是数字，所以没有存⼊到状态中，因为 + 号是运算符指令，作用是加和目前的状态，于是通知「运算单元」工作。运算单元收到要加和状态中的值的通知后，就会把状态中的1 和 2 读入并计算，再将计算的结果 3 存放到状态中；

![image-20211207144933276](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071449338.png)

5. 最后，运算单元将结果返回给控制单元，控制单元将结果传输给读写头，读写头向右移动，把结果 3写入到纸带的格子中；

![image-20211207145027094](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071450147.png)

​	通过上面的图灵机计算 1 + 2 的过程，可以发现图灵机主要功能就是读取纸带格子中的内容，然后交给控制单元识别字符是数字还是运算符指令，如果是数字则存入到图灵机状态中，如果是运算符，则通知运算符单元读取状态中的数值进行计算，计算结果最终返回给读写头，读写头把结果写⼊到纸带的格⼦中。

​	事实上，图灵机这个看起来很简单的工作方式，和我们今天的计算机是基本⼀样的。接下来，我们⼀同再看看当今计算机的组成以及工作方式。

#### 冯诺伊曼模型

​	在 1945 年冯诺依曼和其他计算机科学家们提出了计算机具体实现的报告，其遵循了图灵机的设计，而且还提出用电子元件构造计算机，并约定了用⼆进制进行计算和存储，还定义计算机基本结构为 5 个部分，分别是**中央处理器（CPU）、内存、输⼊设备、输出设备、总线**。

![image-20211207145327017](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071453072.png)

这 5 个部分也被称为冯诺依曼模型，接下来看看这 5 个部分的具体作用。

##### 内存

​	我们的程序和数据都是存储在内存，存储的区域是线性的。

​	数据存储的单位是一个**二进制位（bit）**，即0或1。最小一个地址为内存总字节数-1，这种结构好似我们程序里的数组，所以内存的读写任何一个数据的速度都是一样的。

##### 中央处理器

​	中央处理器也就是我们常说的 CPU，32 位和 64 位 CPU 最主要区别在于⼀次能计算多少字节数据：

- 32 位 CPU ⼀次可以计算 4 个字节；
- 64 位 CPU ⼀次可以计算 8 个字节；

这里的 32 位和 64 位，通常称为 CPU 的位宽。

​	之所以 CPU 要这样设计，是为了能**计算更大的数值**，如果是 8 位的 CPU，那么⼀次只能计算 1 个字节`0~255` 范围内的数值，这样就无法⼀次完成计算 `10000 * 500` ，于是为了能⼀次计算大数的运算，CPU 需要支持多个 byte ⼀起计算，所以 CPU 位宽越大，可以计算的数值就越大，比如如说 32 位 CPU 能计算的最大整数是 `4294967295` 。

​	CPU 内部还有⼀些组件，常见的有寄存器、控制单元和逻辑运算单元等。其中，控制单元负责控制 CPU ⼯作，逻辑运算单元负责计算，而寄存器可以分为多种类，每种寄存器的功能又不尽相同。

​	CPU 中的寄存器主要作用是存储计算时的数据，你可能好奇为什么有了内存还需要寄存器？原因很简单， 因为内存离 CPU 太远了，而寄存器就在 CPU 里，还紧挨着控制单元和逻辑运算单元，自然计算时速度会很快。

常见的寄存器种类：

- **通用寄存器**：用来存放需要进行运算的数据，比如需要进行加和运算的两个数据
- **程序计数器**： 用来存储CPU要执行下一条指令[所在的内存地址]，注意不是存储了下一条要执行的指令，此时指令还在内存中，程序计数器只是存储了下一条指令的地址。
- **指令寄存器**：用来存放程序计数器的指向的指令，也就是指令本身，指令被执行完成之前，指令都存储在这里

##### 总线

总线式用于CPU和 内存以及其他设备之间的通信，总线可分为3种：

- **地址总线**： 用于指定CPU将要操作的内存地址；
- **数据总线**： 用于读写内存的数据；
- **控制总线**： 用于发送和接受信号，比如中断、设备复位等信号，CPU收到信号后自然进行相应，这时也需要控制总线；

当 CPU 要读写内存数据的时候，⼀般需要通过两个总线：

1. 首先要通过「地址总线」来指定内存的地址；
2. 再通过「数据总线」来传输数据；

##### 输入、输出设备

​	输⼊设备向计算机输⼊数据，计算机经过计算后，把数据输出给输出设备。期间，如果输⼊设备是键盘， 按下按键时是需要和 CPU 进行交互的，这时就需要用到控制总线了。

#### 线路位宽和CPU位宽

​	数据是如何通过线路传输的呢？其实是通过操作电压，低电压表示 0，高压电压则表示 1。

​	如果构造了高低高这样的信号，其实就是 101 ⼆进制数据，⼗进制则表示 5，如果只有⼀条线路，就意味 着每次只能传递 1 bit 的数据，即 0 或 1，那么传输 101 这个数据，就需要 3 次才能传输完成，这样的效 率非常低。

​	这样⼀位⼀位传输的方式，称为**串行**，下⼀个 bit 必须等待上⼀个 bit 传输完成才能进行传输。当然，想⼀次多传⼀些数据，增加线路即可，这时数据就可以**并行**传输。

​	为了避免低效率的串行传输的方式，线路的位宽最好⼀次就能访问到所有的内存地址。 CPU 要想操作的内存地址就需要地址总线，如果地址总线只有 1 条，那每次只能表示 「0 或 1」这两种情况，所以 CPU ⼀次 只能操作 2 个内存地址，如果想要 CPU 操作 4G 的内存，那么就需要 32 条地址总线，因为 2 ^ 32 = 4G 。

​	知道了线路位宽的意义后，我们再来看看 CPU 位宽。

​	CPU 的位宽最好不要小于线路位宽，比如 32 位 CPU 控制 40 位宽的地址总线和数据总线的话，工作起来就会非常复杂且麻烦，所以 32 位的 CPU 最好和 32 位宽的线路搭配，因为 32 位 CPU ⼀次最多只能操作32 位宽的地址总线和数据总线。

​	如果用32 位 CPU 去加和两个 64 位大小的数字，就需要把这 2 个 64 位的数字分成 2 个低位 32 位数字和 2 个⾼位 32 位数字来计算，先加个两个低位的 32 位数字，算出进位，然后加和两个高位的 32 位数字， 最后再加上进位，就能算出结果了，可以发现 32 位 CPU 并不能⼀次性计算出加和两个 64 位数字的结 果。

​	但是并不代表 64 位 CPU 性能比 32 位 CPU 高很多，很少应用需要算超过 32 位的数字，所以**如果计算的数额不超过32位数字的情况下，32位和64 位CPU 之间没什么区别的，只有当计算超过32位数字的情况下，64位的优势才能体现出来**。

​	另外，32 位 CPU 最大只能操作 4GB 内存，就算你装了 8 GB 内存条，也没用。而 64 位 CPU 寻址范围则很大，理论最大的寻址空间为 `2^64` 。

#### 程序执行的基本过程

​	在前面，我们知道了程序在图灵机的执行过程，接下来我们来看看程序在冯诺依曼模型上是怎么执行的。 程序实际上是⼀条⼀条指令，所以程序的运行过程就是把每⼀条指令⼀步⼀步的执行起来，负责执行指令 的就是 CPU 了。

![image-20211207152610066](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071526134.png)

CPU执行程序的过程如下：

1. CPU 读取「程序计数器」的值，这个值是指令的内存地址，然后 CPU 的「控制单元」操作「地址总线」指定需要访问的内存地址，接着通知内存设备准备数据，数据准备好后通过「数据总线」将指令数据传给 CPU，CPU 收到内存传来的数据后，将这个指令数据存⼊到「指令寄存器」。
2. 第⼆步，CPU 分析「指令寄存器」中的指令，确定指令的类型和参数，如果是计算类型的指令，就把指令交给「逻辑运算单元」运算；如果是存储类型的指令，则交由「控制单元」执行；
3. 第三步，CPU 执行完指令后，「程序计数器」的值自增，表示指向下⼀条指令。这个自增的大小，由CPU 的位宽决定，比如 32 位的 CPU，指令是 4 个字节，需要 4 个内存地址存放，因此「程序计数器」的值会自增 4；

简单总结⼀下就是，⼀个程序执行的时候，CPU 会根据程序计数器里的内存地址，从内存里面把需要执 的指令读取到指令寄存器里面执行，然后根据指令长度自增，开始顺序读取下⼀条指令。 CPU 从程序计数器读取指令、到执行、再到下⼀条指令，这个过程会不断循环，直到程序执行结束，这个不断循环的过程被称为 **CPU** **的指令周期**。

#### a = 1 + 2 执行具体过程

​	知道了基本的程序执行过程后，接下来用a = 1 + 2 的作为例子，进⼀步分析该程序在冯诺伊曼模型的执行过程。

​	CPU 是不认识 a = 1 + 2 这个字符串，这些字符串只是方便我们程序员认识，要想这段程序能跑起来，还需要把整个程序翻译成**汇编语言**的程序，这个过程称为编译成汇编代码。

​	针对汇编代码，我们还需要用汇编器翻译成机器码，这些机器码由 0 和 1 组成的机器语言，这⼀条条机器码，就是⼀条条的**计算机指令**，这个才是 CPU 能够真正认识的东西。

​	下面来看看 a = 1 + 2 在 32 位 CPU 的执行过程。

​	程序编译过程中，编译器通过分析代码，发现 1 和 2 是数据，于是程序运行时，内存会有个专门的区域来存放这些数据，这个区域就是「数据段」。如下图，数据 1 和 2 的区域位置：

- 数据 1 被存放到 0x100 位置；
- 数据 2 被存放到 0x104 位置；

注意，数据和指令是分开区域存放的，存放指令区域的地方称为「正文段」。

![image-20211207155323804](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071553874.png)

​	编译器会把 a = 1 + 2 翻译成 4 条指令，存放到正文段中。如图，这 4 条指令被存放到了 0x200 ~ 0x20c 的区域中：

- 0x200 的内容是 load 指令将 0x100 地址中的数据 1 装⼊到寄存器 R0 ；
-  0x204 的内容是 load 指令将 0x104 地址中的数据 2 装⼊到寄存器 R1 ； 
- 0x208 的内容是 add 指令将寄存器 R0 和 R1 的数据相加，并把结果存放到寄存器 R2 ；
- 0x20c 的内容是 store 指令将寄存器 R2 中的数据存回数据段中的 0x108 地址中，这个地址也就 是变量 a 内存中的地址；

​	编译完成后，具体执⾏程序的时候，程序计数器会被设置为 0x200 地址，然后依次执行这 4 条指令。

​	上面的例子中，由于是在 32 位 CPU 执⾏的，因此⼀条指令是占 32 位大小，所以你会发现每条指令间隔4 个字节。

​	而数据的大小是根据你在程序中指定的变量类型，比如 int 类型的数据则占 4 个字节， char 类型的数据则占 1 个字节。

##### 指令

​	上⾯的例⼦中，图中指令的内容我写的是简易的汇编代码，⽬的是为了⽅便理解指令的具体内容，事实上指令的内容是⼀串⼆进制数字的机器码，每条指令都有对应的机器码，CPU 通过解析机器码来知道指令的内容。

​	不同的 CPU 有不同的指令集，也就是对应着不同的汇编语⾔和不同的机器码，接下来选⽤最简单的 MIPS指集，来看看机器码是如何⽣成的，这样也能明⽩⼆进制的机器码的具体含义。

​	MIPS 的指令是⼀个 32 位的整数，高6 位代表着操作码，表示这条指令是⼀条什么样的指令，剩下的 26位不同指令类型所表示的内容也就不相同，主要有三种类型R、I 和 J。

![image-20211207170704938](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071707084.png)

⼀起具体看看这三种类型的含义：

- ***R* 指令**，⽤在算术和逻辑操作，⾥⾯由读取和写⼊数据的寄存器地址。如果是逻辑位移操作，后⾯还有位移操作的「位移量」，⽽最后的「功能码」则是再前⾯的操作码不够的时候，扩展操作码来表示对应的具体指令的；

- ***I* 指令**，⽤在数据传输、条件分⽀等。这个类型的指令，就没有了位移量和操作码，也没有了第三个寄存器，⽽是把这三部分直接合并成了⼀个地址值或⼀个常数；

- ***J* 指令**，⽤在跳转，⾼ 6 位之外的 26 位都是⼀个跳转后的地址；

​	接下来，我们把前⾯例⼦的这条指令：「 add 指令将寄存器 R0 和 R1 的数据相加，并把结果放⼊到R2 」，翻译成机器码。

![image-20211207170919918](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071709964.png)

加和运算 add 指令是属于 R 指令类型：

- add 对应的 MIPS 指令⾥操作码是 000000 ，以及最末尾的功能码是 100000 ，这些数值都是固定的，查⼀下 MIPS 指令集的⼿册就能知道的；

- rs 代表第⼀个寄存器 R0 的编号，即 00000 ；

- rt 代表第⼆个寄存器 R1 的编号，即 00001 ；

- rd 代表⽬标的临时寄存器 R2 的编号，即 00010 ；

- 因为不是位移操作，所以位移量是 00000

​	把上⾯这些数字拼在⼀起就是⼀条 32 位的 MIPS 加法指令了，那么⽤ 16 进制表示的机器码则是`0x00011020` 。

​	编译器在编译程序的时候，会构造指令，这个过程叫做指令的编码。CPU 执⾏程序的时候，就会解析指令，这个过程叫作**指令的解码**。

​	现代大多数 CPU 都使⽤来流水线的⽅式来执⾏指令，所谓的流⽔线就是把⼀个任务拆分成多个⼩任务，于是⼀条指令通常分为 4 个阶段，称为 4 级流⽔线，如下图：

![流水线](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071712013.png)

四个阶段的具体含义：

1.  CPU 通过程序计数器读取对应内存地址的指令，这个部分称为 **Fetch（取得指令）**；
1.  CPU 对指令进⾏解码，这个部分称为 **Decode（指令译码）**；
1.   CPU 执⾏指令，这个部分称为 **Execution（执⾏指令）**；
1.   CPU 将计算结果存回寄存器或者将寄存器的值存⼊内存，这个部分称为 **Store（数据回写）**

​	上⾯这 4 个阶段，我们称为**指令周期（Instrution Cycle）**，CPU 的⼯作就是⼀个周期接着⼀个周期，周⽽复始。

​	事实上，不同的阶段其实是由计算机中的不同组件完成的：

![image-20211207171427814](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071714863.png)

1. 取指令的阶段，我们的指令是存放在**存储器**⾥的，实际上，通过程序计数器和指令寄存器取出指令的过程，是由**控制器**操作的；
2. 指令的译码过程，也是由**控制器**进⾏的；
3. 指令执⾏的过程，⽆论是进⾏算术操作、逻辑操作，还是进⾏数据传输、条件分⽀操作，都是由**算术逻辑单元**操作的，也就是由**运算器**处理的。但是如果是⼀个简单的⽆条件地址跳转，则是直接在**控制器**⾥⾯完成的，不需要⽤到运算器。

##### 指令的类型

指令从功能⻆度划分，可以分为 5 大类：

- **数据传输类型的指令**，⽐如` store/load` 是寄存器与内存间数据传输的指令， `mov `是将⼀个内存地址的数据移动到另⼀个内存地址的指令；
- **运算类型的指令**，⽐如加减乘除、位运算、⽐较⼤⼩等等，它们最多只能处理两个寄存器中的数据；
- **跳转类型的指令**，通过修改程序计数器的值来达到跳转执⾏指令的过程，⽐如编程中常⻅的 `if-else` 、 `swtich-case` 、函数调⽤等。
- **信号类型的指令**，比如发生中断的指令`trap`
- **闲置类型的指令**，⽐如指令 `nop `，执⾏后 CPU 会空转⼀个周期；

##### 指令的执行速度

> 时钟周期

​	CPU 的硬件参数都会有 `GHz `这个参数，⽐如⼀个 1 `GHz` 的 CPU，指的是时钟频率是 1 G，代表着 1 秒会产⽣ 1G 次数的脉冲信号，每⼀次脉冲信号⾼低电平的转换就是⼀个周期，称为**时钟周期**。

​	对于 CPU 来说，在⼀个时钟周期内，CPU 仅能完成⼀个最基本的动作，时钟频率越⾼，时钟周期就越短，⼯作速度也就越快。

​	⼀个时钟周期⼀定能执⾏完⼀条指令吗？答案是不⼀定的，⼤多数指令不能在⼀个时钟周期完成，通常需要若⼲个时钟周期。不同的指令需要的时钟周期是不同的，加法和乘法都对应着⼀条 CPU 指令，但是乘法需要的时钟周期就要⽐加法多。

> 如何让程序跑得更快？

​	程序执⾏的时候，耗费的 CPU 时间少就说明程序是快的，对于程序的 CPU 执⾏时间，我们可以拆解成**CPU时钟周期数（CPU Cycles）和时钟周期时间（Clock Cycle Time）的乘积**

![image-20211207172354914](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071723967.png)

​	时钟周期时间就是我们前⾯提及的 CPU 主频，主频越⾼说明 CPU 的⼯作速度就越快，⽐如我⼿头上的电脑的 CPU 是 2.4 GHz 四核 Intel Core i5，这⾥的 2.4 GHz 就是电脑的主频，时钟周期时间就是 1/2.4G。

​	要想 CPU 跑的更快，⾃然缩短时钟周期时间，也就是提升 CPU 主频，但是今⾮彼⽇，摩尔定律早已失效，当今的 CPU 主频已经很难再做到翻倍的效果了。

​	另外，换⼀个更好的 CPU，这个也是我们软件⼯程师控制不了的事情，我们应该把⽬光放到另外⼀个乘法因⼦ —— CPU 时钟周期数，如果能减少程序所需的 CPU 时钟周期数量，⼀样也是能提升程序的性能的。

​	对于 CPU 时钟周期数我们可以进⼀步拆解成：「**指令数 x 每条指令的平均时钟周期数（Cycles Per Instruction，简称 CPI** **）**」，于是程序的 CPU 执⾏时间的公式可变成如下：

![image-20211207172638810](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071726854.png)

因此，要想程序跑的更快，优化这三者即可：

- **指令数**： 表示执⾏程序所需要多少条指令，以及哪些指令。这个层⾯是基本靠编译器来优化，毕竟同样的代码，在不同的编译器，编译出来的计算机指令会有各种不同的表示⽅式。
- **每条指令的平均时钟周期数CPI**： 表示⼀条指令需要多少个时钟周期数，现代⼤多数 CPU 通过流⽔线技术（Pipline），让⼀条指令需要的 CPU 时钟周期数尽可能的少；
- **时钟周期时间**，表示计算机主频，取决于计算机硬件。有的 CPU ⽀持超频技术，打开了超频意味着把 CPU 内部的时钟给调快了，于是 CPU ⼯作速度就变快了，但是也是有代价的，CPU 跑的越快，散热的压⼒就会越⼤，CPU 会很容易奔溃。

#### 总结

> 64 位相⽐ 32 位 CPU 的优势在哪吗？64 位 CPU 的计算性能⼀定⽐ 32 位 CPU ⾼很多吗？

64 位相⽐ 32 位 CPU 的优势主要体现在两个⽅⾯：

- 64 位 CPU 可以⼀次计算超过 32 位的数字，⽽ 32 位 CPU 如果要计算超过 32 位的数字，要分多步骤进⾏计算，效率就没那么⾼，但是⼤部分应⽤程序很少会计算那么⼤的数字，所以**只有运算大数字的时候，64位 CPU的优势才能体现出来，否则和 32位CPU的计算性能相差不⼤**。
- 64 位 CPU 可以**寻址更大的内存空间**，32 位 CPU 最⼤的寻址地址是 4G，即使你加了 8G ⼤⼩的内存，也还是只能寻址到 4G，⽽ 64 位 CPU 最⼤寻址地址是 2^64 ，远超于 32 位 CPU 最⼤寻址地址的 2^32 。

> 你知道软件的 32 位和 64 位之间的区别吗？再来 32 位的操作系统可以运⾏在 64 位的电脑上吗？64 位的操作系统可以运⾏在 32 位的电脑上吗？如果不⾏，原因是什么？

​	64 位和 32 位软件，实际上代表指令是 64 位还是 32 位的：

- 如果 32 位指令在 64 位机器上执⾏，需要⼀套兼容机制，就可以做到兼容运⾏了。但是 **如果64位指令在32位机器上执行，就比较困难了，因为32位寄存器存不下64位的指令**
- 操作系统其实也是⼀种程序，我们也会看到操作系统会分成 32 位操作系统、64 位操作系统，其代表意义就是操作系统中程序的指令是多少位，⽐如 64 位操作系统，指令也就是 64 位，因此不能装在32 位机器上。

​	总之，硬件的 64 位和 32 位指的是 CPU 的位宽，软件的 64 位和 32 位指的是指令的位宽。 

### 1.2 存储器金字塔

​	⼤家如果想⾃⼰组装电脑的话，肯定需要购买⼀个 CPU，但是存储器⽅⾯的设备，分类⽐较多，那我们肯

定不能只买⼀种存储器，⽐如你除了要买内存，还要买硬盘，⽽针对硬盘我们还可以选择是固态硬盘还是

机械硬盘。

​	相信⼤家都知道内存和硬盘都属于计算机的存储设备，断电后内存的数据是会丢失的，⽽硬盘则不会，因

为硬盘是持久化存储设备，同时也是⼀个 I/O 设备。

​	但其实 CPU 内部也有存储数据的组件，这个应该⽐较少⼈注意到，⽐如**寄存器**、**CPU L1/L2/L3 Cache**

也都是属于存储设备，只不过它们能存储的数据⾮常⼩，但是它们因为靠近 CPU 核⼼，所以访问速度都⾮

常快，快过硬盘好⼏个数量级别。

​	问题来了，**那机械硬盘、固态硬盘、内存这三个存储器，到底和** **CPU L1 Cache** **相⽐速度差多少倍呢？**

​	在回答这个问题之前，我们先来看看「**存储器的层次结构**」，好让我们对存储器设备有⼀个整体的认识。

![image-20211209145701608](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091457771.png)



#### 存储器的层次结构

​	我们想象中⼀个场景，⼤学期末准备考试了，你前去图书馆临时抱佛脚。那么，在看书的时候，我们的⼤

脑会思考问题，也会记忆知识点，另外我们通常也会把常⽤的书放在⾃⼰的桌⼦上，当我们要找⼀本不常

⽤的书，则会去图书馆的书架找。

​	就是这么⼀个⼩⼩的场景，已经把计算机的存储结构基本都涵盖了。

​	我们可以把 CPU ⽐喻成我们的⼤脑，⼤脑正在思考的东⻄，就好⽐ CPU 中的**寄存器**，处理速度是最快

的，但是能存储的数据也是最少的，毕竟我们也不能⼀下同时思考太多的事情，除⾮你练过。

​	我们⼤脑中的记忆，就好⽐ **CPU Cache**，中⽂称为 CPU ⾼速缓存，处理速度相⽐寄存器慢了⼀点，但是

能存储的数据也稍微多了⼀些。

​	CPU Cache 通常会分为**L1、L2、L3三层**，其中 L1 Cache 通常分成「数据缓存」和「指令缓存」，L1

是距离 CPU 最近的，因此它⽐ L2、L3 的读写速度都快、存储空间都⼩。我们⼤脑中短期记忆，就好⽐

L1 Cache，⽽⻓期记忆就好⽐ L2/L3 Cache。

​	寄存器和 CPU Cache 都是在 CPU 内部，跟 CPU 挨着很近，因此它们的读写速度都相当的快，但是能存

储的数据很少，毕竟 CPU 就这么丁点⼤。

​	当我们⼤脑记忆中没有资料的时候，可以从书桌或书架上拿书来阅读，那我们桌⼦上的书，就好⽐**内存**，

我们虽然可以⼀伸⼿就可以拿到，但读写速度肯定远慢于寄存器，那图书馆书架上的书，就好⽐**硬盘**，能

存储的数据⾮常⼤，但是读写速度相⽐内存差好⼏个数量级，更别说跟寄存器的差距了。

![image-20211209145937525](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091459596.png)

​	我们从图书馆书架取书，把书放到桌⼦上，再阅读书，我们⼤脑就会记忆知识点，然后再经过⼤脑思考，

这⼀系列过程相当于，数据从硬盘加载到内存，再从内存加载到 CPU 的寄存器和 Cache 中，然后再通过

CPU 进⾏处理和计算。

​	**对于存储器，它的速度越快、能耗会越⾼、⽽且材料的成本也是越贵的，以⾄于速度快的存储器的容量都⽐较⼩。**

​	CPU ⾥的寄存器和 Cache，是整个计算机存储器中价格最贵的，虽然存储空间很⼩，但是读写速度是极快

的，⽽相对⽐较便宜的内存和硬盘，速度肯定⽐不上 CPU 内部的存储器，但是能弥补存储空间的不⾜。

存储器通常可以分为这么⼏个级别：

![image-20211209150110914](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091501110.png)

- 寄存器
- CPU Cache
  - L1-Cache
  - L2-Cache
  - L3-Cache
- 内存
- SSD/HDD硬盘



##### 寄存器

​	最靠近 CPU 的控制单元和逻辑计算单元的存储器，就是寄存器了，它使⽤的材料速度也是最快的，因此价格也是最贵的，那么数量不能很多。

​	存储器的数量通常在⼏⼗到⼏百之间，每个寄存器可以⽤来存储⼀定的字节（byte）的数据。⽐如：

- 32 位 CPU 中⼤多数寄存器可以存储 `4` 个字节；
- 64 位 CPU 中⼤多数寄存器可以存储` 8` 个字节。

​	寄存器的访问速度⾮常快，⼀般要求在半个 CPU 时钟周期内完成读写，CPU 时钟周期跟 CPU 主频息息相关，⽐如 2 GHz 主频的 CPU，那么它的时钟周期就是 1/2G，也就是 0.5ns（纳秒）。
​	CPU 处理⼀条指令的时候，除了读写寄存器，还需要解码指令、控制指令执⾏和计算。如果寄存器的速度太慢，则会拉⻓指令的处理周期，从⽽给⽤户的感觉，就是电脑「很慢」。

##### CPU Cache

​	CPU Cache ⽤的是⼀种叫 `SRAM（Static Random-AccessMemory，静态随机存储器）` 的芯⽚。SRAM之所以叫“静态”存储器，是因为只要有电，数据就可以保持存在，⽽⼀旦断电，数据就会丢失了。在 SRAM ⾥⾯，⼀个 bit 的数据，通常需要 6 个晶体管，所以 SRAM 的存储密度不⾼，同样的物理空间下，能存储的数据是有限的，不过也因为 SRAM 的电路简单，所以访问速度⾮常快。

​	CPU 的⾼速缓存，通常可以分为 L1、L2、L3 这样的三层⾼速缓存，也称为⼀级缓存、⼆次缓存、三次缓存。

![image-20211209151050920](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091510993.png)

###### L1 高速缓存

​	L1 ⾼速缓存的访问速度⼏乎和寄存器⼀样快，通常只需要 2~4 个时钟周期，⽽⼤⼩在⼏⼗ KB 到⼏百

KB 不等。每个 CPU 核⼼都有⼀块属于⾃⼰的 L1 ⾼速缓存，指令和数据在 L1 是分开存放的，所以 L1 ⾼速缓存通常分成**指令缓存**和**数据缓存**。

​	在 Linux 系统，我们可以通过这条命令，查看 CPU ⾥的 L1 Cache 「数据」缓存的容量⼤⼩：

```shell
$ cat /sys/devices/system/cpu/cpu0/cache/index0/size
# 32K
```

⽽查看 L1 Cache 「指令」缓存的容量⼤⼩，则是：

```shell
$ cat /sys/devices/system/cpu/cpu0/cache/index1/size
# 32K
```



###### L2 高速缓存

​	L2 ⾼速缓存同样每个 CPU 核⼼都有，但是 L2 ⾼速缓存位置⽐ L1 ⾼速缓存距离 CPU 核⼼ 更远，它⼤⼩⽐ L1 ⾼速缓存更⼤，CPU 型号不同⼤⼩也就不同，通常⼤⼩在⼏百 KB 到⼏ MB 不等，访问速度则更慢，速度在 10~20 个时钟周期。

在 Linux 系统，我们可以通过这条命令，查看 CPU ⾥的 L2 Cache 的容量⼤⼩:

```shell
$ cat /sys/devices/system/cpu/cpu0/cache/index2/size
# 256K
```

###### L3 高速缓存

​	L3 ⾼速缓存通常是多个 CPU 核⼼共⽤的，位置⽐ L2 ⾼速缓存距离 CPU 核⼼ 更远，⼤⼩也会更⼤些，通常⼤⼩在⼏ MB 到⼏⼗ MB 不等，具体值根据 CPU 型号⽽定。访问速度相对也⽐较慢⼀些，访问速度在 20~60 个时钟周期。

在 Linux 系统，我们可以通过这条命令，查看 CPU ⾥的 L3 Cache 的容量⼤⼩：

```shell
$ cat /sys/devices/system/cpu/cpu0/cache/index3/size
# 3072K
```

##### 内存

​	内存⽤的芯⽚和 CPU Cache 有所不同，它使⽤的是⼀种叫作`DRAM（Dynamic Random Access Memory，动态随机存取存储器）`的芯⽚。相⽐ SRAM，DRAM 的密度更⾼，功耗更低，有更⼤的容量，⽽且造价⽐ SRAM 芯⽚便宜很多。DRAM 存储⼀个 bit 数据，只需要⼀个晶体管和⼀个电容就能存储，但是因为数据会被存储在电容⾥，电容会不断漏电，所以需要「定时刷新」电容，才能保证数据不会被丢失，这就是 DRAM 之所以被称为「动态」存储器的原因，只有不断刷新，数据才能被存储起来。DRAM 的数据访问电路和刷新电路都⽐ SRAM 更复杂，所以访问的速度会更慢，内存速度⼤概在200~300 个 时钟周期之间。

##### SSD/HDD硬盘

​	`SSD（Solid-state disk）` 就是我们常说的固体硬盘，结构和内存类似，但是它相⽐内存的优点是断电后数据还是存在的，⽽内存、寄存器、⾼速缓存断电后数据都会丢失。内存的读写速度⽐ SSD ⼤概快10~1000 倍。当然，还有⼀款传统的硬盘，也就是机械硬盘（*Hard Disk Drive, HDD*），它是通过物理读写的⽅式来访问数据的，因此它访问速度是⾮常慢的，它的速度⽐内存慢 10W 倍左右。由于 SSD 的价格快接近机械硬盘了，因此机械硬盘已经逐渐被 SSD 替代了。

#### 存储器的层次关系

​	现代的⼀台计算机，都⽤上了 CPU Cahce、内存、到 SSD 或 HDD 硬盘这些存储器设备了。其中，存储空间越⼤的存储器设备，其访问速度越慢，所需成本也相对越少。CPU 并不会直接和每⼀种存储器设备直接打交道，⽽是每⼀种存储器设备只和它相邻的存储器设备打交道。⽐如，CPU Cache 的数据是从内存加载过来的，写回数据的时候也只写回到内存，CPU Cache 不会直接把数据写到硬盘，也不会直接从硬盘加载数据，⽽是先加载到内存，再从内存加载到 CPU Cache 中。所以，**每个存储器只和相邻的⼀层存储器设备打交道，并且存储设备为了追求更快的速度，所需的材料成本必然也是更⾼，也正因为成本太⾼，所以CPU内部的寄存器、L1\L2\L3Cache只好⽤较⼩的容量，相反内存、硬盘则可⽤更⼤的容量，这就我们今天所说的存储器层次结构。**

![image-20211209153044869](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091530948.png)

​	另外，当 CPU 需要访问内存中某个数据的时候，如果寄存器有这个数据，CPU 就直接从寄存器取数据即 可，如果寄存器没有这个数据，CPU 就会查询 L1 ⾼速缓存，如果 L1 没有，则查询 L2 ⾼速缓存，L2 还 是没有的话就查询  L3  ⾼速缓存，L3 依然没有的话，才去内存中取数据。所以，存储层次结构也形成了**缓存**的体系。

![未命名文件](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091805012.png)



#### 存储器之间的实际价格和性能差距

​	前⾯我们知道了，速度越快的存储器，造价成本往往也越⾼，那我们就以实际的数据来看看，不同层级的存储器之间的性能和价格差异。下⾯这张表格是不同层级的存储器之间的成本对⽐图：

![image-20211209180638200](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091806268.png)

​	你可以看到 L1 Cache 的访问延时是 1 纳秒，⽽内存已经是 100 纳秒了，相⽐ L1 Cache 速度慢了 100倍。另外，机械硬盘的访问延时更是⾼达 10 毫秒，相⽐ L1 Cache 速度慢了 10000000 倍，差了好⼏个数量级别。在价格上，每⽣成 MB ⼤⼩的 L1 Cache 相⽐内存贵了 466 倍，相⽐机械硬盘那更是贵了 175000倍。我在某东逛了下各个存储器设备的零售价，8G 内存 + 1T 机械硬盘 + 256G 固态硬盘的总价格，都不及⼀块 Intle i5-10400 的 CPU 的价格，这款 CPU 的⾼速缓存的总⼤⼩也就⼗多 MB。

#### 总结

​	各种存储器之间的关系，可以⽤我们在图书馆学习这个场景来理解。CPU 可以⽐喻成我们的⼤脑，我们当前正在思考和处理的知识的过程，就好⽐ CPU 中的**寄存器**处理数据的过程，速度极快，但是容量很⼩。⽽ CPU 中的 **L1-L3 Cache** 好⽐我们⼤脑中的短期记忆和⻓期记忆，需要⼩⼩花费点时间来调取数据并处理。我们⾯前的桌⼦就相当于**内存**，能放下更多的书（数据），但是找起来和看起来就要花费⼀些时间，相⽐CPU Cache 慢不少。⽽图书馆的书架相当于**硬盘**，能放下⽐内存更多的数据，但找起来就更费时间了，可以说是最慢的存储器设备了。 

​	寄存器、CPU Cache，到内存、硬盘，这样⼀层层下来的存储器，访问速度越来越慢，存储容量越来越⼤，价格也越来越便宜，⽽且每个存储器只和相邻的⼀层存储器设备打交道，于是这样就形成了存储器的层次结构。再来回答，开头的问题：那机械硬盘、固态硬盘、内存这三个存储器，到底和 CPU L1 Cache 相⽐速度差多少倍呢？

​	CPU L1 Cache 随机访问延时是 1 纳秒，内存则是 100 纳秒，所以 **CPU L1 Cache⽐内存快100倍左右**。SSD 随机访问延时是 150 微秒，所以 **CPU L1 Cache⽐SSD快150000倍左右**。最慢的机械硬盘随机访问延时已经⾼达 10 毫秒，我们来看看机械硬盘到底有多「⻳速」：

- **SSD比机械硬盘快70倍左右**
- **内存比机械硬盘快100000倍左右**
- **CPU L1 Cache比机械硬盘快10000000倍左右 **

​	我们把上述的时间⽐例差异放⼤后，就能⾮常直观感受到它们的性能差异了。如果 CPU 访问 L1 Cache 的缓存时间是 1 秒，那访问内存则需要⼤约 2 分钟，随机访问 SSD ⾥的数据则需要 1.7 天，访问机械硬盘那更久，⻓达近 4 个⽉。

### 1.3 如何写出让CPU跑得更快的代码？

​	代码都是由 CPU 跑起来的，我们代码写的好与坏就决定了 CPU 的执⾏效率，特别是在编写计算密集型的程序，更要注重 CPU 的执⾏效率，否则将会⼤⼤影响系统性能。CPU 内部嵌⼊了 CPU Cache（⾼速缓存），它的存储容量很⼩，但是离 CPU 核⼼很近，所以缓存的读写速度是极快的，那么如果 CPU 运算时，直接从 CPU Cache 读取数据，⽽不是从内存的话，运算速度就会很快。但是，⼤多数⼈不知道 CPU Cache 的运⾏机制，以⾄于不知道如何才能够写出能够配合 CPU Cache ⼯作机制的代码，⼀旦你掌握了它，你写代码的时候，就有新的优化思路了。那么，接下来我们就来看看，CPU Cache 到底是什么样的，是如何⼯作的呢，⼜该写出让 CPU 执⾏更快的代码呢？

![image-20211209181454012](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091814083.png)

#### CPU Cache有多快？

​	你可能会好奇为什么有了内存，还需要 CPU Cache？根据摩尔定律，CPU 的访问速度每 18 个⽉就会翻倍，相当于每年增⻓ 60% 左右，内存的速度当然也会不断增⻓，但是增⻓的速度远⼩于 CPU，平均每年只增⻓ 7% 左右。于是，CPU 与内存的访问性能的差距不断拉⼤。到现在，⼀次内存访问所需时间是 200~300 多个时钟周期，这意味着 CPU 和内存的访问速度已经相差200~300 多倍了。为了弥补 CPU 与内存两者之间的性能差异，就在 CPU 内部引⼊了 CPU Cache，也称⾼速缓存。 CPU Cache 通常分为⼤⼩不等的三级缓存，分别是 **L1 Cache、L2 Cache、L3 Cache**。由于 CPU Cache 所使⽤的材料是 SRAM，价格⽐内存使⽤的 DRAM ⾼出很多，在当今每⽣产 1 MB ⼤⼩的 CPU Cache 需要 7 美⾦的成本，⽽内存只需要 0.015 美⾦的成本，成本⽅⾯相差了 466 倍，所以 CPU Cache 不像内存那样动辄以 GB 计算，它的⼤⼩是以 KB 或 MB 来计算的。

​	在 Linux 系统中，我们可以使⽤下图的⽅式来查看各级 CPU Cache 的⼤⼩，⽐如我这⼿上这台服务器，离 CPU 核⼼最近的 L1 Cache 是 32KB，其次是 L2 Cache 是 256KB，最⼤的 L3 Cache 则是 3MB。

![image-20211209181948173](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091819247.png)

​	其中，**L1 Cache** **通常会分为「数据缓存」和「指令缓存」**，这意味着数据和指令在 L1 Cache 这⼀层是分开缓存的，上图中的 index0 也就是数据缓存，⽽ index1 则是指令缓存，它两的⼤⼩通常是⼀样的。另外，你也会注意到，L3 Cache ⽐ L1 Cache 和 L2 Cache ⼤很多，这是因为 **L1 Cache和L2 Cache都是每个CPU核心特有的，而L3 Cache是多个CPU核心共享的。**程序执⾏时，会先将内存中的数据加载到共享的 L3 Cache 中，再加载到每个核⼼独有的 L2 Cache，最后进⼊到最快的 L1 Cache，之后才会被 CPU 读取。它们之间的层级关系，如下图：

![image-20211209182158677](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091821740.png)

​	越靠近 CPU 核⼼的缓存其访问速度越快，CPU 访问 L1 Cache 只需要 2~4 个时钟周期，访问 L2 Cache⼤约 10~20 个时钟周期，访问 L3 Cache ⼤约 20~60 个时钟周期，⽽访问内存速度⼤概在 200~300个 时钟周期之间。所以，CPU  从  L1  Cache 读取数据的速度，相⽐从内存读取的速度，会快多倍。如下表格：

![image-20211209182227877](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091822993.png)

#### CPU Cache的数据结构和读取过程是怎么样的？

​	CPU Cache 的数据是从内存中读取过来的，它是以⼀⼩块⼀⼩块读取数据的，⽽不是按照单个数组元素来读取数据的，在 CPU Cache 中的，这样⼀⼩块⼀⼩块的数据，称为 **Cache Line（缓存块）**。

:::tip

![image-20211209182731593](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091827652.png)

:::

​	⽐如，有⼀个 int array[100] 的数组，当载⼊ array[0] 时，由于这个数组元素的⼤⼩在内存只占 4 字节，不⾜ 64 字节，CPU 就会**顺序加载**数组元素到 array[15] ，意味着 array[0]~array[15] 数组元素都会被缓存在 CPU Cache 中了，因此当下次访问这些数组元素时，会直接从 CPU Cache 读取，⽽不⽤再从内存中读取，⼤⼤提⾼了 CPU 读取数据的性能。事实上，CPU 读取数据的时候，⽆论数据是否存放到 Cache 中，CPU 都是先访问 Cache，只有当 Cache中找不到数据时，才会去访问内存，并把内存中的数据读⼊到 Cache 中，CPU 再从 CPU Cache 读取数据。

![image-20211209182913147](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091829247.png)

​	

​	这样的访问机制，跟我们使⽤「内存作为硬盘的缓存」的逻辑是⼀样的，如果内存有缓存的数据，则直接返回，否则要访问⻳速⼀般的硬盘。那 CPU 怎么知道要访问的内存数据，是否在 Cache ⾥？如果在的话，如何找到 Cache 对应的数据呢？我们从最简单、基础的**直接映射Cache（Direct Mapped Cache）** 说起，来看看整个 CPU Cache 的数据结构和访问逻辑。

​	前⾯，我们提到 CPU 访问内存数据时，是⼀⼩块⼀⼩块数据读取的，具体这⼀⼩块数据的⼤⼩，取决于`coherency_line_size` 的值，⼀般 64 字节。在内存中，这⼀块的数据我们称为**内存块（Block）**，读取的时候我们要拿到数据所在内存块的地址。对于直接映射 Cache 采⽤的策略，就是把内存块的地址始终「映射」在⼀个 **`CPU Line`（缓存块）** 的地址，⾄于映射关系实现⽅式，则是使⽤「取模运算」，取模运算的结果就是内存块地址对应的 `CPU Line`（缓存块） 的地址。举个例⼦，内存共被划分为 32 个内存块，CPU Cache 共有 8 个 `CPU Line`，假设 CPU 想要访问第 15 号内存块，如果 15 号内存块中的数据已经缓存在 CPU Line 中的话，则是⼀定映射在 7 号 CPU Line 中，因为 15 % 8 的值是 7。机智的你肯定发现了，使⽤取模⽅式映射的话，就会出现多个内存块对应同⼀个 CPU Line，⽐如上⾯的例⼦，除了 15 号内存块是映射在 7 号 CPU Line 中，还有 7 号、23 号、31 号内存块都是映射到 7 号 CPULine 中。

![image-20211209183416918](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091834024.png)

​	因此，为了区别不同的内存块，在对应的 CPU Line 中我们还会存储⼀个**组标记（Tag）**。这个组标记会记 录当前  CPU  Line 中存储的数据对应的内存块，我们可以⽤这个组标记来区分不同的内存块。除了组标记信息外，CPU Line 还有两个信息：

1. 从内存加载过来的实际存放**数据（Data）**
2. **有效位（Valid bit）**，它是用来标记对应的CPU Line中的数据是否有效的，如果有效位是0，无论CPU Line中是否有数据，CPU都会访问内存，重新加载数据。

​	CPU 在从 CPU Cache 读取数据的时候，并不是读取 CPU Line 中的整个数据块，⽽是读取 CPU 所需要的⼀个数据⽚段，这样的数据统称为⼀个**字（word）**。那怎么在对应的 CPU Line 中数据块中找到所需的字呢？答案是，需要⼀个**偏移量(offset)**。因此，⼀个内存的访问地址，包括**组标记、CPU Line索引、偏移量**这三种信息，于是 CPU 就能通过这些信息，在 CPU Cache 中找到缓存的数据。⽽对于 CPU Cache ⾥的数据结构，则是由**索引+有效位+组标记+数据块**组成。

![image-20211210085818864](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112100858387.png)

​	如果内存中的数据已经在 CPU Cahe 中了，那 CPU 访问⼀个内存地址的时候，会经历这 4 个步骤：

1. 根据内存地址中索引信息，计算在 CPU Cahe 中的索引，也就是找出对应的 CPU Line 的地址；

2. 找到对应 CPU Line 后，判断 CPU Line 中的有效位，确认 CPU Line 中数据是否是有效的，如果是⽆

效的，CPU 就会直接访问内存，并重新加载数据，如果数据有效，则往下执⾏；

3. 对⽐内存地址中组标记和 CPU Line 中的组标记，确认 CPU Line 中的数据是我们要访问的内存数

据，如果不是的话，CPU 就会直接访问内存，并重新加载数据，如果是的话，则往下执⾏；

4. 根据内存地址中偏移量信息，从 CPU Line 的数据块中，读取对应的字。

​		到这⾥，相信你对直接映射 Cache 有了⼀定认识，但其实除了直接映射 Cache 之外，还有其他通过内存地址找到 CPU Cache 中的数据的策略，⽐如全相连 Cache （*Fully Associative Cache*）、组相连 Cache（*Set Associative Cache*）等，这⼏种策策略的数据结构都⽐较相似，我们理解了直接映射 Cache 的⼯作⽅式，其他的策略如果你有兴趣去看，相信很快就能理解的了。

#### 如何写出让 CPU 跑得更快的代码？

​	我们知道 CPU 访问内存的速度，⽐访问 CPU Cache 的速度慢了 100 多倍，所以如果 CPU 所要操作的数据在 CPU Cache 中的话，这样将会带来很⼤的性能提升。访问的数据在 CPU Cache 中的话，意味着**缓存命中**，缓存命中率越⾼的话，代码的性能就会越好，CPU 也就跑的越快。于是，「如何写出让 CPU 跑得更快的代码？」这个问题，可以改成「如何写出 CPU 缓存命中率⾼的代码？」。在前⾯我也提到， L1 Cache 通常分为「数据缓存」和「指令缓存」，这是因为 CPU 会别处理数据和指令，⽐如 1+1=2 这个运算， + 就是指令，会被放在「指令缓存」中，⽽输⼊数字 1 则会被放在「数据缓存」⾥。因此，**我们要分开来看「数据缓存」和「指令缓存」的缓存命中率**。

###### 如何提升数据缓存的命中率？

​	假设要遍历⼆维数组，有以下两种形式，虽然代码执⾏结果是⼀样，但你觉得哪种形式效率最⾼呢？为什么⾼呢？

![image-20211210090453889](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112100904208.png)

​	经过测试，形式⼀` array[i][j] `执⾏时间⽐形式⼆ `array[j][i] `快好⼏倍。之所以有这么⼤的差距，是因为⼆维数组 array 所占⽤的内存是连续的，⽐如⻓度 N 的指是 2 的话，那么内存中的数组元素的布局顺序是这样的：

![image-20211210090635448](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112100906589.png)

​	形式⼀⽤` array[i][j] `访问数组元素的顺序，正是和内存中数组元素存放的顺序⼀致。当 CPU 访问`array[0][0]` 时，由于该数据不在 Cache 中，于是会「顺序」把跟随其后的 3 个元素从内存中加载到 CPU Cache，这样当 CPU 访问后⾯的 3 个数组元素时，就能在 CPU Cache 中成功地找到数据，这意味着缓存命中率很⾼，缓存命中的数据不需要访问内存，这便⼤⼤提⾼了代码的性能。

⽽如果⽤形式⼆的 `array[j][i] `来访问，则访问的顺序就是：

![image-20211210090836147](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112100908370.png)

​	你可以看到，访问的⽅式跳跃式的，⽽不是顺序的，那么如果 N 的数值很⼤，那么操作` array[j][i] `时，是没办法把 `array[j+1][i] `也读⼊到 CPU Cache 中的，既然 `array[j+1][i] `没有读取到 CPU Cache，那么就需要从内存读取该数据元素了。很明显，这种不连续性、跳跃式访问数据元素的⽅式，可能不能充分利⽤到了 CPU Cache 的特性，从⽽代码的性能不⾼。那访问` array[0][0] `元素时，CPU 具体会⼀次从内存中加载多少元素到 CPU Cache 呢？这个问题，在前⾯我们也提到过，这跟 CPU Cache Line 有关，它表示 **CPU Cache** **⼀次性能加载数据的⼤⼩**，可以在Linux ⾥通过 coherency_line_size 配置查看 它的⼤⼩，通常是 64 个字节。

![image-20211210091105571](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112100911803.png)

​	也就是说，当 CPU 访问内存数据时，如果数据不在 CPU Cache 中，则会⼀次性会连续加载 64 字节⼤⼩的数据到 CPU Cache，那么当访问` array[0][0] `时，由于该元素不⾜ 64 字节，于是就会往后**顺序**`array[0][0]~array[0][15] `到 CPU Cache 中。顺序访问的 `array[i][j] `因为利⽤了这⼀特点，所以就会⽐跳跃式访问的 array[j][i] 要快。**因此，遇到这种遍历数组的情况时，按照内存布局顺序访问，将可以有效的利⽤** **CPU Cache** **带来的好处，这样我们代码的性能就会得到很⼤的提升。**

###### 如何提升指令缓存的命中率？

​	提升数据的缓存命中率的⽅式，是按照内存布局顺序访问，那针对指令的缓存该如何提升呢？我们以⼀个例⼦来看看，有⼀个元素为 0 到 100 之间随机数字组成的⼀维数组：

```c
int array[N];
for (i = 0; i <　N; i++) {
	array[i] = rand() % 100;
}
```

接下来，对这个数组做两个操作：

```C
// 操作一： 数组遍历: 循环遍历数组，把小于50的数组元素置为0；
for(i=0;i<N;i++){
    if(array[i] < 50) {
    	array[i] = 0;
    }
}
// 操作二：排序: 将数组排序
sort(array, array + N);
```

​	那么问题来了，你觉得先遍历再排序速度快，还是先排序再遍历速度快呢？在回答这个问题之前，我们先了解 CPU 的**分⽀预测器**。对于 if 条件语句，意味着此时⾄少可以选择跳转到两段不同的指令执⾏，也就是 if 还是 else 中的指令。那么，**如果分⽀预测可以预测到接下来要执⾏if⾥的指令，还是else指令的话，就可以「提前」把这些指令放在指令缓存中，这样CPU可以直接从Cache读取到指令，于是执⾏速度就会很快**。

​	当数组中的元素是随机的，分⽀预测就⽆法有效⼯作，⽽当数组元素都是是顺序的，分⽀预测器会动态地根据历史命中数据对未来进⾏预测，这样命中率就会很⾼。因此，先排序再遍历速度会更快，这是因为排序之后，数字是从⼩到⼤的，那么前⼏次循环命中 if < 50的次数会⽐较多，于是分⽀预测就会缓存 if ⾥的 array[i] = 0 指令到 Cache 中，后续 CPU 执⾏该指令就只需要从 Cache 读取就好了。如果你肯定代码中的 if 中的表达式判断为 true 的概率⽐较⾼，我们可以使⽤显示分⽀预测⼯具，⽐如在 C/C++ 语⾔中编译器提供了 likely 和 unlikely 这两种宏，如果 if 条件为 ture 的概率⼤，则可以⽤ likely 宏把 if ⾥的表达式包裹起来，反之⽤ unlikely 宏。实际上，CPU ⾃身的动态分⽀预测已经是⽐较准的了，所以只有当⾮常确信 CPU 预测的不准，且能够知道实际的概率情况时，才建议使⽤这两种宏。

![image-20211210133740298](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101337506.png)

###### **如果提升多核** **CPU** **的缓存命中率？**

​	在单核 CPU，虽然只能执⾏⼀个进程，但是操作系统给每个进程分配了⼀个时间⽚，时间⽚⽤完了，就调度下⼀个进程，于是各个进程就按时间⽚交替地占⽤ CPU，从宏观上看起来各个进程同时在执⾏。⽽现代 CPU 都是多核⼼的，进程可能在不同 CPU 核⼼来回切换执⾏，这对 CPU Cache 不是有利的，虽然 L3 Cache 是多核⼼之间共享的，但是 L1 和 L2 Cache 都是每个核⼼独有的，**如果⼀个进程在不同核⼼来回切换，各个核⼼的缓存命中率就会受到影响**，相反如果进程都在同⼀个核⼼上执⾏，那么其数据的 L1和 L2 Cache 的缓存命中率可以得到有效提⾼，缓存命中率⾼就意味着 CPU 可以减少访问 内存的频率。当有多个同时执⾏「计算密集型」的线程，为了防⽌因为切换到不同的核⼼，⽽导致缓存命中率下降的问题，我们可以把**线程绑定在某⼀个** **CPU** **核⼼上**，这样性能可以得到⾮常可观的提升。在 Linux 上提供了 `sched_setaffinity` ⽅法，来实现将线程绑定到某个 CPU 核⼼这⼀功能。

![image-20211210133942144](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101339239.png)

###### 总结

​	由于随着计算机技术的发展，CPU 与 内存的访问速度相差越来越多，如今差距已经⾼达好⼏百倍了，所以CPU 内部嵌⼊了 CPU Cache 组件，作为内存与 CPU 之间的缓存层，CPU Cache 由于离 CPU 核⼼很近，所以访问速度也是⾮常快的，但由于所需材料成本⽐较⾼，它不像内存动辄⼏个 GB ⼤⼩，⽽是仅有⼏⼗ KB 到 MB ⼤⼩。当 CPU 访问数据的时候，先是访问 CPU Cache，如果缓存命中的话，则直接返回数据，就不⽤每次都从内存读取速度了。因此，缓存命中率越⾼，代码的性能越好。但需要注意的是，当 CPU 访问数据时，如果 CPU Cache 没有缓存该数据，则会从内存读取数据，但是并不是只读⼀个数据，⽽是⼀次性读取⼀块⼀块的数据存放到 CPU Cache 中，之后才会被 CPU 读取。内存地址映射到 CPU Cache 地址⾥的策略有很多种，其中⽐较简单是直接映射 Cache，它巧妙的把内存地址拆分成「索引 + 组标记 + 偏移量」的⽅式，使得我们可以将很⼤的内存地址，映射到很⼩的 CPU Cache 地址⾥。要想写出让 CPU 跑得更快的代码，就需要写出缓存命中率⾼的代码，CPU L1 Cache 分为数据缓存和指令缓存，因⽽需要分别提⾼它们的缓存命中率：

- 对于数据缓存，我们在遍历数据的时候，应该按照内存布局的顺序操作，这是因为 CPU Cache 是根据 CPU Cache Line 批量操作数据的，所以顺序地操作连续内存数据时，性能能得到有效的提升；

- 对于指令缓存，有规律的条件分⽀语句能够让 CPU 的分⽀预测器发挥作⽤，进⼀步提⾼执⾏的效率；

​	另外，对于多核 CPU 系统，线程可能在不同 CPU 核⼼来回切换，这样各个核⼼的缓存命中率就会受到影响，于是要想提⾼进程的缓存命中率，可以考虑把线程绑定 CPU 到某⼀个 CPU 核⼼。

### 1.4 CPU缓存一致性

![image-20211210134253734](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101342827.png)

#### CPU Cache的数据的写入

​	随着时间的推移，CPU 和内存的访问性能相差越来越⼤，于是就在 CPU 内部嵌⼊了 CPU Cache（⾼速缓存），CPU Cache 离 CPU 核⼼相当近，因此它的访问速度是很快的，于是它充当了 CPU 与内存之间的缓存⻆⾊。CPU Cache 通常分为三级缓存：L1 Cache、L2 Cache、L3 Cache，级别越低的离 CPU 核⼼越近，访问速度也快，但是存储容量相对就会越⼩。其中，在多核⼼的 CPU ⾥，每个核⼼都有各⾃的 L1/L2 Cache， ⽽ L3 Cache 是所有核⼼共享使⽤的。

![image-20211210134420407](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101344480.png)

​	我们先简单了解下 CPU Cache 的结构，CPU Cache 是由很多个 Cache Line 组成的，CPU Line 是 CPU从内存读取数据的基本单位，⽽ CPU Line 是由各种标志（Tag）+ 数据块（Data Block）组成，你可以在下图清晰的看到：

![image-20211210134457232](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101344332.png)

​	我们当然期望 CPU 读取数据的时候，都是尽可能地从 CPU Cache 中读取，⽽不是每⼀次都要从内存中获取数据。所以，身为程序员，我们要尽可能写出缓存命中率⾼的代码，这样就有效提⾼程序的性能，具体的做法，你可以参考我上⼀篇⽂章「如何写出让 CPU 跑得更快的代码？」。事实上，数据不光是只有读操作，还有写操作，那么如果数据写⼊ Cache 之后，内存与 Cache 相对应的数据将会不同，这种情况下 Cache 和内存数据都不⼀致了，于是我们肯定是要把 Cache 中的数据同步到内存⾥的。问题来了，那在什么时机才把 Cache 中的数据写回到内存呢？为了应对这个问题，下⾯介绍两种针对写⼊数据的⽅法：

- **写直达**（Write Through）
- **写回**（Write Back）

##### 写直达

​	保持内存与 Cache ⼀致性最简单的⽅式是，**把数据同时写⼊内存和 Cache** **中**，这种⽅法称为**写直达（Write Through）**。在这个⽅法⾥，写⼊前会先判断数据是否已经在 CPU Cache ⾥⾯了：

- 如果数据已经在Cache里面，先将数据更新到Cache里面，再写人到内存里面；
- 如果数据没在Cache里面，就直接把数据更新到内存里面。

![image-20211210134808530](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101348629.png)

​	写直达法很直观，也很简单，但是问题明显，⽆论数据在不在 Cache ⾥⾯，每次写操作都会写回到内存，这样写操作将会花费⼤量的时间，⽆疑性能会受到很⼤的影响。



##### 写回

​	既然写直达由于每次写操作都会把数据写回到内存，⽽导致影响性能，于是为了要减少数据写回内存的频 率，就出现了**写回（Write Back）**的方法。在写回机制中，**当发生写操作时，新数据仅仅被写人Cache Block里，只有当修改过的Cache Block【被替换】时才需要写到内存中**，减少了数据写回内存的频率，这样便可以提⾼系统的性能。

![未命名文件](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101404849.png)

具体流程为：

1. 如果当发生写操作时，数据已经在CPU Cache里的话，则把数据更新到CPU Cache里，同时标记CPU Cache里的这个Cache Block为脏（Dirty）的，这个脏的标记表示这个时候，我们CPU Cache里面的这个Cache Block的数据和内存是不一致的，这种情况是不用把数据写到内存里的。
2. 如果发生写操作时，数据所对应的 Cache Block ⾥存放的是「别的内存地址的数据」的话，就要检 查这个  Cache  Block ⾥的数据有没有被标记为脏的，如果是脏的话，我们就要把这个  Cache Block⾥的数据写回到内存，然后再把当前要写⼊的数据，写⼊到这个 Cache Block ⾥，同时也把它标记为 脏的；如果  Cache  Block  ⾥⾯的数据没有被标记为脏，则就直接将数据写⼊到这个  Cache Block⾥，然后再把这个  Cache  Block 标记为脏的就好了。

​	可以发现写回这个⽅法，在把数据写⼊到  Cache 的时候，只有在缓存不命中，同时数据对应的  Cache 中 的 Cache Block 为脏标记的情况下，才会将数据写到内存中，⽽在缓存命中的情况下，则在写⼊后 Cache 后，只需把该数据对应的  Cache  Block 标记为脏即可，⽽不⽤写到内存⾥。这样的好处是，如果我们⼤量的操作都能够命中缓存，那么⼤部分时间⾥ CPU 都不需要读写内存，⾃然性能相⽐写直达会⾼很多。



##### 缓存一致性问题

​	现在  CPU  都是多核的，由于  L1/L2  Cache 是多个核⼼各⾃独有的，那么会带来多核⼼的 **缓存一致性（Cache Coherence）**的问题，如果不能保证缓存一致性的问题，就可能造成结果错误。那缓存⼀致性的问题具体是怎么发⽣的呢？我们以⼀个含有两个核⼼的 CPU 作为例⼦看⼀看。 假设 A 号核⼼和 B 号核⼼同时运⾏两个线程，都操作共同的变量 i（初始值为 0  ）。

![image-20211210141407313](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101414419.png)

​	这时如果 A 号核⼼执⾏了 i++ 语句的时候，为了考虑性能，使⽤了我们前⾯所说的写回策略，先把值为1 的执⾏结果写⼊到 L1/L2 Cache 中，然后把 L1/L2 Cache 中对应的 Block 标记为脏的，这个时候数据其实没有被同步到内存中的，因为写回策略，只有在 A 号核⼼中的这个 Cache Block 要被替换的时候，数据才会写⼊到内存⾥。如果这时旁边的 B 号核⼼尝试从内存读取 i 变量的值，则读到的将会是错误的值，因为刚才 A 号核⼼更新i 值还没写⼊到内存中，内存中的值还依然是 0。**这个就是所谓的缓存一致性问题，A号核心和B号核心的缓存，在这个时候是不一致的，从而导致执行结果的错误。**

 ![image-20211210141656484](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101416565.png)

​	那么，要解决这⼀问题，就需要⼀种机制，来同步两个不同核⼼⾥⾯的缓存数据。要实现的这个机制的 话，要保证做到下⾯这  2 点：

1. 某个  CPU 核⼼⾥的  Cache 数据更新时，必须要传播到其他核⼼的  Cache，这个称为 **写传播（Write Propagation）**
2. 某个CPU核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的，这个称为 **事务的串形化（Transaction Serialization）**

​	第⼀点写传播很容易就理解，当某个核⼼在 Cache 更新了数据，就需要同步到其他核⼼的 Cache ⾥。⽽ 对于第⼆点事务事的串形化，我们举个例⼦来理解它。假设我们有⼀个含有 4 个核⼼的 CPU，这 4 个核⼼都操作共同的变量 i（初始值为 0 ）。A 号核⼼先把 i值变为 100，⽽此时同⼀时间，B 号核⼼先把 i 值变为 200，这⾥两个修改，都会「传播」到 C 和 D 号核⼼。那么问题就来了，C 号核⼼先收到了  A 号核⼼更新数据的事件，再收到  B 号核⼼更新数据的事件，因此  C号核⼼看到的变量  i 是先变成  100，后变成 200。⽽如果 D 号核⼼收到的事件是反过来的，则 D 号核⼼看到的是变量 i 先变成 200，再变成 100，虽然是做 到了写传播，但是各个  Cache ⾥⾯的数据还是不⼀致的。所以，我们要保证 C 号核⼼和 D 号核⼼都能看到相同**顺序的数据变化**，⽐如变量 i 都是先变成 100，再变 成  200，这样的过程就是事务的串形化。

 

![image-20211210142032941](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101420041.png)

要实现事务串形化，要做到2点：

1. CPU核心对于 Cache 中数据的操作，需要同步给其他  CPU 核⼼；
2. 要引入“锁”的概念，如果两个 CPU 核⼼⾥有相同数据的 Cache，那么对于这个 Cache 数据的更 新，只有拿到了「锁」，才能进⾏对应的数据更新。

那接下来我们看看，写传播和事务串形化具体是⽤什么技术实现的。



#### 总线嗅探

​	写传播的原则就是当某个 CPU 核⼼更新了 Cache 中的数据，要把该事件⼴播通知到其他核⼼。最常⻅实现的⽅式是 **总线嗅探（Bus Snooping）**。

​	我还是以前⾯的 i 变量例⼦来说明总线嗅探的⼯作机制，当 A 号 CPU 核⼼修改了 L1 Cache 中 i 变量的值，通过总线把这个事件⼴播通知给其他所有的核⼼，然后每个 CPU 核⼼都会监听总线上的⼴播事件，并检查是否有相同的数据在⾃⼰的 L1 Cache ⾥⾯，如果 B 号 CPU 核⼼的 L1 Cache 中有该数据，那么也需要把该数据更新到⾃⼰的 L1 Cache。

​	可以发现，总线嗅探⽅法很简单， CPU 需要每时每刻监听总线上的⼀切活动，但是不管别的核⼼的Cache 是否缓存相同的数据，都需要发出⼀个⼴播事件，这⽆疑会加重总线的负载。另外，总线嗅探只是保证了某个 CPU 核⼼的 Cache 更新数据这个事件能被其他 CPU 核⼼知道，但是并不能保证事务串形化。于是，有⼀个协议基于总线嗅探机制实现了事务串形化，也⽤状态机机制降低了总线带宽压⼒，这个协议就是 MESI 协议，这个协议就做到了 CPU 缓存⼀致性。

#### MESI协议

MESI 协议其实是 4 个状态单词的开头字⺟缩写，分别是：

- `Modified` ： 已修改
- `Exclusive`: 独占
- `Shared`：共享
- `Invalidated`: 已失效

这四个状态来标记Cache Line四个不同的状态。

​	「已修改」状态就是我们前⾯提到的脏标记，代表该 Cache Block 上的数据已经被更新过，但是还没有写到内存⾥。

​	「已失效」状态，表示的是这个 Cache Block ⾥的数据已经失效了，不可以读取该状态的数据。

​	「独占」和「共享」状态都代表 Cache Block ⾥的数据是⼲净的，也就是说，这个时候 Cache Block ⾥的数据和内存⾥⾯的数据是⼀致性的。「独占」和「共享」的差别在于，独占状态的时候，数据只存储在⼀个 CPU 核⼼的 Cache ⾥，⽽其他CPU 核⼼的 Cache 没有该数据。这个时候，如果要向独占的 Cache 写数据，就可以直接⾃由地写⼊，⽽不需要通知其他 CPU 核⼼，因为只有你这有这个数据，就不存在缓存⼀致性的问题了，于是就可以随便操作该数据。另外，在「独占」状态下的数据，如果有其他核⼼从内存读取了相同的数据到各⾃的 Cache ，那么这个时候，独占状态下的数据就会变成共享状态。那么，「共享」状态代表着相同的数据在多个 CPU 核⼼的 Cache ⾥都有，所以当我们要更新 Cache ⾥⾯的数据的时候，不能直接修改，⽽是要先向所有的其他 CPU 核⼼⼴播⼀个请求，要求先把其他核⼼的Cache 中对应的 Cache Line 标记为「⽆效」状态，然后再更新当前 Cache ⾥⾯的数据。

我们举个具体的例⼦来看看这四个状态的转换：

1.  当 A 号 CPU 核⼼从内存读取变量 i 的值，数据被缓存在 A 号 CPU 核⼼⾃⼰的 Cache ⾥⾯，此时其他 CPU 核⼼的 Cache 没有缓存该数据，于是标记 Cache Line 状态为「独占」，此时其 Cache 中的数据与内存是⼀致的；

2. 然后 B 号 CPU 核⼼也从内存读取了变量 i 的值，此时会发送消息给其他 CPU 核⼼，由于 A 号 CPU

核⼼已经缓存了该数据，所以会把数据返回给 B 号 CPU 核⼼。在这个时候， A 和 B 核⼼缓存了相同

的数据，Cache Line 的状态就会变成「共享」，并且其 Cache 中的数据与内存也是⼀致的；

3. 当 A 号 CPU 核⼼要修改 Cache 中 i 变量的值，发现数据对应的 Cache Line 的状态是共享状态，则要向所有的其他 CPU 核⼼⼴播⼀个请求，要求先把其他核⼼的 Cache 中对应的 Cache Line 标记为「⽆效」状态，然后 A 号 CPU 核⼼才更新 Cache ⾥⾯的数据，同时标记 Cache Line 为「已修改」状态，此时 Cache 中的数据就与内存不⼀致了。

4. 如果 A 号 CPU 核⼼「继续」修改 Cache 中 i 变量的值，由于此时的 Cache Line 是「已修改」状态，因此不需要给其他 CPU 核⼼发送消息，直接更新数据即可。

5. 如果 A 号 CPU 核⼼的 Cache ⾥的 i 变量对应的 Cache Line 要被「替换」，发现 Cache Line 状态是「已修改」状态，就会在替换前先把数据同步到内存。

​	所以，可以发现当 Cache Line 状态是「已修改」或者「独占」状态时，修改更新其数据不需要发送⼴播给其他 CPU 核⼼，这在⼀定程度上减少了总线带宽压⼒。事实上，整个 MESI 的状态可以⽤⼀个有限状态机来表示它的状态流转。还有⼀点，对于不同状态触发的事件操作，可能是来⾃本地 CPU 核⼼发出的⼴播事件，也可以是来⾃其他 CPU 核⼼通过总线发出的⼴播、事件。

下图即是 MESI 协议的状态图：

![image-20211210145407957](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101454067.png)

​	MESI 协议的四种状态之间的流转过程，我汇总成了下⾯的表格，你可以更详细的看到每个状态转换的原 因：

![image-20211210145442911](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101454040.png)

![image-20211210145453749](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101454890.png)



#### 总结

​	CPU 在读写数据的时候，都是在 CPU Cache 读写数据的，原因是 Cache 离 CPU 很近，读写性能相⽐内 存⾼出很多。对于 Cache ⾥没有缓存 CPU 所需要读取的数据的这种情况，CPU 则会从内存读取数据，并 将数据缓存到 Cache ⾥⾯，最后 CPU 再从 Cache  读取数据。⽽对于数据的写⼊，CPU 都会先写⼊到 Cache ⾥⾯，然后再在找个合适的时机写⼊到内存，那就有「写 直达」和「写回」这两种策略来保证  Cache  与内存的数据⼀致性：

- **写直达**： 只要有数据写⼊，都会直接把数据写⼊到内存⾥⾯，这种⽅式简单直观，但是性能就会受限 于内存的访问速度；
- **写回**： 对于已经缓存在 Cache 的数据的写⼊，只需要更新其数据就可以，不⽤写⼊到内存，只有在 需要把缓存⾥⾯的脏数据交换出去的时候，才把数据同步到内存⾥，这种⽅式在缓存命中率⾼的情 况，性能会更好；

​	当今 CPU 都是多核的，每个核⼼都有各⾃独⽴的 L1/L2 Cache，只有 L3 Cache 是多个核⼼之间共享的。 所以，我们要确保多核缓存是⼀致性的，否则会出现错误的结果。要想实现缓存⼀致性，关键是要满⾜  2 点：

1. **写传播**： 当某个  CPU  核⼼发⽣写⼊操作时，需要把该事件⼴播通知给其他核⼼；
2. **事务串形化**： ，这个很重要，只有保证了这个，才能保障我们的数据是真正⼀致的，我们的 程序在各个不同的核⼼上运⾏的结果也是⼀致的；

​	基于总线嗅探机制的  MESI 协议，就满⾜上⾯了这两点，因此它是保障缓存⼀致性的协议。MESI 协议，是已修改、独占、共享、已实现这四个状态的英⽂缩写的组合。整个 MSI 状态的变更，则是 根据来⾃本地 CPU 核⼼的请求，或者来⾃其他 CPU 核⼼通过总线传输过来的请求，从⽽构成⼀个流动的 状态机。另外，对于在「已修改」或者「独占」状态的 Cache Line，修改更新其数据不需要发送⼴播给其 他 CPU 核⼼。

### 1.5 CPU是如何执行任务的？

你清楚下⾯这⼏个问题吗？

- 有了内存，为什么还需要 CPU Cache？
- CPU 是怎么读写数据的？
- 如何让 CPU 能读取数据更快⼀些？
- CPU 伪共享是如何发⽣的？⼜该如何避免？
- CPU 是如何调度任务的？如果你的任务对响应要求很⾼，你希望它总是能被先调度，这该怎么办？
- ...

这篇，我们就来回答这些问题。

 ![image-20211210145957324](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101459407.png)



#### 1.5.1 CPU如何读写数据的？

![现代CPU 的架构图](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101501975.png)

​	由上图的CPU架构图可知，⼀个 CPU ⾥通常会有多个 CPU 核⼼，⽐如上图中的 1 号和 2 号 CPU 核⼼，并且每个 CPU 核⼼都有⾃⼰的 L1 Cache 和 L2 Cache，⽽ L1 Cache 通常分为 dCache（数据缓存） 和 iCache（指令缓 存），L3 Cache 则是多个核⼼共享的，这就是  CPU  典型的缓存层次。

​	上⾯提到的都是 CPU 内部的 Cache，放眼外部的话，还会有内存和硬盘，这些存储设备共同构成了⾦字 塔存储层次。如下图所示：

![image-20211210150317762](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101503869.png)

​	从上图也可以看到，从上往下，存储设备的容量会越⼤，⽽访问速度会越慢。⾄于每个存储设备的访问延 时，你可以看下图的表格：

![image-20211210150338979](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101503062.png)

​	你可以看到， CPU 访问 L1 Cache 速度⽐访问内存快 100 倍，这就是为什么 CPU ⾥会有 L1~L3 Cache的原因，⽬的就是把  Cache  作为  CPU 与内存之间的缓存层，以减少对内存的访问频率。CPU 从内存中读取数据到 Cache 的时候，并不是⼀个字节⼀个字节读取，⽽是⼀块⼀块的⽅式来读取数 据的，这⼀块⼀块的数据被称为 CPU Line（缓存⾏），所以 **CPU Line 是 CPU 从内存读取数据到 Cache 的单位**。⾄于 CPU Line ⼤⼩，在 Linux 系统可以⽤下⾯的⽅式查看到，你可以看我服务器的 L1 Cache Line ⼤⼩ 是 64 字节，也就意味 L1 Cache ⼀次载⼊数据的⼤⼩是 64  字节。

![image-20211210150508675](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101505743.png)

​	那么对数组的加载， CPU 就会加载数组⾥⾯连续的多个数据到 Cache ⾥，因此我们应该按照物理内存地 址分布的顺序去访问元素，这样访问数组元素的时候，Cache 命中率就会很⾼，于是就能减少从内存读取 数据的频率，  从⽽可提⾼程序的性能。但是，在我们不使⽤数组，⽽是使⽤单独的变量的时候，则会有**Cache 伪共享**的问题，**Cache 伪共享**问 题上是⼀个性能杀⼿，我们应该要规避它。接下来，就来看看  Cache  伪共享是什么？⼜如何避免这个问题？

​	现在假设有⼀个双核⼼的  CPU，这两个  CPU  核⼼并⾏运⾏着两个不同的线程，它们同时从内存中读取两个不同的数据，分别是类型为`long`的变量  A  和 B，这个两个数据的地址在物理内存上是**连续**的，如果Cahce Line 的⼤⼩是 64 字节，并且变量 A 在 Cahce Line   的开头位置，那么这两个数据是位于**同⼀个Cache Line** 中，⼜因为 CPU Line 是 CPU 从内存读取数据到 Cache   的单位，所以这两个数据会被同时读⼊到了两个 CPU 核⼼中各⾃ Cache  中。我们来思考⼀个问题，如果这两个不同核⼼的线程分别修改不同的数据，⽐如 1 号 CPU 核⼼的线程只修 改了 变量 A，或 2 号 CPU 核⼼的线程的线程只修改了变量   B，会发⽣什么呢？

![image-20211210150815513](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101508586.png)

##### 分析伪共享的问题

​	现在我们结合保证多核缓存⼀致的 MESI 协议，来说明这⼀整个的过程。

1. 最开始变量 A 和 B 都还不在 Cache ⾥⾯，假设 1 号核⼼绑定了线程 A，2 号核⼼绑定了线程 B，线程 A 只会读写变量 A，线程 B 只会读写变量  B。

   ![](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101511538.png)

2. 1 号核⼼读取变量 A，由于 CPU 从内存读取数据到 Cache 的单位是 Cache Line，也正好变量 A 和 变 量 B 的数据归属于同⼀个 Cache Line，所以 A 和 B 的数据都会被加载到 Cache，并将此 Cache Line 标 记为「独占」状态。

![image-20211210151203943](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101512018.png)

3. 接着，2 号核⼼开始从内存⾥读取变量 B，同样的也是读取 Cache Line ⼤⼩的数据到 Cache 中，此 Cache Line 中的数据也包含了变量 A 和 变量 B，此时 1 号和 2 号核⼼的 Cache Line 状态变为「共享」状 态。

![image-20211210151218627](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101512712.png)

4. 1 号核⼼需要修改变量 A，发现此  Cache Line 的状态是「共享」状态，所以先需要通过总线发送消息 给 2 号核⼼，通知 2 号核⼼把 Cache 中对应的 Cache Line 标记为「已失效」状态，然后 1 号核⼼对应的 Cache  Line  状态变成「已修改」状态，并且修改变量 A。

![image-20211210151301822](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101513896.png)

5. 之后，2 号核⼼需要修改变量 B，此时 2 号核⼼的 Cache 中对应的 Cache Line 是已失效状态，另外由 于 1 号核⼼的 Cache 也有此相同的数据，且状态为「已修改」状态，所以要先把 1 号核⼼的 Cache 对应 的 Cache Line 写回到内存，然后 2 号核⼼再从内存读取 Cache Line ⼤⼩的数据到 Cache 中，最后把变 量  B 修改到  2 号核⼼的  Cache 中，并将状态标记为「已修改」状态。

![image-20211210151331219](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101513320.png)

​	所以，可以发现如果 1 号和 2 号 CPU 核⼼这样持续交替的分别修改变量 A 和 B，就会重复 ④ 和 ⑤ 这两 个步骤，Cache 并没有起到缓存的效果，虽然变量 A 和 B 之间其实并没有任何的关系，但是因为同时归属 于⼀个 Cache Line ，这个 Cache Line 中的任意数据被修改后，都会相互影响，从⽽出现 ④ 和 ⑤ 这两个 步骤。因此，这种因为多个线程同时读写同⼀个  Cache Line 的不同变量时，⽽导致  CPU Cache  失效的现象称为 **伪共享（False Sharing）**。



##### 避免伪共享的方法

​	因此，对于多个线程共享的热点数据，即经常会修改的数据，应该避免这些数据刚好在同一个 Cache Line 中，否则就会出现为伪共享的问题。接下来，看看在实际项目中是用什么方式来避免伪共享的问题的。在 Linux 内核中存在 `__cacheline_aligned_in_smp` 宏定义，是用于解决伪共享的问题。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161337102.webp)

从上面的宏定义，我们可以看到：

- 如果在多核（MP）系统里，该宏定义是 `__cacheline_aligned`，也就是 Cache Line 的大小；
- 而如果在单核系统里，该宏定义是空的；

​	因此，针对在同一个 Cache Line 中的共享的数据，如果在多核之间竞争比较严重，为了防止伪共享现象的发生，可以采用上面的宏定义使得变量在 Cache Line 里是对齐的。

举个例子，有下面这个结构体：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161338343.webp)

​	结构体里的两个成员变量 a 和 b 在物理内存地址上是连续的，于是它们可能会位于同一个 Cache Line 中，如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161338714.webp)

​	所以，为了防止前面提到的 Cache 伪共享问题，我们可以使用上面介绍的宏定义，将 b 的地址设置为 Cache Line 对齐地址，如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161339633.webp)

这样 a 和 b 变量就不会在同一个 Cache Line 中了，如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161339521.webp)

​	所以，**避免  Cache 伪共享实际上是用空间换时间的思想，浪费一部分 Cache 空间，从而换来性能的提升。**

​	我们再来看一个应用层面的规避方案，有一个 Java 并发框架 Disruptor 使用「字节填充 + 继承」的方式，来避免伪共享的问题。Disruptor 中有一个 RingBuffer 类会经常被多个线程使用，代码如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161340507.webp)

​	你可能会觉得 RingBufferPad 类里 7 个 long 类型的名字很奇怪，但事实上，它们虽然看起来毫无作用，但却对性能的提升起到了至关重要的作用。

​	我们都知道，CPU Cache 从内存读取数据的单位是 CPU Line，一般 64 位 CPU 的 CPU Line 的大小是 64 个字节，一个 long 类型的数据是 8 个字节，所以 CPU 一下会加载 8 个 long 类型的数据。

​	根据 JVM 对象继承关系中父类成员和子类成员，内存地址是连续排列布局的，因此 RingBufferPad 中的 7 个 long 类型数据作为 Cache Line **前置填充**，而 RingBuffer 中的 7 个 long 类型数据则作为 Cache Line **后置填充**，这 14 个 long 变量没有任何实际用途，更不会对它们进行读写操作。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161340747.webp)

​		另外，RingBufferFelds 里面定义的这些变量都是 `final` 修饰的，意味着第一次加载之后不会再修改， 又**由于「前后」各填充了 7 个不会被读写的 long 类型变量，所以无论怎么加载 Cache Line，这整个 Cache Line 里都没有会发生更新操作的数据，于是只要数据被频繁地读取访问，就自然没有数据被换出 Cache 的可能，也因此不会产生伪共享的问题**。

#### 1.5.2 CPU如何选择线程的？

​	了解完 CPU 读取数据的过程后，我们再来看看 CPU 是根据什么来选择当前要执行的线程。在 Linux 内核中，进程和线程都是用 `tark_struct` 结构体表示的，区别在于线程的 tark_struct 结构体里部分资源是共享了进程已创建的资源，比如内存地址空间、代码段、文件描述符等，所以 Linux 中的线程也被称为轻量级进程，因为线程的 tark_struct 相比进程的 tark_struct 承载的 资源比较少，因此以「轻」得名。

​	一般来说，没有创建线程的进程，是只有单个执行流，它被称为是主线程。如果想让进程处理更多的事情，可以创建多个线程分别去处理，但不管怎么样，它们对应到内核里都是 `tark_struct`。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161343272.webp)

​	所以，Linux 内核里的调度器，调度的对象就是 `tark_struct`，接下来我们就把这个数据结构统称为**任务**。

在 Linux 系统中，根据任务的优先级以及响应要求，主要分为两种，其中优先级的数值越小，优先级越高：

- **实时任务**，对系统的响应时间要求很高，也就是要尽可能快的执行实时任务，优先级在 `0~99` 范围内的就算实时任务；
- **普通任务**，响应时间没有很高的要求，优先级在 `100~139` 范围内都是普通任务级别；

##### 1.5.2.1 调度类

​	由于任务有优先级之分，Linux 系统为了保障高优先级的任务能够尽可能早的被执行，于是分为了这几种调度类，如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161345157.webp)

​	Deadline 和 Realtime 这两个调度类，都是应用于实时任务的，这两个调度类的调度策略合起来共有这三种，它们的作用如下：

- ***SCHED_DEADLINE***：是按照 deadline 进行调度的，距离当前时间点最近的 deadline 的任务会被优先调度；
- ***SCHED_FIFO***：对于相同优先级的任务，按先来先服务的原则，但是优先级更高的任务，可以抢占低优先级的任务，也就是优先级高的可以「插队」；
- ***SCHED_RR***：对于相同优先级的任务，轮流着运行，每个任务都有一定的时间片，当用完时间片的任务会被放到队列尾部，以保证相同优先级任务的公平性，但是高优先级的任务依然可以抢占低优先级的任务；

而 Fair 调度类是应用于普通任务，都是由 CFS 调度器管理的，分为两种调度策略：

- ***SCHED_NORMAL***：普通任务使用的调度策略；
- ***SCHED_BATCH***：后台任务的调度策略，不和终端进行交互，因此在不影响其他需要交互的任务，可以适当降低它的优先级。

##### 1.5.2.2 完全公平调度

​	我们平日里遇到的基本都是普通任务，对于普通任务来说，公平性最重要，在 Linux 里面，实现了一个基于 CFS 的调度算法，也就是**完全公平调度（Completely Fair Scheduling）**。

​	这个算法的理念是想让分配给每个任务的 CPU 时间是一样，于是它为每个任务安排一个虚拟运行时间 `vruntime`，如果一个任务在运行，其运行的越久，该任务的 vruntime 自然就会越大，而没有被运行的任务，`vruntime` 是不会变化的。那么，**在 CFS 算法调度的时候，会优先选择 vruntime 少的任务**，以保证每个任务的公平性。这就好比，让你把一桶的奶茶平均分到 10 杯奶茶杯里，你看着哪杯奶茶少，就多倒一些；哪个多了，就先不倒，这样经过多轮操作，虽然不能保证每杯奶茶完全一样多，但至少是公平的。

​	当然，上面提到的例子没有考虑到优先级的问题，虽然是普通任务，但是普通任务之间还是有优先级区分的，所以在计算虚拟运行时间 vruntime 还要考虑普通任务的**权重值**，注意权重值并不是优先级的值，内核中会有一个 nice 级别与权重值的转换表，nice 级别越低的权重值就越大，至于 nice 值是什么，我们后面会提到。
于是就有了以下这个公式：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161348408.webp)

​	你可以不用管 NICE_0_LOAD 是什么，你就认为它是一个常量，那么在「同样的实际运行时间」里，高权重任务的 vruntime 比低权重任务的 vruntime **少**，你可能会奇怪为什么是少的？你还记得 CFS 调度吗，它是会优先选择 vruntime 少的任务进行调度，所以高权重的任务就会被优先调度了，于是高权重的获得的实际运行时间自然就多了。

##### 1.5.2.3 CPU运行队列

​	一个系统通常都会运行着很多任务，多任务的数量基本都是远超 CPU 核心数量，因此这时候就需要**排队**。事实上，每个 CPU 都有自己的**运行队列（Run Queue, rq）**，用于描述在此 CPU 上所运行的所有进程，其队列包含三个运行队列:

- Deadline 运行队列 dl_rq
- 实时任务运行队列 rt_rq 
- CFS 运行队列 csf_rq

​	其中 csf_rq 是用红黑树来描述的，按 vruntime 大小来排序的，最左侧的叶子节点，就是下次会被调度的任务。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161351824.webp)

​	这几种调度类是有优先级的，优先级如下：Deadline > Realtime > Fair，这意味着 Linux 选择下一个任务执行的时候，会按照此优先级顺序进行选择，也就是说先从 `dl_rq` 里选择任务，然后从 `rt_rq` 里选择任务，最后从 `csf_rq` 里选择任务。因此，**实时任务总是会比普通任务优先被执行**。



##### 1.5.2.4 调整优先级

​	如果我们启动任务的时候，没有特意去指定优先级的话，默认情况下都是普通任务，普通任务的调度类是 Fail，由 CFS 调度器来进行管理。CFS 调度器的目的是实现任务运行的公平性，也就是保障每个任务的运行的时间是差不多的。

​	如果你想让某个普通任务有更多的执行时间，可以调整任务的 `nice` 值，从而让优先级高一些的任务执行更多时间。nice 的值能设置的范围是 `-20～19`， 值越低，表明优先级越高，因此 -20 是最高优先级，19 则是最低优先级，默认优先级是 0。

​	是不是觉得 nice 值的范围很诡异？事实上，nice 值并不是表示优先级，而是表示优先级的修正数值，它与优先级（priority）的关系是这样的：priority(new) = priority(old) + nice。内核中，priority 的范围是 0~139，值越低，优先级越高，其中前面的 0~99 范围是提供给实时任务使用的，而 nice 值是映射到 100~139，这个范围是提供给普通任务用的，因此 nice 值调整的是普通任务的优先级。

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZdrBIMQROWxSKCX3uKvOOFzJF4jc0KXY6yELkleteo1gPyo1ZUTLndb0ptiaIH8wzpB9WibdnWicmAuw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​	在前面我们提到了，权重值与 nice 值的关系的，nice 值越低，权重值就越大，计算出来的 vruntime 就会越少，由于 CFS 算法调度的时候，就会优先选择 vruntime 少的任务进行执行，所以 nice 值越低，任务的优先级就越高。

我们可以在启动任务的时候，可以指定 nice 的值，比如将 mysqld 以 -3 优先级：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161354780.webp)

如果想修改已经运行中的任务的优先级，则可以使用 `renice` 来调整 nice 值：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161403465.webp)

​	nice 调整的是普通任务的优先级，所以不管怎么缩小 nice 值，任务永远都是普通任务，如果某些任务要求实时性比较高，那么你可以考虑改变任务的优先级以及调度策略，使得它变成实时任务，比如：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161354951.webp)

##### 1.5.2.5 总结

​	理解 CPU 是如何读写数据的前提，是要理解 CPU 的架构，CPU 内部的多个 Cache + 外部的内存和磁盘都就构成了金字塔的存储器结构，在这个金字塔中，越往下，存储器的容量就越大，但访问速度就会小。

​	CPU 读写数据的时候，并不是按一个一个字节为单位来进行读写，而是以 CPU Line 大小为单位，CPU Line 大小一般是 64 个字节，也就意味着 CPU 读写数据的时候，每一次都是以 64 字节大小为一块进行操作。

​	因此，如果我们操作的数据是数组，那么访问数组元素的时候，按内存分布的地址顺序进行访问，这样能充分利用到 Cache，程序的性能得到提升。但如果操作的数据不是数组，而是普通的变量，并在多核 CPU 的情况下，我们还需要避免 Cache Line 伪共享的问题。

​	所谓的 Cache Line 伪共享问题就是，多个线程同时读写同一个 Cache Line 的不同变量时，而导致 CPU Cache 失效的现象。那么对于多个线程共享的热点数据，即经常会修改的数据，应该避免这些数据刚好在同一个 Cache Line 中，避免的方式一般有 Cache Line 大小字节对齐，以及字节填充等方法。

​	系统中需要运行的多线程数一般都会大于 CPU 核心，这样就会导致线程排队等待 CPU，这可能会产生一定的延时，如果我们的任务对延时容忍度很低，则可以通过一些人为手段干预 Linux 的默认调度策略和优先级。

### 1.6 软中断

#### 1.6.1 中断是什么？

​	先来看看什么是中断？在计算机中，中断是系统用来响应硬件设备请求的一种机制，操作系统收到硬件的中断请求，会打断正在执行的进程，然后调用内核中的中断处理程序来响应请求。

​	这样的解释可能过于学术了，容易云里雾里，我就举个生活中取外卖的例子。小林中午搬完砖，肚子饿了，点了份白切鸡外卖。虽然平台上会显示配送进度，但是我也不能一直傻傻地盯着呀，时间很宝贵，当然得去干别的事情，等外卖到了配送员会通过「电话」通知我，电话响了，我就会停下手中地事情，去拿外卖。这里的打电话，其实就是对应计算机里的中断，没接到电话的时候，我可以做其他的事情，只有接到了电话，也就是发生中断，我才会停下当前的事情，去进行另一个事情，也就是拿外卖。从这个例子，我们可以知道，**中断是一种异步的事件处理机制，可以提高系统的并发处理能力**。

​	操作系统收到了中断请求，会打断其他进程的运行，所以**中断请求的响应程序，也就是中断处理程序，要尽可能快的执行完，这样可以减少对正常进程运行调度地影响。**而且，中断处理程序在响应中断时，可能还会「临时关闭中断」，这意味着，如果当前中断处理程序没有执行完之前，系统中其他的中断请求都无法被响应，也就说中断有可能会丢失，所以中断处理程序要短且快。

​	还是回到外卖的例子，小林到了晚上又点起了外卖，这次为了犒劳自己，共点了两份外卖，一份小龙虾和一份奶茶，并且是由不同地配送员来配送，那么问题来了，当第一份外卖送到时，配送员给我打了长长的电话，说了一些杂七杂八的事情，比如给个好评等等，但如果这时另一位配送员也想给我打电话。很明显，这时第二位配送员因为我在通话中（相当于关闭了中断响应），自然就无法打通我的电话，他可能尝试了几次后就走掉了（相当于丢失了一次中断）。



#### 1.6.2 什么是软中断？

​	前面我们也提到了，中断请求的处理程序应该要短且快，这样才能减少对正常进程运行调度地影响，而且中断处理程序可能会暂时关闭中断，这时如果中断处理程序执行时间过长，可能在还未执行完中断处理程序前，会丢失当前其他设备的中断请求。

​	那 Linux 系统**为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是「上半部和下半部分」**。

- **上半部用来快速处理中断**，一般会暂时关闭中断请求，主要负责处理跟硬件紧密相关或者时间敏感的事情。
- **下半部用来延迟处理上半部未完成的工作**，一般以「内核线程」的方式运行。

​	前面的外卖例子，由于第一个配送员长时间跟我通话，则导致第二位配送员无法拨通我的电话，其实当我接到第一位配送员的电话，可以告诉配送员说我现在下楼，剩下的事情，等我们见面再说（上半部），然后就可以挂断电话，到楼下后，在拿外卖，以及跟配送员说其他的事情（下半部）。这样，第一位配送员就不会占用我手机太多时间，当第二位配送员正好过来时，会有很大几率拨通我的电话。

​	再举一个计算机中的例子，常见的网卡接收网络包的例子。网卡收到网络包后，会通过**硬件中断**通知内核有新的数据到了，于是内核就会调用对应的中断处理程序来响应该事件，这个事件的处理也是会分成上半部和下半部。上部分要做到快速处理，所以只要把网卡的数据读到内存中，然后更新一下硬件寄存器的状态，比如把状态更新为表示数据已经读到内存中的状态值。接着，内核会触发一个**软中断**，把一些处理比较耗时且复杂的事情，交给「软中断处理程序」去做，也就是中断的下半部，其主要是需要从内存中找到网络数据，再按照网络协议栈，对网络数据进行逐层解析和处理，最后把数据送给应用程序。

所以，中断处理程序的上部分和下半部可以理解为：

- **上半部直接处理硬件请求，也就是硬中断**，主要是负责耗时短的工作，特点是快速执行；
- **下半部是由内核触发，也就说软中断**，主要是负责上半部未完成的工作，通常都是耗时比较长的事情，特点是延迟执行；

​	还有一个区别，硬中断（上半部）是会打断 CPU 正在执行的任务，然后立即执行中断处理程序，而软中断（下半部）是以内核线程的方式执行，并且每一个 CPU 都对应一个软中断内核线程，名字通常为「ksoftirqd/CPU 编号」，比如 0 号 CPU 对应的软中断内核线程的名字是 `ksoftirqd/0`

不过，软中断不只是包括硬件设备中断处理程序的下半部，一些内核自定义事件也属于软中断，比如内核调度等、RCU 锁（内核里常用的一种锁）等。

#### 1.6.3 系统里有哪些软中断？

​	在 Linux 系统里，我们可以通过查看 `/proc/softirqs` 的 内容来知晓「软中断」的运行情况，以及 `/proc/interrupts` 的 内容来知晓「硬中断」的运行情况。

​	接下来，就来简单的解析下  `/proc/softirqs` 文件的内容，在我服务器上查看到的文件内容如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161407007.webp)

​	你可以看到，每一个 CPU 都有自己对应的不同类型软中断的**累计运行次数**，有 3 点需要注意下。

1. 要注意第一列的内容，它是代表着软中断的类型，在我的系统里，软中断包括了 10 个类型，分别对应不同的工作类型，比如：
   -  `NET_RX` 表示网络接收中断
   - `NET_TX` 表示网络发送中断
   - `TIMER` 表示定时中断
   - `RCU` 表示 RCU 锁中断
   - `SCHED` 表示内核调度中断。
2. 要注意同一种类型的软中断在不同 CPU 的分布情况，正常情况下，同一种中断在不同 CPU 上的累计次数相差不多，比如我的系统里，`NET_RX` 在 CPU0 、CPU1、CPU2、CPU3 上的中断次数基本是同一个数量级，相差不多。
3. 这些数值是系统运行以来的累计中断次数，数值的大小没什么参考意义，但是系统的**中断次数的变化速率**才是我们要关注的，我们可以使用 `watch -d cat /proc/softirqs` 命令查看中断次数的变化速率。

​	前面提到过，软中断是以内核线程的方式执行的，我们可以用 `ps` 命令可以查看到，下面这个就是在我的服务器上查到软中断内核线程的结果：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161407885.webp)

​	可以发现，内核线程的名字外面都有有中括号，这说明 ps 无法获取它们的命令行参数，所以一般来说，名字在中括号里到，都可以认为是内核线程。而且，你可以看到有 4 个 `ksoftirqd` 内核线程，这是因为我这台服务器的 CPU 是 4 核心的，每个 CPU 核心都对应着一个内核线程。



#### 1.6.4 如何定位软中断CPU使用率过高的问题？

要想知道当前的系统的软中断情况，我们可以使用 `top` 命令查看，下面是一台服务器上的 top 的数据：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161410413.webp)

​	上图中的黄色部分 `si`，就是 CPU 在软中断上的使用率，而且可以发现，每个 CPU 使用率都不高，两个 CPU 的使用率虽然只有 3% 和 4% 左右，但是都是用在软中断上了。另外，也可以看到 CPU 使用率最高的进程也是软中断 `ksoftirqd`，因此可以认为此时系统的开销主要来源于软中断。如果要知道是哪种软中断类型导致的，我们可以使用 `watch -d cat /proc/softirqs` 命令查看每个软中断类型的中断次数的变化速率。

```shell
# 查看每个软中断类型的中断次数的变化速率
watch -d cat /proc/softirqs
```



![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161410535.webp)

​	一般对于网络 I/O 比较高的 Web 服务器，`NET_RX` 网络接收中断的变化速率相比其他中断类型快很多。如果发现 `NET_RX` 网络接收中断次数的变化速率过快，接下里就可以使用 `sar -n DEV` 查看网卡的网络包接收速率情况，然后分析是哪个网卡有大量的网络包进来。

```shell
# 查看网卡的网络包接受速率情况
sar -n DEV
```

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161425121.webp)

​	接着，在通过 `tcpdump` 抓包，分析这些包的来源，如果是非法的地址，可以考虑加防火墙，如果是正常流量，则要考虑硬件升级等。

#### 1.6.5总结

​	为了避免由于中断处理程序执行时间过长，而影响正常进程的调度，Linux 将中断处理程序分为上半部和下半部：

- 上半部，对应硬中断，由硬件触发中断，用来快速处理中断；
- 下半部，对应软中断，由内核触发中断，用来异步处理上半部未完成的工作；

​	Linux 中的软中断包括网络收发、定时、调度、RCU 锁等各种类型，可以通过查看 /proc/softirqs 来观察软中断的累计中断次数情况，如果要实时查看中断次数的变化率，可以使用 watch -d cat /proc/softirqs 命令。

​	每一个 CPU 都有各自的软中断内核线程，我们还可以用 ps 命令来查看内核线程，一般名字在中括号里面到，都认为是内核线程。如果在 top 命令发现，CPU 在软中断上的使用率比较高，而且 CPU 使用率最高的进程也是软中断 ksoftirqd 的时候，这种一般可以认为系统的开销被软中断占据了。

​	这时我们就可以分析是哪种软中断类型导致的，一般来说都是因为网络接收软中断导致的，如果是的话，可以用 sar 命令查看是哪个网卡的有大量的网络包接收，再用 tcpdump 抓网络包，做进一步分析该网络包的源头是不是非法地址，如果是就需要考虑防火墙增加规则，如果不是，则考虑硬件升级等。

### 1.7  **为什么** **0.1 + 0.2** **不等于** **0.3** ？

#### 1.7.1 为什么负数要用补码表示？

十进制数转二进制采用的是**除 2 取余法**，比如数字 8 转二进制的过程如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161429429.webp)

​	接着，我们看看「整数类型」的数字在计算机的存储方式，这其实很简单，也很直观，就是将十进制的数字转换成二进制即可。我们以 `int` 类型的数字作为例子，int 类型是 `32` 位的，其中**最高位是作为「符号标志位」**，正数的符号位是 `0`，负数的符号位是 `1`，**剩余的 31 位则表示二进制数据**。那么，对于 int 类型的数字 1 的二进制数表示如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161425643.webp)

​	而负数就比较特殊了点，负数在计算机中是以「补码」表示的，**所谓的补码就是把正数的二进制全部取反再加 1**，比如 -1 的二进制是把数字 1 的二进制取反后再加 1，如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161429483.webp)

​	不知道你有没有想过，为什么计算机要用补码的方式来表示负数？在回答这个问题前，我们假设不用补码的方式来表示负数，而只是把最高位的符号标志位变为 1 表示负数，如下图过程：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161425644.webp)

如果采用这种方式来表示负数的二进制的话，试想一下 `-2 + 1` 的运算过程，如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161429665.webp)

​	按道理，`-2 + 1 = -1`，但是上面的运算过程中得到结果却是 `-3`，所可以发现，这种负数的表示方式是不能用常规的加法来计算了，就需要特殊处理，要先判断数字是否为负数，如果是负数就要把加法操作变成减法操作才可以得到正确对结果。到这里，我们就可以回答前面提到的「负数为什么要用补码方式来表示」的问题了。

​	如果负数不是使用补码的方式表示，则在做基本对加减法运算的时候，**还需要多一步操作来判断是否为负数，如果为负数，还得把加法反转成减法，或者把减法反转成加法**，这就非常不好了，毕竟加减法运算在计算机里是很常使用的，所以为了性能考虑，应该要尽量简化这个运算过程。**而用了补码的表示方式，对于负数的加减法操作，实际上是和正数加减法操作一样的**。你可以看到下图，用补码表示的负数在运算 `-2 + 1` 过程的时候，其结果是正确的：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161425674.webp)

#### 1.7.2 十进制小数与二进制的转换

​	好了，整数十进制转二进制我们知道了，接下来看看小数是怎么转二进制的，小数部分的转换不同于整数部分，它采用的是**乘 2 取整法**，将十进制中的小数部分乘以 2 作为二进制的一位，然后继续取小数部分乘以 2 作为下一位，直到不存在小数为止。

话不多说，我们就以 `8.625` 转二进制作为例子，直接上图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161430743.webp)

​	最后把「整数部分 + 小数部分」结合在一起后，其结果就是 `1000.101`。但是，并不是所有小数都可以用二进制表示，前面提到的 0.625 小数是一个特例，刚好通过乘 2 取整法的方式完整的转换成二进制。

如果我们用相同的方式，来把 `0.1` 转换成二进制，过程如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161429446.webp)

可以发现，`0.1` 的二进制表示是无限循环的。

​	**由于计算机的资源是有限的，所以是没办法用二进制精确的表示 0.1，只能用「近似值」来表示，就是在有限的精度情况下，最大化接近 0.1 的二进制数，于是就会造成精度缺失的情况**。

对于二进制小数转十进制时，需要注意一点，小数点后面的指数幂是**负数**。

比如，二进制 `0.1` 转成十进制就是 `2^(-1)`，也就是十进制 `0.5`，二进制 `0.01` 转成十进制就是 `2^-2`，也就是十进制 `0.25`，以此类推。

举个例子，二进制 `1010.101` 转十进制的过程，如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161429355.webp)

#### 1.7.3 计算机是怎么存小数的

​	`1000.101` 这种二进制小数是「定点数」形式，代表着小数点是定死的，不能移动，如果你移动了它的小数点，这个数就变了， 就不再是它原来的值了。然而，计算机并不是这样存储的小数的，计算机存储小数的采用的是**浮点数**，名字里的「浮点」表示小数点是可以浮动的。

​	比如 `1000.101` 这个二进制数，可以表示成 `1.000101 x 2^3`，类似于数学上的科学记数法。既然提到了科学计数法，我再帮大家复习一下。比如有个很大的十进制数 1230000，我们可以也可以表示成 `1.23 x 10^6`，这种方式就称为科学记数法。该方法在小数点左边只有一个数字，而且把这种整数部分没有前导 0 的数字称为**规格化**，比如 `1.0 x 10^(-9)` 是规格化的科学记数法，而 `0.1 x 10^(-9)` 和 `10.0 x 10^(-9)` 就不是了。因此，如果二进制要用到科学记数法，同时要规范化，那么不仅要保证基数为 2，还要保证小数点左侧只有 1 位，而且必须为 1。所以通常将 `1000.101` 这种二进制数，规格化表示成 `1.000101 x 2^3`，其中，最为关键的是 000101 和 3 这两个东西，它就可以包含了这个二进制小数的所有信息：

- `000101` 称为**尾数**，即小数点后面的数字；
- `3` 称为**指数**，指定了小数点在数据中的位置；

现在绝大多数计算机使用的浮点数，一般采用的是 IEEE 制定的国际标准，这种标准形式如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161439207.webp)

这三个重要部分的意义如下：

- *符号位*：表示数字是正数还是负数，为 0 表示正数，为 1 表示负数；
- *指数位*：指定了小数点在数据中的位置，指数可以是负数，也可以是正数，**指数位的长度越长则数值的表达范围就越大**；
- *尾数位*：小数点右侧的数字，也就是小数部分，比如二进制 1.0011 x 2^(-2)，尾数部分就是 0011，而且**尾数的长度决定了这个数的精度**，因此如果要表示精度更高的小数，则就要提高尾数位的长度；

​	用 `32` 位来表示的浮点数，则称为**单精度浮点数**，也就是我们编程语言中的 `float` 变量，而用 `64` 位来表示的浮点数，称为**双精度浮点数**，也就是 `double` 变量，它们的结构如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZdv0uU8qPicibKURPOLPdWRgVC4Tt6eT4wgxcic65icFnOpzkibdKIdxV8ZPBnKOhFIkB3NvrIZGlemCzw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可以看到：

- double 的尾数部分是 52 位，float 的尾数部分是 23 位，由于同时都带有一个固定隐含位（这个后面会说），所以 double 有 53 个二进制有效位，float 有 24 个二进制有效位，所以所以它们的精度在十进制中分别是 `log10(2^53)` 约等于 `15.95` 和 `log10(2^24)` 约等于 `7.22` 位，因此 double 的有效数字是 `15~16` 位，float 的有效数字是 `7~8` 位，这些是有效位是包含整数部分和小数部分；
- double 的指数部分是 11 位，而 float 的指数位是 8 位，意味着 double 相比 float 能表示更大的数值范围；

​	那二进制小数，是如何转换成二进制浮点数的呢？我们就以 `10.625` 作为例子，看看这个数字在 float 里是如何存储的。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161439302.webp)

首先，我们计算出 10.625 的二进制小数为 1010.101。然后**把小数点，移动到第一个有效数字后面**，即将 1010.101 右移 `3` 位成 `1.010101`，右移 3 位就代表 +3，左移 3 位就是 -3。**float 中的「指数位」就跟这里移动的位数有关系，把移动的位数再加上「偏移量」，float 的话偏移量是 127，相加后就是指数位的值了**，即指数位这 8 位存的是 `10000010`（十进制 130），因此你可以认为「指数位」相当于指明了小数点在数据中的位置。`1.010101` 这个数的**小数点右侧的数字就是 float 里的「尾数位」**，由于尾数位是 23 位，则后面要补充 0，所以最终尾数位存储的数字是 `01010100000000000000000`。在算指数的时候，你可能会有疑问为什么要加上偏移量呢？

​	前面也提到，指数可能是正数，也可能是负数，即指数是有符号的整数，而有符号整数的计算是比无符号整数麻烦的，所以为了减少不必要的麻烦，在实际存储指数的时候，需要把指数转换成**无符号整数**。float 的指数部分是 8 位，IEEE 标准规定单精度浮点的指数取值范围是 `-126 ~ +127`，于是为了把指数转换成无符号整数，就要加个**偏移量**，比如 float 的指数偏移量是 `127`，这样指数就不会出现负数了。比如，指数如果是 8，则实际存储的指数是 8 + 127（偏移量）= 135，即把 135 转换为二进制之后再存储，而当我们需要计算实际的十进制数的时候，再把指数减去「偏移量」即可。

​	细心的朋友肯定发现，移动后的小数点左侧的有效位（即 1）消失了，它并没有存储到 float 里。这是因为 IEEE 标准规定，二进制浮点数的小数点左侧只能有 1 位，并且还只能是 1，**既然这一位永远都是 1，那就可以不用存起来了**。于是就让 23 位尾数只存储小数部分，然后在计算时会**自动把这个 1 加上，这样就可以节约 1 位的空间，尾数就能多存一位小数，相应的精度就更高了一点**。

那么，对于我们在从 float 的二进制浮点数转换成十进制时，要考虑到这个隐含的 1，转换公式如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161445283.webp)

举个例子，我们把下图这个 float 的数据转换成十进制，过程如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161445587.webp)

#### 1.7.4 0.1 + 0.2 == 0.3 ?

​	前面提到过，并不是所有小数都可以用「完整」的二进制来表示的，比如十进制 0.1 在转换成二进制小数的时候，是一串无限循环的二进制数，计算机是无法表达无限循环的二进制数的，毕竟计算机的资源是有限。因此，计算机只能用「近似值」来表示该二进制，那么意味着计算机存放的小数可能不是一个真实值。现在基本都是用  IEEE 754 规范的「单精度浮点类型」或「双精度浮点类型」来存储小数的，根据精度的不同，近似值也会不同。

那计算机是存储 0.1 是一个怎么样的二进制浮点数呢？

偷个懒，我就不自己手动算了，可以使用 binaryconvert 这个工具，将十进制 0.1 小数转换成 float 浮点数：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161445828.webp)

​	可以看到，8 位指数部分是 `01111011`，23 位的尾数部分是 `10011001100110011001101`，可以看到尾数部分是 `0011` 是一直循环的，只不过尾数是有长度限制的，所以只会显示一部分，所以是一个近似值，精度十分有限。

接下来，我们看看 0.2 的 float 浮点数：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161445774.webp)

可以看到，8 位指数部分是 `01111100`，稍微和 0.1 的指数不同，23 位的尾数部分是 `10011001100110011001101` 和 0.1 的尾数部分是相同的，也是一个近似值。

0.1 的二进制浮点数转换成十进制的结果是 `0.100000001490116119384765625`：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161449852.webp)

0.2 的二进制浮点数转换成十进制的结果是 `0.20000000298023223876953125`：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161449225.webp)

这两个结果相加就是 `0.300000004470348358154296875`：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161449679.webp)

​	所以，你会看到**在计算机中 0.1 + 0.2 并不等于完整的 0.3**。这主要是**因为有的小数无法可以用「完整」的二进制来表示，所以计算机里只能采用近似数的方式来保存，那两个近似数相加，得到的必然也是一个近似数**。我们在 JavaScript 里执行 0.1 + 0.2，你会得到下面这个结果：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161445765.webp)

​	结果和我们前面推到的类似，因为 JavaScript 对于数字都是使用 IEEE 754 标准下的双精度浮点类型来存储的。而我们二进制只能精准表达 2 除尽的数字 1/2, 1/4, 1/8，但是对于 0.1(1/10) 和 0.2(1/5)，在二进制中都无法精准表示时，需要根据精度舍入。我们人类熟悉的十进制运算系统，可以精准表达 2 和 5 除尽的数字，例如 1/2, 1/4, 1/5(0.2), 1/8, 1/10(0.1)。当然，十进制也有无法除尽的地方，例如 1/3, 1/7，也需要根据精度舍入。



#### 1.7.5 总结

> 为什么负数要用补码表示？

​	负数之所以用补码的方式来表示，主要是为了统一和正数的加减法操作一样，毕竟数字的加减法是很常用的一个操作，就不要搞特殊化，尽量以统一的方式来运算。

> 十进制小数怎么转成二进制？

十进制整数转二进制使用的是「除 2 取余法」，十进制小数使用的是「乘 2 取整法」。

> 计算机是怎么存小数的？

计算机是以浮点数的形式存储小数的，大多数计算机都是 IEEE 754 标准定义的浮点数格式，包含三个部分：

- 符号位：表示数字是正数还是负数，为 0 表示正数，为 1 表示负数；
- 指数位：指定了小数点在数据中的位置，指数可以是负数，也可以是正数，指数位的长度越长则数值的表达范围就越大；
- 尾数位：小数点右侧的数字，也就是小数部分，比如二进制 1.0011 x 2^(-2)，尾数部分就是 0011，而且尾数的长度决定了这个数的精度，因此如果要表示精度更高的小数，则就要提高尾数位的长度；

用 32 位来表示的浮点数，则称为单精度浮点数，也就是我们编程语言中的 float 变量，而用 64 位来表示的浮点数，称为双精度浮点数，也就是 double 变量。

> 0.1 + 0.2 == 0.3 吗？

​	不是的，0.1 和 0.2 这两个数字用二进制表达会是一个一直循环的二进制数，比如 0.1 的二进制表示为 0.0 0011 0011 0011… （0011 无限循环)，对于计算机而言，0.1 无法精确表达，这是浮点数计算造成精度损失的根源。因此，IEEE 754 标准定义的浮点数只能根据精度舍入，然后用「近似值」来表示该二进制，那么意味着计算机存放的小数可能不是一个真实值。0.1 + 0.2 并不等于完整的 0.3，这主要是因为这两个小数无法用「完整」的二进制来表示，只能根据精度舍入，所以计算机里只能采用近似数的方式来保存，那两个近似数相加，得到的必然也是一个近似数。





## 二、操作系统结构

### 2.1 内核

#### 2.1.1 什么是内核

​	什么是内核呢？计算机是由各种外部硬件设备组成的，比如内存、cpu、硬盘等，如果每个应用都要和这些硬件设备对接通信协议，那这样太累了。所以，这个中间人就由内核来负责，**让内核作为应用连接硬件设备的桥梁**，应用程序只需关心与内核交互，不用关心硬件的细节。

![内核](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161451429.webp)

#### 2.1.2 内核有哪些能力呢？

现代操作系统，内核一般会提供 4 个基本能力：

- **管理进程、线程**，决定哪个进程、线程使用 CPU，也就是进程调度的能力；
- **管理内存**，决定内存的分配和回收，也就是内存管理的能力；
- **管理硬件设备**，为进程与硬件设备之间提供通信能力，也就是硬件通信能力；
- **提供系统调用**，如果应用程序要运行更高权限运行的服务，那么就需要有系统调用，它是用户程序与操作系统之间的接口。

#### 2.1.3 内核是怎么工作的？

​	内核具有很高的权限，可以控制 cpu、内存、硬盘等硬件，而应用程序具有的权限很小，因此大多数操作系统，把内存分成了两个区域：

- **内核空间**，这个内存空间只有内核程序可以访问；
- **用户空间**，这个内存空间专门给应用程序使用；

​	用户空间的代码只能访问一个局部的内存空间，而内核空间的代码可以访问所有内存空间。因此，当程序使用用户空间时，我们常说该程序在**用户态**执行，而当程序使内核空间时，程序则在**内核态**执行。应用程序如果需要进入内核空间，就需要通过「系统调用」，下面来看看系统调用的过程：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161454041.webp)

​	内核程序执行在内核态，用户程序执行在用户态。当应用程序使用系统调用时，会产生一个中断。发生中断后， CPU 会中断当前在执行的用户程序，转而跳转到中断处理程序，也就是开始执行内核程序。内核处理完后，主动触发中断，把 CPU 执行权限交回给用户程序，回到用户态继续工作。



### 2.2 Linux设计

​	Linux 的开山始祖是来自一位名叫 Linus Torvalds 的芬兰小伙子，他在 1991 年用 C 语言写出了第一版的 Linux 操作系统，那年他 22 岁。完成第一版 Linux 后，Linux Torvalds 就在网络上发布了 Linux 内核的源代码，每个人都可以免费下载和使用。

Linux 内核设计的理念主要有这几个点：

- ***MutiTask***，多任务
- ***SMP***，对称多处理
- ***ELF***，可执行文件链接格式
- ***Monolithic Kernel***，宏内核

#### 2.2.1 MutiTask

​	MutiTask 的意思是**多任务**，代表着 Linux 是一个多任务的操作系统。多任务意味着可以有多个任务同时执行，这里的「同时」可以是并发或并行：

- 对于单核 CPU 时，可以让每个任务执行一小段时间，时间到就切换另外一个任务，从宏观角度看，一段时间内执行了多个任务，这被称为**并发**。
- 对于多核 CPU 时，多个任务可以同时被不同核心的 CPU 同时执行，这被称为**并行**。

#### 2.2.2 SMP

​	SMP 的意思是**对称多处理**，代表着每个 CPU 的地位是相等的，对资源的使用权限也是相同的，多个 CPU 共享同一个内存，每个 CPU 都可以访问完整的内存和硬件资源。这个特点决定了 Linux 操作系统不会有某个 CPU 单独服务应用程序或内核程序，而是每个程序都可以被分配到任意一个 CPU 上被执行。

#### 2.2.3 ELF

​	ELF 的意思是**可执行文件链接格式**，它是 Linux 操作系统中可执行文件的存储格式，你可以从下图看到它的结构：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161459164.webp)

​	ELF 把文件分成了一个个分段，每一个段都有自己的作用，具体每个段的作用这里我就不详细说明了，感兴趣的同学可以去看《程序员的自我修养——链接、装载和库》这本书。

​	ELF 文件有两种索引：

- Program header table 中记录了「运行时」所需的段
-  Section header table 记录了二进制文件中各个「段的首地址」。

​	那 ELF 文件怎么生成的呢？我们编写的代码，首先通过「编译器」编译成汇编代码，接着通过「汇编器」变成目标代码，也就是目标文件，最后通过「链接器」把多个目标文件以及调用的各种函数库链接起来，形成一个可执行文件，也就是 ELF 文件。

​	那 ELF 文件是怎么被执行的呢？执行 ELF 文件的时候，会通过「装载器」把 ELF 文件装载到内存里，CPU 读取内存中的指令和数据，于是程序就被执行起来了。

#### 2.2.4 Monolithic Kernel

​	Monolithic Kernel 的意思是**宏内核**，Linux 内核架构就是宏内核，意味着 Linux 的内核是一个完整的可执行程序，且拥有最高的权限。宏内核的特征是系统内核的所有模块，比如进程调度、内存管理、文件系统、设备驱动等，都运行在内核态。不过，Linux 也实现了动态加载内核模块的功能，例如大部分设备驱动是以可加载模块的形式存在的，与内核其他模块解藕，让驱动开发和驱动加载更为方便、灵活。

![分别为宏内核、微内核、混合内核的操作系统结构](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161501677.webp)

​	与宏内核相反的是**微内核**，微内核架构的内核只保留最基本的能力，比如进程调度、虚拟机内存、中断等，把一些应用放到了用户空间，比如驱动程序、文件系统等。这样服务与服务之间是隔离的，单个服务出现故障或者完全攻击，也不会导致整个操作系统挂掉，提高了操作系统的稳定性和可靠性。 

​	微内核内核功能少，可移植性高，相比宏内核有一点不好的地方在于，由于驱动程序不在内核中，而且驱动程序一般会频繁调用底层能力的，于是驱动和硬件设备交互就需要频繁切换到内核态，这样会带来性能损耗。华为的鸿蒙操作系统的内核架构就是微内核。

​	还有一种内核叫**混合类型内核**，它的架构有点像微内核，内核里面会有一个最小版本的内核，然后其他模块会在这个基础上搭建，然后实现的时候会跟宏内核类似，也就是把整个内核做成一个完整的程序，大部分服务都在内核中，这就像是宏内核的方式包裹着一个微内核。



### 2.3 Windows设计

​	当今 Windows 7、Windows 10 使用的内核叫 Windows NT，NT 全称叫 New Technology。

![Windows NT结构图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161502534.webp)

​	Windows 和 Linux 一样，同样支持 MutiTask 和 SMP，但不同的是，**Windows 的内核设计是混合型内核**，在上图你可以看到内核中有一个 *MicroKernel* 模块，这个就是最小版本的内核，而整个内核实现是一个完整的程序，含有非常多模块。Windows 的可执行文件的格式与 Linux 也不同，所以这两个系统的可执行文件是不可以在对方上运行的。Windows 的可执行文件格式叫 PE，称为**可移植执行文件**，扩展名通常是`.exe`、`.dll`、`.sys`等。

PE 的结构你可以从下图中看到，它与 ELF 结构有一点相似。

![PE文件结构](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161503218.webp)

### 2.4 总结

对于内核的架构一般有这三种类型：

- **宏内核**，包含多个模块，整个内核像一个完整的程序；
- **微内核**，有一个最小版本的内核，一些模块和服务则由用户态管理；
- **混合内核**，是宏内核和微内核的结合体，内核中抽象出了微内核的概念，也就是内核中会有一个小型的内核，其他模块就在这个基础上搭建，整个内核是个完整的程序；

​	Linux 的内核设计是采用了宏内核，Windows 的内核设计则是采用了混合内核。这两个操作系统的可执行文件格式也不一样， Linux 可执行文件格式叫作 ELF，Windows 可执行文件格式叫作 PE。



## 三、内存管理

### 虚拟内存

​	**单片机的CPU是直接操作内存的物理地址**，如果有多个程序使用同一个物理地址，是无法同时运行多个程序的，因为相同位置的内容会被擦除。

![image-20211207190517552](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071905602.png)

> 操作系统是如何解决这个问题呢？

​	这里问题的关键是程序都引用了绝对物理地址，这是我们需要避免的。

​	我们可以把进程所使⽤的地址「隔离」开来，即让操作系统为每个进程分配独⽴的⼀套「**虚拟地址**」，⼈⼈都有，⼤家⾃⼰玩⾃⼰的地址就⾏，互不⼲涉。但是有个前提每个进程都不能访问物理地址，⾄于虚拟地址最终怎么落到物理内存⾥，对进程来说是透明的，操作系统已经把这些都安排的明明⽩⽩了。

![image-20211207190740455](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071907504.png)

​	**操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。**

​	如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运⾏的时候，写⼊的是不同的物理地址，这样就不会冲突了。

于是，这里就引出了两种地址的概念：

- **虚拟内存地址**（*Virtual Memory Address*）： 程序所使用的内存地址
- **物理内存地址**（*Physical Memory Address*）： 存在硬件⾥⾯的空间地址

​	操作系统引⼊了虚拟内存，进程持有的虚拟地址会通过 CPU 芯⽚中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存，如下图所示：

![image-20211207191117307](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071911358.png)

> 操作系统是如何管理虚拟地址和物理地址之间的关系？

​	主要有两种⽅式，分别是**内存分段和内存分页**。



### 内存分段

​	程序是由若⼲个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。**不同的段是有不同的属性的，所以就用分段（Segmentation）的形式把这些段分离出来**。



> 分段机制下，虚拟地址和物理地址是如何映射的？

​	分段机制下的虚拟地址由两部分组成，**段选择因子**和**段内偏移量**。

![image-20211207193423814](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071934876.png)

- **段选择因子**： 保存在段寄存器⾥⾯。段选择⼦⾥⾯最重要的是**段号**，⽤作段表的索引。**段表**⾥⾯保存的是这个**段的基地址、段的界限和特权等级**等
- **段内偏移量**：大小位于0与段界限之间。`段基地址 + 段内偏移量 = 物理内存地址`。

​	由于虚拟地址是通过**段表**与物理地址进⾏映射的，分段机制会把程序的虚拟地址分成 4 个段，每个段在段表中有⼀个项，在这⼀项找到段的基地址，再加上偏移量，于是就能找到物理内存中的地址，如下图：

![image-20211207194149422](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071941484.png)

​	如果要访问段 3 中偏移量 500 的虚拟地址，那么`物理地址 = 段3基地址 7000 + 偏移量 500 `

​	分段的办法很好，解决了程序本身不需要关⼼具体的物理内存地址的问题，但它也有⼀些不⾜之处：

1. 存在**内存碎片**问题
2. **内存交换的效率低**

接下来，说说为什么会有这两个问题。

> 分段为什么会产生内存碎片问题？

​	我们来看看这样⼀个例⼦。假设有 1G 的物理内存，⽤户执⾏了多个程序，其中：

- 游戏占⽤了 512MB 内存
- 浏览器占⽤了 128MB 内存
- ⾳乐占⽤了 256 MB 内存。

​	这个时候，如果我们关闭了浏览器，则空闲内存还有 1024 - 512 - 256 = 256MB。

​	如果这个 256MB 不是连续的，被分成了两段 128 MB 内存，这就会导致没有空间再打开⼀个 200MB 的程序。

![image-20211207194739589](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071947659.png)

这⾥的内存碎⽚的问题共有两处地⽅：

- **外部内存碎片**： 产⽣了多个不连续的⼩物理内存，导致新的程序⽆法被装载；
- **内部内存碎片**： 程序所有的内存都被装载到了物理内存，但是这个程序有部分的内存可能并不是很常使⽤，这也会导致内存的浪费；

针对上⾯两种内存碎⽚的问题，解决的⽅式会有所不同。

解决外部内存碎⽚的问题就是**内存交换**。

​	可以把⾳乐程序占⽤的那 256MB 内存写到硬盘上，然后再从硬盘上读回来到内存⾥。不过再读回的时候，我们不能装载回原来的位置，⽽是紧紧跟着那已经被占⽤了的 512MB 内存后⾯。这样就能空缺出连续的 256MB 空间，于是新的 200MB 程序就可以装载进来。

​	这个内存交换空间，在 Linux 系统⾥，也就是我们常看到的 Swap 空间，这块空间是从硬盘划分出来的，⽤于内存与硬盘的空间交换。

> 再来看看，分段为什么会导致内存交换效率低？

​	对于多进程的系统来说，⽤分段的⽅式，内存碎⽚是很容易产⽣的，产⽣了内存碎⽚，那不得不重新Swap 内存区域，这个过程会产⽣性能瓶颈。

​	因为硬盘的访问速度要⽐内存慢太多了，每⼀次内存交换，我们都需要把⼀⼤段连续的内存数据写到硬盘上。

​	所以，**如果内存交换的时候，交换的是⼀个占内存空间很⼤的程序，这样整个机器都会显得卡顿。**

​	为了解决内存分段的内存碎⽚和内存交换效率低的问题，就出现了内存分⻚。

### 内存分页

​	分段的好处就是能产⽣连续的内存空间，但是会出现内存碎⽚和内存交换的空间太⼤的问题。

​	要解决这些问题，那么就要想出能少出现⼀些内存碎⽚的办法。另外，当需要进⾏内存交换的时候，让需要交换写⼊或者从磁盘装载的数据更少⼀点，这样就可以解决问题了。这个办法，也就是**内存分页**（*Paging*）。

> 什么是内存分页

​	**分页是把整个虚拟和物理内存空间切成⼀段段固定尺⼨的⼤⼩**。这样⼀个连续并且尺⼨固定的内存空间，我们叫**⻚**（*Page*）。在 Linux 下，每⼀⻚的⼤⼩为 4KB 。

> 内存分页是怎么联系虚拟地址和物理地址的？

虚拟地址与物理地址之间通过**页表**来映射，如下图：

![image-20211207200544350](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072005417.png)

​	⻚表是存储在内存⾥的，**内存管理单元** （*MMU*）就做将虚拟内存地址转换成物理地址的⼯作。

​	⽽当进程访问的虚拟地址在⻚表中查不到时，系统会产⽣⼀个**缺页异常**，进⼊系统内核空间分配物理内存、更新进程⻚表，最后再返回⽤户空间，恢复进程的运⾏。

> 分页是怎么解决分段的内存碎片、内存交换效率低的问题？

​	由于内存空间都是预先划分好的，也就不会像分段会产⽣间隙⾮常⼩的内存，这正是分段会产⽣内存碎⽚的原因。⽽**采⽤了分页，那么释放的内存都是以页为单位释放的，也就不会产生无法给进程使用的小内存。**

​	如果内存空间不够，操作系统会把其他正在运⾏的进程中的「最近没被使⽤」的内存⻚⾯给释放掉，也就是暂时写在硬盘上，称为**换出**（*Swap Out*）。⼀旦需要的时候，再加载进来，称为**换⼊**（*Swap In*）。所以，⼀次性写⼊磁盘的也只有少数的⼀个⻚或者⼏个⻚，不会花太多时间，**内存交换的效率就相对⽐较高**

![image-20211207202650936](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072026999.png)

​	更进⼀步地，分⻚的⽅式使得我们在加载程序的时候，不再需要⼀次性都把程序加载到物理内存中。我们完全可以在进⾏虚拟内存和物理内存的⻚之间的映射之后，并不真的把⻚加载到物理内存⾥，⽽是**只有在程序运⾏中，需要⽤到对应虚拟内存页里面的指令和数据时，再加载到物理内存⾥⾯去。**

> 分⻚机制下，虚拟地址和物理地址是如何映射的？

​	在分⻚机制下，虚拟地址分为两部分，**⻚号**和**⻚内偏移**。⻚号作为⻚表的索引，**⻚表**包含物理⻚每⻚所在**物理内存的基地址**，这个基地址与⻚内偏移的组合就形成了物理内存地址，⻅下图： 

![image-20211207202905602](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072029664.png)

总结⼀下，对于⼀个内存地址转换，其实就是这样三个步骤：

1. 把虚拟内存地址，切分成⻚号和偏移量；
2. 根据⻚号，从⻚表⾥⾯，查询对应的物理⻚号；
3. 直接拿物理⻚号，加上前⾯的偏移量，就得到了物理内存地址。

下⾯举个例⼦，虚拟内存中的⻚通过⻚表映射为了物理内存中的⻚，如下图：

![image-20211207203153944](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072031008.png)

这看起来似乎没什么⽑病，但是放到实际中操作系统，这种简单的分⻚是肯定是会有问题的。

> 简单的分⻚有什么缺陷吗？

​	有空间上的缺陷。

​	因为操作系统是可以同时运⾏⾮常多的进程的，那这不就意味着⻚表会⾮常的庞⼤。

​	在 32 位的环境下，虚拟地址空间共有 4GB，假设⼀个⻚的⼤⼩是 4KB（2^12），那么就需要⼤约 100 万 （2^20） 个⻚，每个「⻚表项」需要 4 个字节⼤⼩来存储，那么整个 4GB 空间的映射就需要有 4MB的内存来存储⻚表。

​	这 4MB ⼤⼩的⻚表，看起来也不是很⼤。但是要知道每个进程都是有⾃⼰的虚拟地址空间的，也就说都有⾃⼰的⻚表。

​	那么， 100 个进程的话，就需要 400MB 的内存来存储⻚表，这是⾮常⼤的内存了，更别说 64 位的环境了。

##### 多级页表

​	要解决上⾯的问题，就需要采⽤⼀种叫作**多级页表**（*Multi-Level Page Table*）的解决⽅案。

​	在前⾯我们知道了，对于单⻚表的实现⽅式，在 32 位和⻚⼤⼩ 4KB 的环境下，⼀个进程的⻚表需要装下 100 多万个「⻚表项」，并且每个⻚表项是占⽤ 4 字节⼤⼩的，于是相当于每个⻚表需占⽤ 4MB ⼤⼩的空间。

​	我们把这个 100 多万个「⻚表项」的单级⻚表再分⻚，将⻚表（⼀级⻚表）分为 1024 个⻚表（⼆级⻚表），每个表（⼆级⻚表）中包含 1024 个「⻚表项」，形成**⼆级分⻚**。如下图所示：

![image-20211207203623567](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072036629.png)

> 你可能会问，分了⼆级表，映射 4GB 地址空间就需要 4KB（⼀级⻚表）+ 4MB（⼆级⻚表）的内存，这样占⽤空间不是更⼤了吗？

​	当然如果 4GB 的虚拟地址全部都映射到了物理内存上的话，⼆级分⻚占⽤空间确实是更⼤了，但是，我们往往不会为⼀个进程分配那么多内存。

​	其实我们应该换个⻆度来看问题，还记得计算机组成原理⾥⾯⽆处不在的**局部性原理**么？

​	每个进程都有 4GB 的虚拟地址空间，⽽显然对于⼤多数程序来说，其使⽤到的空间远未达到 4GB，因为会存在部分对应的⻚表项都是空的，根本没有分配，对于已分配的⻚表项，如果存在最近⼀定时间未访问的⻚表，在物理内存紧张的情况下，操作系统会将⻚⾯换出到硬盘，也就是说不会占⽤物理内存。

​	如果使⽤了⼆级分⻚，⼀级⻚表就可以覆盖整个 4GB 虚拟地址空间，但**如果某个⼀级⻚表的⻚表项没有被⽤到，也就不需要创建这个⻚表项对应的⼆级⻚表了，即可以在需要时才创建⼆级⻚表**。做个简单的计算，假设只有 20% 的⼀级⻚表项被⽤到了，那么⻚表占⽤的内存空间就只有 4KB（⼀级⻚表） + 20% *4MB（⼆级⻚表）= 0.804MB ，这对⽐单级⻚表的 4MB 是不是⼀个巨⼤的节约？

​	那么为什么不分级的⻚表就做不到这样节约内存呢？我们从⻚表的性质来看，保存在内存中的⻚表承担的职责是将虚拟地址翻译成物理地址。假如虚拟地址在⻚表中找不到对应的⻚表项，计算机系统就不能⼯作了。所以**⻚表⼀定要覆盖全部虚拟地址空间，不分级的⻚表就需要有 100多万个⻚表项来映射，⽽⼆级分⻚则只需要** **1024** **个⻚表项**（此时⼀级⻚表覆盖到了全部虚拟地址空间，⼆级⻚表在需要时创建）。

​	我们把⼆级分⻚再推⼴到多级⻚表，就会发现⻚表占⽤的内存空间更少了，这⼀切都要归功于对局部性原理的充分应⽤。

对于 64 位的系统，两级分⻚肯定不够了，就变成了四级⽬录，分别是：

- **全局页目录项PGD**(Page Global Directory)
- **上层⻚⽬录项 PUD**（*Page Upper Directory*）；
- **中间⻚⽬录项 PMD**（*Page Middle Directory*）；
- **⻚表项 PTE**（*Page Table Entry*）；

![image-20211207205736792](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072057862.png)

##### TLB

​	多级⻚表虽然解决了空间上的问题，但是虚拟地址到物理地址的转换就多了⼏道转换的⼯序，这显然就降低了这俩地址转换的速度，也就是带来了时间上的开销。

​	程序是有局部性的，即在⼀段时间内，整个程序的执⾏仅限于程序中的某⼀部分。相应地，执⾏所访问的存储空间也局限于某个内存区域。

![image-20211207205955327](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072059387.png)

​	我们就可以利⽤这⼀特性，把最常访问的⼏个⻚表项存储到访问速度更快的硬件，于是计算机科学家们， 就在 CPU 芯⽚中，加⼊了⼀个专⻔存放程序最常访问的⻚表项的  Cache，这个  Cache 就是 TLB（*Translation  Lookaside  Buffer*） ，通常称为⻚表缓存、转址旁路缓存、快表等。

![image-20211207210112285](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072101340.png)

​	在  CPU 芯⽚⾥⾯，封装了内存管理单元（*Memory Management Unit*）芯⽚，它⽤来完成地址转换和  TLB的访问与交互。

​	有了 TLB 后，那么 CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的⻚表。 TLB  的命中率其实是很⾼的，因为程序最常访问的⻚就那么⼏个。

### 段页式内存管理

​	内存分段和内存分⻚并不是对⽴的，它们是可以组合起来在同⼀个系统中使⽤的，那么组合起来后，通常 称为**段⻚式内存管理**。

![image-20211207210237933](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072102002.png)

段⻚式内存管理实现的⽅式：

1. 先将程序划分为多个有逻辑意义的段，也就是前⾯提到的分段机制；
2. 接着再把每个段划分为多个⻚，也就是对分段划分出来的连续空间，再划分固定⼤⼩的⻚； 这样，地址结构就由**段号、段内页号和页内位移**三部分组成。

​	⽤于段⻚式地址变换的数据结构是每⼀个程序⼀张段表，每个段⼜建⽴⼀张⻚表，段表中的地址是⻚表的 起始地址，⽽⻚表中的地址则为某⻚的物理⻚号，如图所示：

![image-20211207210459681](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072104757.png)

段⻚式地址变换中要得到物理地址须经过三次内存访问：

1. 访问段表，得到⻚表起始地址；
2. 访问⻚表，得到物理⻚号
3. 将物理⻚号与⻚内位移组合，得到物理地址

可⽤软、硬件相结合的⽅法实现段⻚式地址变换，这样虽然增加了硬件成本和系统开销，但提⾼了内存的 利⽤率。



### Linux内存管理 

那么，Linux  操作系统采⽤了哪种⽅式来管理内存呢？

> 在回答这个问题前，我们得先看看 Intel 处理器的发展历史。

​	早期 Intel 的处理器从 80286 开始使⽤的是段式内存管理。但是很快发现，光有段式内存管理⽽没有⻚式 内存管理是不够的，这会使它的  X86  系列会失去市场的竞争⼒。因此，在不久以后的  80386 中就实现了对⻚式内存管理。也就是说，80386 除了完成并完善从 80286 开始的段式内存管理的同时还实现了⻚式内存 管理。

​	但是这个 80386 的⻚式内存管理设计时，没有绕开段式内存管理，⽽是建⽴在段式内存管理的基础上，这 就意味着，**⻚式内存管理的作⽤是在由段式内存管理所映射⽽成的地址上再加上⼀层地址映射。**

​	由于此时由段式内存管理映射⽽成的地址不再是“物理地址”了，Intel 就称之为“线性地址”（也称虚拟地 址）。于是，段式内存管理先将逻辑地址映射成线性地址，然后再由⻚式内存管理将线性地址映射成物理 地址。

![image-20211207213455124](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072134182.png)

这⾥说明下逻辑地址和线性地址：

- 程序所使⽤的地址，通常是没被段式内存管理映射的地址，称为逻辑地址；
- 通过段式内存管理映射的地址，称为线性地址，也叫虚拟地址； 逻辑地址是「段式内存管理」转换前的地址，线性地址则是「⻚式内存管理」转换前的地址。

> 了解完 Intel 处理器的发展历史后，我们再来说说 Linux 采⽤了什么⽅式管理内存？

​	**Linux内存主要采⽤的是⻚式内存管理，但同时也不可避免地涉及了段机制**。

​	这主要是上⾯ Intel 处理器发展历史导致的，因为 Intel X86 CPU ⼀律对程序中使⽤的地址先进⾏段式映 射，然后才能进⾏⻚式映射。既然  CPU 的硬件结构是这样，Linux 内核也只好服从  Intel  的选择。

​	但是事实上，Linux 内核所采取的办法是使段式映射的过程实际上不起什么作⽤。也就是说，“上有政策， 下有对策”，若惹不起就躲着⾛。

​	**Linux系统中的每个段都是从0地址开始的整个4GB虚拟空间（32位环境下），也就是所有的段的起始地址都一样的。这意味着，Linux系统的代码，包括操作系统本身的代码和应用程序代码，所面对的地址空间都是线性地址空间（虚拟地址），这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被用于访问控制和内存保护。**

> Linux   的虚拟地址空间是如何分布的？

​	在 Linux 操作系统中，虚拟地址空间的内部⼜被分为**内核空间和⽤户空间**两部分，不同位数的系统，地址 空间的范围也不同。⽐如最常⻅的  32 位和  64 位系统，如下所示：

![image-20211207214115875](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072141943.png)

通过这里可以看出： 

- 32位系统的内核空间占用1G，位于最高处，剩下的3G是用户空间
- 64位系统的内核空间和用户空间都是128T，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。

再来说说，内核空间与⽤户空间的区别：

- 进程在⽤户态时，只能访问⽤户空间内存；
- 只有进⼊内核态后，才可以访问内核空间的内存；

虽然每个进程都各⾃有独⽴的虚拟内存，但是**每个虚拟内存中的内核地址，其实关联的都是相同的物理内存**。这样，进程切换到内核态后，就可以很⽅便地访问内核空间内存。

![image-20211207214453133](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072144199.png)

接下来，进⼀步了解虚拟空间的划分情况，⽤户空间和内核空间划分的⽅式是不同的，内核空间的分布情 况就不多说了。

我们看看⽤户空间分布的情况，以  32 位系统为例，我画了⼀张图来表示它们的关系：

![image-20211207214518441](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072145524.png)

通过这张图你可以看到，⽤户空间内存，从**低到⾼分别是  7  种不同的内存段：

- 程序文件段，包括二进制可执行代码
- 已初始化数据段，包括静态常量；包括未初始化的静态变量； 堆段，包括动态分配的内存，从低地址开始向上增⻓。
- ⽂件映射段，包括动态库、共享内存等，从低地址开始向上增⻓（[跟硬件和内核版本有关](http://lishiwen4.github.io/linux/linux-process-memory-location)）；
- 栈段，包括局部变量和函数调⽤的上下⽂等。栈的⼤⼩是固定的，⼀般是 8MB。当然系统也提供了参数，以便我们⾃定义⼤⼩；

在这  7  个内存段中，堆和⽂件映射段的内存是动态分配的。⽐如说，使⽤  C 标准库的`malloc()`或者`mmap()`，就可以分别在堆和文件映射段动态分配内存。

### 总结

​	为了在多进程环境下，使得进程之间的内存地址不受影响，相互隔离，于是操作系统就为每个进程独⽴分配⼀套**虚拟地址空间**，每个程序只关⼼⾃⼰的虚拟地址就可以，实际上⼤家的虚拟地址都是⼀样的，但分布到物理地址内存是不⼀样的。作为程序，也不⽤关⼼物理地址的事情。

​	每个进程都有⾃⼰的虚拟空间，⽽物理内存只有⼀个，所以当启⽤了⼤量的进程，物理内存必然会很紧张，于是操作系统会通过**内存交换**技术，把不常使⽤的内存暂时存放到硬盘（换出），在需要的时候再装载回物理内存（换⼊）。

​	那既然有了虚拟地址空间，那必然要把虚拟地址「映射」到物理地址，这个事情通常由操作系统来维护。那么对于虚拟地址与物理地址的映射关系，可以有**分段**和**分⻚**的⽅式，同时两者结合都是可以的。

​	内存分段是根据程序的逻辑⻆度，分成了栈段、堆段、数据段、代码段等，这样可以分离出不同属性的段，同时是⼀块连续的空间。但是每个段的⼤⼩都不是统⼀的，这就会导致内存碎⽚和内存交换效率低的问题。

​	于是，就出现了内存分⻚，把虚拟空间和物理空间分成⼤⼩固定的⻚，如在 Linux 系统中，每⼀⻚的⼤⼩为 4KB 。由于分了⻚后，就不会产⽣细⼩的内存碎⽚。同时在内存交换的时候，写⼊硬盘也就⼀个⻚或⼏个⻚，这就⼤⼤提⾼了内存交换的效率。

​	再来，为了解决简单分⻚产⽣的⻚表过⼤的问题，就有了**多级⻚表**，它解决了空间上的问题，但这就会导致 CPU 在寻址的过程中，需要有很多层表参与，加⼤了时间上的开销。于是根据程序的**局部性原理**，在CPU 芯⽚中加⼊了 **TLB**，负责缓存最近常被访问的⻚表项，⼤⼤提⾼了地址的转换速度。

​	**Linux系统主要采⽤了分⻚管理，但是由于Intel处理器的发展史，Linux系统⽆法避免分段管理**。于是Linux 就把所有段的基地址设为 0 ，也就意味着所有程序的地址空间都是线性地址空间（虚拟地址），相

当于屏蔽了 CPU 逻辑地址的概念，所以段只被⽤于访问控制和内存保护。

另外，Linxu 系统中虚拟空间分布可分为**⽤户态**和**内核态**两部分，其中⽤户态的分布：代码段、全局变量、

BSS、函数栈、堆内存、映射区。



## 四、进程和线程

### 4.1进程、线程基础知识

>  先来看看一则小故事

​	我们写好的⼀⾏⾏代码，为了让其⼯作起来，我们还得把它送进城（**进程**）⾥，那既然进了城⾥，那肯定

不能胡作⾮为了。

​	城⾥⼈有城⾥⼈的规矩，城中有个专⻔管辖你们的城管（**操作系统**），⼈家让你休息就休息，让你⼯作就

⼯作，毕竟摊位不多，每个⼈都要占这个摊位来⼯作，城⾥要⼯作的⼈多着去了。

​	所以城管为了公平起⻅，它使⽤⼀种策略（**调度**）⽅式，给每个⼈⼀个固定的⼯作时间（**时间⽚**），时间

到了就会通知你去休息⽽换另外⼀个⼈上场⼯作。

​	另外，在休息时候你也不能偷懒，要记住⼯作到哪了，不然下次到你⼯作了，你忘记⼯作到哪了，那还怎

么继续？

​	有的⼈，可能还进⼊了县城（**线程**）⼯作，这⾥相对轻松⼀些，在休息的时候，要记住的东⻄相对较少，

⽽且还能共享城⾥的资源。

#### 4.1.1 进程

##### 4.1.1.1 进程的概念

​	我们编写的代码只是⼀个存储在硬盘的静态⽂件，通过编译后就会⽣成⼆进制可执⾏⽂件，当我们运⾏这

个可执⾏⽂件后，它会被装载到内存中，接着 CPU 会执⾏程序中的每⼀条指令，那么这个**运⾏中的程序**， 就被称为**进程（Process）**。

​	现在我们考虑有⼀个会读取硬盘⽂件数据的程序被执⾏了，那么当运⾏到读取⽂件的指令时，就会去从硬

盘读取数据，但是硬盘的读写速度是⾮常慢的，那么在这个时候，如果 CPU 傻傻的等硬盘返回数据的话，

那 CPU 的利⽤率是⾮常低的。

​	做个类⽐，你去煮开⽔时，你会傻傻的等⽔壶烧开吗？很明显，⼩孩也不会傻等。我们可以在⽔壶烧开之

前去做其他事情。当⽔壶烧开了，我们⾃然就会听到“嘀嘀嘀”的声⾳，于是再把烧开的⽔倒⼊到⽔杯⾥就

好了。

​	所以，当进程要从硬盘读取数据时，CPU 不需要阻塞等待数据的返回，⽽是去执⾏另外的进程。当硬盘数

据返回时，CPU 会收到个**中断**，于是 CPU 再继续运⾏这个进程。

![image-20211208090455641](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112080905771.png)

​	这种**多个程序、交替执⾏**的思想，就有  CPU  管理多个进程的初步想法。

​	对于⼀个⽀持多进程的系统，CPU 会从⼀个进程快速切换⾄另⼀个进程，其间每个进程各运⾏⼏⼗或⼏百 个毫秒。

​	虽然单核的 CPU 在某⼀个瞬间，只能运⾏⼀个进程。但在 1 秒钟期间，它可能会运⾏多个进程，这样就 产⽣**并⾏的错觉**，实际上这是**并发**。

> 并行和并发有什么区别？

![image-20211208090710753](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112080907824.png)

> 进程和程序的关系的类比

​	到了晚饭时间，⼀对⼩情侣肚⼦都咕咕叫了，于是男⽣⻅机⾏事，就想给⼥⽣做晚饭，所以他就在⽹上找

了辣⼦鸡的菜谱，接着买了⼀些鸡⾁、辣椒、⾹料等材料，然后边看边学边做这道菜。

![image-20211208090829569](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112080908628.png)

​	突然，⼥⽣说她想喝可乐，那么男⽣只好把做菜的事情暂停⼀下，并在⼿机菜谱标记做到哪⼀个步骤，把

状态信息记录了下来。

​	然后男⽣听从⼥⽣的指令，跑去下楼买了⼀瓶冰可乐后，⼜回到厨房继续做菜。

​	**这体现了，CPU可以从一个进程（做菜）切换到另外一个进程（买可乐），在切换前必须要记录当前进程中运行的状态信息，以备下次切换回来的时候可以恢复运行**

​	所以，可以发现进程有着「**运⾏** **-** **暂停** **-** **运⾏**」的活动规律。

##### 4.1.1.2 进程的状态

​	在上⾯，我们知道了进程有着「运⾏ - 暂停 - 运⾏」的活动规律。⼀般说来，⼀个进程并不是⾃始⾄终连

续不停地运⾏的，它与并发执⾏中的其他进程的执⾏是相互制约的。

​	它有时处于运⾏状态，有时⼜由于某种原因⽽暂停运⾏处于等待状态，当使它暂停的原因消失后，它⼜进

⼊准备运⾏状态。

所以，**在⼀个进程的活动期间⾄少具备三种基本状态，即运⾏状态、就绪状态、阻塞状态。**

![image-20211208091944866](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112080919953.png)

上图中各个状态的意义：

- 运⾏状态（*Runing*）：该时刻进程占⽤ CPU；

- 就绪状态（*Ready*）：可运⾏，由于其他进程处于运⾏状态⽽暂时停⽌运⾏；

- 阻塞状态（*Blocked*）：该进程正在等待某⼀事件发⽣（如等待输⼊/输出操作的完成）⽽暂时停⽌运

  ⾏，这时，即使给它CPU控制权，它也⽆法运⾏；

当然，进程还有另外两个基本状态：

- 创建状态（*new*）：进程正在被创建时的状态；
- 结束状态（*Exit*）：进程正在从系统中消失时的状态；

于是，⼀个完整的进程状态的变迁如下图：

![image-20211208092212035](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112080922111.png)

再来详细说明⼀下进程的状态变迁：

- **NULL -> 创建状态**：⼀个新进程被创建时的第⼀个状态；

- **创建状态 -> 就绪状态**：当进程被创建完成并初始化后，⼀切就绪准备运⾏时，变为就绪状态，这个

  过程是很快的；

- **就绪态 -> 运⾏状态**：处于就绪状态的进程被操作系统的进程调度器选中后，就分配给 CPU 正式运⾏

  该进程；

- **运⾏状态 ->结束状态**：当进程已经运⾏完成或出错时，会被操作系统作结束状态处理；

- **运⾏状态 -> 就绪状态**：处于运⾏状态的进程在运⾏过程中，由于分配给它的运⾏时间⽚⽤完，操作

  系统会把该进程变为就绪态，接着从就绪态选中另外⼀个进程运⾏；

- **运⾏状态 -> 阻塞状态**：当进程请求某个事件且必须等待时，例如请求 I/O 事件；

- **阻塞状态 -> 就绪状态**：当进程要等待的事件完成时，它从阻塞状态变到就绪状态；

​	如果有⼤量处于阻塞状态的进程，进程可能会占⽤着物理内存空间，显然不是我们所希望的，毕竟物理内

存空间是有限的，被阻塞状态的进程占⽤着物理内存就⼀种浪费物理内存的⾏为。

​	所以，在虚拟内存管理的操作系统中，通常会把**阻塞状态的进程的物理内存空间换出到硬盘**，等需要再次

运⾏的时候，再从硬盘换⼊到物理内存。

![image-20211208092927822](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112080929901.png)

​	那么，就需要⼀个新的状态，来**描述进程没有占⽤实际的物理内存空间的情况，这个状态就是挂起状态**。 这跟阻塞状态是不⼀样，阻塞状态是等待某个事件的返回。

​	另外，挂起状态可以分为两种：

- **阻塞挂起状态**： 进程在外存（硬盘）并等待某个事件的出现；
- **就绪挂起状态**： 进程在外存（硬盘），但只要进⼊内存，即刻⽴刻运⾏； 这两种挂起状态加上前⾯的五种状态，就变成了七种状态变迁，⻅如下图：

![image-20211208093122504](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112080931585.png)

导致进程挂起的原因不只是因为进程所使⽤的内存空间不在物理内存，还包括如下情况：

- 通过 sleep 让进程间歇性挂起，其⼯作原理是设置⼀个定时器，到期后唤醒进程。

- ⽤户希望挂起⼀个程序的执⾏，⽐如在 Linux 中⽤ Ctrl+Z 挂起进程；

##### 4.1.1.3 进程的控制结构

​	在操作系统中，是⽤**进程控制块**（*process control block*，*PCB*）数据结构来描述进程的。

​	那 PCB 是什么呢？**PCB** **是进程存在的唯⼀标识**，这意味着⼀个进程的存在，必然会有⼀个 PCB，如果进程消失了，那么PCB 也会随之消失。

> PCB 具体包含什么信息?

**进程描述信息**：

- 进程标识符：标识各个进程，每个进程都有⼀个并且唯⼀的标识符；
- ⽤户标识符：进程归属的⽤户，⽤户标识符主要为共享和保护服务；

**进程控制和管理信息**：

- 进程当前状态，如 new、ready、running、waiting 或 blocked 等；
- 进程优先级：进程抢占 CPU 时的优先级；

**资源分配清单**：

- 有关内存地址空间或虚拟地址空间的信息，所打开⽂件的列表和所使⽤的 I/O 设备信息。

**CPU相关信息**：

- CPU 中各个寄存器的值，当进程被切换时，CPU 的状态信息都会被保存在相应的 PCB 中，以便进程

  重新执⾏时，能从断点处继续执⾏。

> 每个PCB是如何组织的？

通常是通过**链表**的⽅式进⾏组织，把具有**相同状态的进程链在⼀起，组成各种队列**。⽐如：

- 将所有处于就绪状态的进程链在⼀起，称为**就绪队列**；

- 把所有因等待某事件⽽处于等待状态的进程链在⼀起就组成各种**阻塞队列**；

- 另外，对于运⾏队列在单核 CPU 系统中则只有⼀个运⾏指针了，因为单核 CPU 在某个时间，只能运

  ⾏⼀个程序。

那么，就绪队列和阻塞队列链表的组织形式如下图：

![image-20211208095928396](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112080959657.png)

​	除了链接的组织⽅式，还有索引⽅式，它的⼯作原理：将同⼀状态的进程组织在⼀个索引表中，索引表项

指向相应的 PCB，不同状态对应不同的索引表。

​	⼀般会选择链表，因为可能⾯临进程创建，销毁等调度导致进程状态发⽣变化，所以链表能够更加灵活的

插⼊和删除。

##### 4.1.1.4 进程的控制

​	我们熟知了进程的状态变迁和进程的数据结构 PCB 后，再来看看进程的**创建、终⽌、阻塞、唤醒**的过程，

这些过程也就是进程的控制。

###### 01 创建进程

​	操作系统允许⼀个进程创建另⼀个进程，⽽且允许⼦进程继承⽗进程所拥有的资源，当⼦进程被终⽌时，

其在⽗进程处继承的资源应当还给⽗进程。同时，终⽌⽗进程时同时也会终⽌其所有的⼦进程。

​	注意：Linux 操作系统对于终⽌有⼦进程的⽗进程，会把⼦进程交给 1 号进程接管。本⽂所指出的进程终

⽌概念是宏观操作系统的⼀种观点，最后怎么实现当然是看具体的操作系统。

创建进程的过程如下：

1. 为新进程分配⼀个唯⼀的进程标识号，并申请⼀个空⽩的 PCB，PCB 是有限的，若申请失败则创建

失败

2. 为进程分配资源，此处如果资源不⾜，进程就会进⼊等待状态，以等待资源；
3. 初始化 PCB；
4. 如果进程的调度队列能够接纳新进程，那就将进程插⼊到就绪队列，等待被调度运⾏；

###### 02 终止进程

进程可以有 3 种终⽌⽅式：正常结束、异常结束以及外界⼲预（信号 kill 掉）。

终⽌进程的过程如下：

1. 查找需要终⽌的进程的 PCB；
2. 如果处于执⾏状态，则⽴即终⽌该进程的执⾏，然后将 CPU 资源分配给其他进程；
3. 如果其还有⼦进程，则应将其所有⼦进程终⽌；
4. 将该进程所拥有的全部资源都归还给⽗进程或操作系统；
5. 将其从 PCB 所在队列中删除；

###### 03 阻塞进程

​	当进程需要等待某⼀事件完成时，它可以调⽤阻塞语句把⾃⼰阻塞等待。⽽⼀旦被阻塞等待，它只能由另

⼀个进程唤醒。

阻塞进程的过程如下：

1. 找到将要被阻塞进程标识号对应的 PCB；
2. 如果该进程为运⾏状态，则保护其现场，将其状态转为阻塞状态，停⽌运⾏；
3. 将该 PCB 插⼊到阻塞队列中去；

###### 04 唤醒进程

​	进程由「运⾏」转变为「阻塞」状态是由于进程必须等待某⼀事件的完成，所以处于阻塞状态的进程是绝

对不可能叫醒⾃⼰的。

​	如果某进程正在等待 I/O 事件，需由别的进程发消息给它，则只有当该进程所期待的事件出现时，才由发

现者进程⽤唤醒语句叫醒它。

唤醒进程的过程如下：

1. 在该事件的阻塞队列中找到相应进程的 PCB；
2. 将其从阻塞队列中移出，并置其状态为就绪状态；
3. 把该 PCB 插⼊到就绪队列中，等待调度程序调度；

​	进程的阻塞和唤醒是⼀对功能相反的语句，如果某个进程调⽤了阻塞语句，则必有⼀个与之对应的唤醒语

句。



##### 4.1.1.5 进程的上下文切换

​	各个进程之间是共享 CPU 资源的，在不同的时候进程之间需要切换，让不同的进程可以在 CPU 执⾏，那

么这个**⼀个进程切换到另⼀个进程运⾏，称为进程的上下⽂切换**。

> 在详细说进程上下⽂切换前，我们先来看看 CPU 上下⽂切换

​	⼤多数操作系统都是多任务，通常⽀持⼤于 CPU 数量的任务同时运⾏。实际上，这些任务并不是同时运⾏的，只是因为系统在很短的时间内，让各个任务分别在 CPU 运⾏，于是就造成同时运⾏的错觉。

​	任务是交给 CPU 运⾏的，那么在每个任务运⾏前，CPU 需要知道任务从哪⾥加载，⼜从哪⾥开始运⾏。

所以，操作系统需要事先帮 CPU 设置好 **CPU** **寄存器和程序计数器**。

​	CPU 寄存器是 CPU 内部⼀个容量⼩，但是速度极快的内存（缓存）。我举个例⼦，寄存器像是你的⼝

袋，内存像你的书包，硬盘则是你家⾥的柜⼦，如果你的东⻄存放到⼝袋，那肯定是⽐你从书包或家⾥柜

⼦取出来要快的多。

​	程序计数器则是⽤来存储 CPU 正在执⾏的指令位置、或者即将执⾏的下⼀条指令位置。

​	所以说，CPU 寄存器和程序计数是 CPU 在运⾏任何任务前，所必须依赖的环境，这些环境就叫做 **CPU**

**上下⽂**。

​	既然知道了什么是 CPU 上下⽂，那理解 CPU 上下⽂切换就不难了。CPU 上下⽂切换就是先把前⼀个任务的 CPU 上下⽂（CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下⽂到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运⾏新任务。

​	**系统内核**会存储保持下来的上下⽂信息，当此任务再次被分配给 CPU 运⾏时，CPU 会重新加载这些上下⽂，这样就能保证任务原来的状态不受影响，让任务看起来还是连续运⾏。上⾯说到所谓的「任务」，主要包含进程、线程和中断。所以，可以根据任务的不同，把 CPU 上下⽂切换分成：**进程上下⽂切换、线程上下⽂切换和中断上下⽂切换**。

> 进程的上下⽂切换到底是切换什么呢？

​	进程是由内核管理和调度的，所以进程的切换只能发⽣在内核态。所以，**进程的上下⽂切换不仅包含了虚拟内存、栈、全局变量等⽤户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。**通常，会把交换的信息保存在进程的 PCB，当要运⾏另外⼀个进程的时候，我们需要从这个进程的 PCB取出上下⽂，然后恢复到 CPU 中，这使得这个进程可以继续执⾏，如下图所示：

![image-20211208101359534](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112081013720.png)

​	⼤家需要注意，进程的上下⽂开销是很关键的，我们希望它的开销越⼩越好，这样可以使得进程可以把更 多时间花费在执⾏程序上，⽽不是耗费在上下⽂切换。

> 发⽣进程上下⽂切换有哪些场景？

- 为了保证所有进程可以得到公平调度，CPU 时间被划分为⼀段段的时间⽚，这些时间⽚再被轮流分配 给各个进程。这样，当某个进程的时间⽚耗尽了，进程就从运⾏状态变为就绪状态，系统从就绪队列 选择另外⼀个进程运⾏；
- 进程在系统资源不⾜（⽐如内存不⾜)时，要等到资源满⾜后才可以运⾏，这个时候进程也会被挂 起，并由系统调度其他进程运⾏；
- 当进程通过睡眠函数  sleep  这样的⽅法将⾃⼰主动挂起时，⾃然也会重新调度；
- 当有优先级更⾼的进程运⾏时，为了保证⾼优先级进程的运⾏，当前进程会被挂起，由⾼优先级进程 来运⾏；
- 发⽣硬件中断时，CPU 上的进程会被中断挂起，转⽽执⾏内核中的中断服务程序； 以上，就是发⽣进程上下⽂切换的常⻅场景了。



#### 4.1.2 线程

​	在早期的操作系统中都是以进程作为独⽴运⾏的基本单位，直到后⾯，计算机科学家们⼜提出了更⼩的能 独⽴运⾏的基本单位，也就是 **线程**

##### 4.1.2.1 为什么使用线程？

​	我们举个例⼦，假设你要编写⼀个视频播放器软件，那么该软件功能的核⼼模块有三个：

- 从视频⽂件当中读取数据；

- 对读取的数据进⾏解压缩；
- 把解压缩后的视频数据播放出来；

对于单进程的实现⽅式，我想⼤家都会是以下这个⽅式：

![image-20211208101940369](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112081019589.png)

对于单进程的这种⽅式，存在以下问题：

- 播放出来的画⾯和声⾳会不连贯，因为当 CPU 能⼒不够强的时候， Read 的时候可能进程就等在这了，这样就会导致等半天才进⾏数据解压和播放；
- 各个函数之间不是并发执⾏，影响资源的使⽤效率；

那改进成多进程的⽅式：

![image-20211208102210394](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112081022519.png)

对于多进程的这种⽅式，依然会存在问题：

- 进程之间如何通信，共享数据？维护进程的系统开销较⼤，如创建进程时，分配资源、建⽴ PCB；终⽌进程时，回收资源、撤销PCB；
- 进程切换时，保存当前进程的状态信息；

那到底如何解决呢？需要有⼀种新的实体，满⾜以下特性：

- 实体之间可以并发运⾏；
- 实体之间共享相同的地址空间；

这个新的实体，就是**线程(Thread** **)**，线程之间可以并发运⾏且共享相同的地址空间。



##### 4.1.2.2 什么是线程？

> 什么是线程？

​	**线程是进程当中的⼀条执行流程。**同⼀个进程内多个线程之间可以共享代码段、数据段、打开的⽂件等资源，但每个线程各⾃都有⼀套独⽴的寄存器和栈，这样可以确保线程的控制流是相对独⽴的。

![image-20211208102559052](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112081025296.png)

> 线程的优缺点？

**线程的优点**：

- ⼀个进程中可以同时存在多个线程；
- 各个线程之间可以并发执⾏；
- 各个线程之间可以共享地址空间和⽂件等资源；

**线程的缺点**：

- 当进程中的⼀个线程崩溃时，会导致其所属进程的所有线程崩溃。

​	举个例⼦，对于游戏的⽤户设计，则不应该使⽤多线程的⽅式，否则⼀个⽤户挂了，会影响其他同个进程

的线程。

##### **4.1.2.3** 线程和进程的比较

线程与进程的⽐较如下：

- 进程是资源（包括内存、打开的⽂件等）分配的单位，线程是 CPU 调度的单位；
- 进程拥有⼀个完整的资源平台，⽽线程只独享必不可少的资源，如寄存器和栈；
- 线程同样具有就绪、阻塞、执⾏三种基本状态，同样具有状态之间的转换关系；
- 线程能减少并发执⾏的时间和空间开销；

对于，线程相⽐进程能减少开销，体现在：

- 线程的创建时间⽐进程快，因为进程在创建的过程中，还需要资源管理信息，⽐如内存管理信息、⽂

件管理信息，⽽线程在创建的过程中，不会涉及这些资源管理信息，⽽是共享它们；

- 线程的终⽌时间⽐进程快，因为线程释放的资源相⽐进程少很多；

- 同⼀个进程内的线程切换⽐进程切换快，因为线程具有相同的地址空间（虚拟内存共享），这意味着同⼀个进程的线程都具有同⼀个⻚表，那么在切换的时候不需要切换⻚表。⽽对于进程之间的切换，切换的时候要把⻚表给切换掉，⽽⻚表的切换过程开销是⽐较⼤的；

- 由于同⼀进程的各线程间共享内存和⽂件资源，那么在线程之间数据传递的时候，就不需要经过内核了，这就使得线程之间的数据交互效率更⾼了；所以，不管是时间效率，还是空间效率线程⽐进程都要⾼。



#####  4.1.2.4  线程的上下文切换

​	在前⾯我们知道了，线程与进程最⼤的区别在于：**线程是调度的基本单位，⽽进程则是资源拥有的基本单位**。所以，所谓操作系统的任务调度，实际上的调度对象是线程，⽽进程只是给线程提供了虚拟内存、全局变量等资源。对于线程和进程，我们可以这么理解：当进程只有⼀个线程时，可以认为进程就等于线程；当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源，这些资源在上下⽂切换时是不需要修改的；另外，线程也有⾃⼰的私有数据，⽐如栈和寄存器等，这些在上下⽂切换时也是需要保存的。

> 线程上下文切换的是什么？

​	这还得看线程是不是属于同⼀个进程：

- 当两个线程不是属于同⼀个进程，则切换的过程就跟进程上下⽂切换⼀样；

- **当两个线程是属于同⼀个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不**

**动，只需要切换线程的私有数据、寄存器等不共享的数据**；

所以，线程的上下⽂切换相⽐进程，开销要⼩很多。



##### **4.1.2.5** 线程的实现

主要有三种线程的实现⽅式：

- **用户线程（User Thread）**: 在⽤户空间实现的线程，不是由内核管理的线程，是由⽤户态的线程库来完成线程的管理；
- **内核线程（Kernel Thread）**：在内核中实现的线程，是由内核管理的线程；
- **轻量级进程（LightWeight Process）**: 在内核中来⽀持⽤户线程；

​	那么，这还需要考虑⼀个问题，⽤户线程和内核线程的对应关系: 

⾸先，第⼀种关系是**多对⼀**的关系，也就是多个⽤户线程对应同⼀个内核线程：

![image-20211211075902262](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110759446.png)

第⼆种是**⼀对⼀**的关系，也就是⼀个⽤户线程对应⼀个内核线程：

 ![image-20211211075935581](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110759671.png)

第三种是**多对多**的关系，也就是多个⽤户线程对应到多个内核线程：

![image-20211211080007131](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110800224.png)

> 用户线程如何理解？存在什么优势和缺陷？

​	⽤户线程是基于⽤户态的线程管理库来实现的，那么**线程控制块（Thread Control Block, TCB）**也是在 库⾥⾯来实现的，对于操作系统⽽⾔是看不到这个  TCB  的，它只能看到整个进程的  PCB。所以，**⽤户线程的整个线程管理和调度，操作系统是不直接参与的，⽽是由⽤户级线程库函数来完成线程的管理，包括线程的创建、终止、同步和调度等。**

 	⽤户级线程的模型，也就类似前⾯提到的**多对⼀**的关系，即多个⽤户线程对应同⼀个内核线程，如下图所 示：

![image-20211211080428230](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110804317.png)

⽤户线程的**优点**：

- 每个进程都需要有它私有的线程控制块（TCB）列表，⽤来跟踪记录它各个线程状态信息（PC、栈指 针、寄存器)，TCB  由⽤户级线程库函数来维护，可⽤于不⽀持线程技术的操作系统；

- ⽤户线程的切换也是由线程库函数来完成的，⽆需⽤户态与内核态的切换，所以速度特别快；

⽤户线程的**缺点**：

- 由于操作系统不参与线程的调度，如果⼀个线程发起了系统调⽤⽽阻塞，那进程所包含的⽤户线程都不能执⾏了。

- 当⼀个线程开始运⾏后，除⾮它主动地交出 CPU 的使⽤权，否则它所在的进程当中的其他线程⽆法 运⾏，因为⽤户态的线程没法打断当前运⾏中的线程，它没有这个特权，只有操作系统才有，但是⽤ 户线程不是由操作系统管理的。

- 由于时间⽚分配给进程，故与其他进程⽐，在多线程执⾏时，每个线程得到的时间⽚较少，执⾏会⽐ 较慢；

 以上，就是⽤户线程的优缺点了。

> 那内核线程如何理解？存在什么优势和缺陷？

​	**内核线程是由操作系统管理的，线程对应的TCB⾃然是放在操作系统⾥的，这样线程的创建、终⽌和管理都是由操作系统负责。**内核线程的模型，也就类似前⾯提到的**⼀对⼀**的关系，即⼀个⽤户线程对应⼀个内核线程，如下图所示：

![image-20211211080931224](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110809311.png)

内核线程的优点：

- 在一个进程当中，如果某个内核线程发起系统调⽤⽽被阻塞，并不会影响其他内核线程的运⾏；
- 分配给线程，多线程的进程获得更多的 CPU 运⾏时间； 

内核线程的缺点：

- 在支持内核线程的操作系统中，由内核来维护进程和线程的上下⽂信息，如  PCB  和  TCB；
- 线程的创建、终⽌和切换都是通过系统调⽤的⽅式来进⾏，因此对于系统来说，系统开销⽐较⼤； 以上，就是内核线程的优缺点了。

> 最后的轻量级进程如何理解？

​	**轻量级进程（Light-weight process，LWP）是内核⽀持的⽤户线程，⼀个进程可有⼀个或多个 LWP，每 个  LWP  是跟内核线程⼀对⼀映射的，也就是  LWP 都是由⼀个内核线程⽀持。**

​	另外，LWP  只能由内核管理并像普通进程⼀样被调度，Linux  内核是⽀持  LWP 的典型例⼦。在⼤多数系统中，LWP与普通进程的区别也在于它只有⼀个最⼩的执⾏上下⽂和调度程序所需的统计信 息。⼀般来说，⼀个进程代表程序的⼀个实例，⽽ LWP 代表程序的执⾏线程，因为⼀个执⾏线程不像进程 那样需要那么多状态信息，所以  LWP  也不带有这样的信息。

​	在  LWP  之上也是可以使⽤⽤户线程的，那么  LWP 与⽤户线程的对应关系就有三种：

- `1:1`: 即⼀个 LWP 对应  ⼀个⽤户线程；
- `N:1`: 即⼀个  LWP 对应多个⽤户线程；
- `M:N`: 即多个 LMP 对应多个⽤户线程；

接下来针对上⾯这三种对应关系说明它们优缺点。先看下图的LWP模型：

![image-20211211081629453](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110816548.png)

**1：1模式**

⼀个线程对应到⼀个  LWP  再对应到⼀个内核线程，如上图的进程 4，属于此模型。

- **优点**： 实现并⾏，当⼀个  LWP  阻塞，不会影响其他 LWP；
- **缺点**： 每⼀个⽤户线程，就产⽣⼀个内核线程，创建线程的开销较⼤。



**N:1模式**

​	多个⽤户线程对应⼀个 LWP 再对应⼀个内核线程，如上图的进程 2，线程管理是在⽤户空间完成的，此模 式中⽤户的线程对操作系统不可⻅。

- 优点：⽤户线程要开⼏个都没问题，且上下⽂切换发⽣⽤户空间，切换的效率较⾼；

- 缺点: ⼀个⽤户线程如果阻塞了，则整个进程都将会阻塞，另外在多核  CPU   中，是没办法充分利⽤

  CPU 的。

**M:N模式**

​	根据前⾯的两个模型混搭⼀起，就形成M:N模型，该模型提供了两级控制，⾸先多个⽤户线程对应到多

个  LWP，LWP  再⼀⼀对应到内核线程，如上图的进程 3。

- **优点**： 综合了前两种优点，⼤部分的线程上下⽂发⽣在⽤户空间，且多个线程⼜可以充分利⽤多核

  CPU 的资源。

**组合模式**

​	如上图的进程  5，此进程结合`1：1`模型和`M:N`模型。开发⼈员可以针对不同的应⽤特点调节内核线程的数⽬来达到物理并⾏性和逻辑并⾏性的最佳⽅案。



#### 4.1.3 调度

​	进程都希望⾃⼰能够占⽤  CPU 进⾏⼯作，那么这涉及到前⾯说过的进程上下⽂切换。⼀旦操作系统把进程切换到运⾏状态，也就意味着该进程占⽤着 CPU 在执⾏，但是当操作系统把进程切换到其他状态时，那就不能在  CPU 中执⾏了，于是操作系统会选择下⼀个要运⾏的进程。选择⼀个进程运⾏这⼀功能是在操作系统中完成的，通常称为**调度程序（scheduler）**。 那到底什么时候调度进程，或以什么原则来调度进程呢？

##### 调度时机

​	在进程的⽣命周期中，当进程从⼀个运⾏状态到另外⼀状态变化的时候，其实会触发⼀次调度。⽐如，以下状态的变化都会触发操作系统的调度：

- **就绪态 -> 运行态**：当进程被创建时，会进⼊到就绪队列，操作系统会从就绪队列选择⼀个进程运⾏；
- **运行态 -> 阻塞态**：当进程发⽣  I/O 事件⽽阻塞时，操作系统必须另外⼀个进程运⾏；
- **运行态 -> 结束态**：当进程退出结束后，操作系统得从就绪队列选择另外⼀个进程运⾏；

​	因为，这些状态变化的时候，操作系统需要考虑是否要让新的进程给  CPU  运⾏，或者是否让当前进程从CPU  上退出来⽽换另⼀个进程运⾏。另外，如果硬件时钟提供某个频率的周期性中断，那么可以根据如何处理时钟中断，把调度算法分为两类：

- **非抢占式调度算法**： 挑选⼀个进程，然后让该进程运⾏直到被阻塞，或者直到该进程退出，才会调⽤另 外⼀个进程，也就是说不会理时钟中断这个事情。
- **抢占式调度算法**： 挑选⼀个进程，然后让该进程只运⾏某段时间，如果在该时段结束时，该进程仍然在 运⾏时，则会把它挂起，接着调度程序从就绪队列挑选另外⼀个进程。这种抢占式调度处理，需要在 时间间隔的末端发⽣时钟中断，以便把 CPU 控制返回给调度程序进⾏调度，也就是常说的时间⽚机 制。



##### 调度原则

1. 如果运⾏的程序，发⽣了 I/O 事件的请求，那 CPU 使⽤率必然会很低，因为此时进程在阻塞等待硬盘的数据返回。这样的过程，势必会造成 CPU 突然的空闲。所以，**为了提⾼ CPU 利⽤率，在这种发送 I/O  事件致使  CPU  空闲的情况下，调度程序需要从就绪队列中选择⼀个进程来运⾏。**

2. 有的程序执⾏某个任务花费的时间会⽐较⻓，如果这个程序⼀直占⽤着   CPU，会造成系统吞吐量（CPU 在单位时间内完成的进程数量）的降低。所以，**要提⾼系统的吞吐率，调度程序要权衡⻓任务和短 任务进程的运⾏完成数量。**

3. 从进程开始到结束的过程中，实际上是包含两个时间，分别是进程运⾏时间和进程等待时间，这 两个时间总和就称为**周转时间**。进程的周转时间越⼩越好，**如果进程的等待时间很⻓⽽运⾏时间很短，那 周转时间就很⻓，这不是我们所期望的，调度程序应该避免这种情况发⽣。**

4. 处于就绪队列的进程，也不能等太久，当然希望这个等待的时间越短越好，这样可以使得进程更 快的在  CPU 中执⾏。所以，**就绪队列中进程的等待时间也是调度程序所需要考虑的原则。**

5. 对于⿏标、键盘这种交互式⽐较强的应⽤，我们当然希望它的响应时间越快越好，否则就会影响

   ⽤户体验了。所以，**对于交互式⽐较强的应⽤，响应时间也是调度程序需要考虑的原则**。

    

![image-20211211091034968](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110910064.png)

针对上⾯的五种调度原则，总结成如下：

- **CPU利用率**：调度程序应确保  CPU 是始终匆忙的状态，这可提⾼  CPU 的利⽤率；
- **系统吞吐量**：吞吐量表示的是单位时间内 CPU 完成进程的数量，⻓作业的进程会占⽤较⻓的 CPU 资 源，因此会降低吞吐量，相反，短作业的进程会提升系统吞吐量；
- **周转时间**：周转时间是进程运⾏和阻塞时间总和，⼀个进程的周转时间越⼩越好；
- **等待时间**：这个等待时间不是阻塞状态的时间，⽽是进程处于就绪队列的时间，等待的时间越⻓，⽤ 户越不满意；
- **响应时间**： ⽤户提交请求到系统第⼀次产⽣响应所花费的时间，在交互式系统中，响应时间是衡量调 度算法好坏的主要标准。



##### 调度算法

​	不同的调度算法适⽤的场景也是不同的。接下来，说说在**单核  CPU 系统**中常⻅的调度算法。

###### 01 先来先服务调度算法

![image-20211211091422934](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110914656.png)

​	最简单的⼀个调度算法，就是⾮抢占式的先来先服务（First  Come  First  Seved, FCFS）算法了。顾名思义，先来后到，**每次从就绪队列选择最先进⼊队列的进程，然后⼀直运⾏，直到进程退出或被阻 塞，才会继续从队列中选择第⼀个进程接着运⾏**。这似乎很公平，但是当⼀个⻓作业先运⾏了，那么后⾯的短作业等待的时间就会很⻓，不利于短作业。FCFS 对⻓作业有利，适⽤于  CPU 繁忙型作业的系统，⽽不适⽤于  I/O 繁忙型作业的系统。

###### 02 最短作业优先调度算法

![image-20211211091547854](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110915922.png)



​	**最短作业优先（Shortest Job First, SJF）调度算法**，同样也是顾名思义，它会优先选择运⾏时间最短的进 程来运⾏，这有助于提⾼系统的吞吐量。这显然对⻓作业不利，很容易造成⼀种极端现象。⽐如，⼀个⻓作业在就绪队列等待运⾏，⽽这个就绪队列有⾮常多的短作业，那么就会使得⻓作业不断的 往后推，周转时间变⻓，致使⻓作业⻓期不会被运⾏。

###### 03 高响应比优先调度算法

​	前⾯的「先来先服务调度算法」和「最短作业优先调度算法」都没有很好的权衡短作业和⻓作业。那么，**⾼响应⽐优先（Highest  Response  Ratio  Next, HRRN）调度算法**主要是权衡了短作业和⻓作业。**每次进⾏进程调度时，先计算「响应⽐优先级」，然后把「响应⽐优先级」最⾼的进程投⼊运⾏**。

>  响应⽐优先级」的计算公式：

![image-20211211091755529](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110917606.png)

从上⾯的公式，可以发现：

- 如果两个进程的「等待时间」相同时，「要求的服务时间」越短，「响应⽐」就越⾼，这样短作业的 进程容易被选中运⾏；
- 如果两个进程「要求的服务时间」相同时，「等待时间」越⻓，「响应⽐」就越⾼，这就兼顾到了⻓ 作业进程，因为进程的响应⽐可以随时间等待的增加⽽提⾼，当其等待时间⾜够⻓时，其响应⽐便可 以升到很⾼，从⽽获得运⾏的机会；

###### 04 时间轮转调度算法

![image-20211211091933872](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110919945.png)

**时间⽚轮转（Round  Robin,  RR）调度算法**：**每个进程被分配⼀个时间段，称为时间⽚（Quantum），即允许该进程在该时间段中运⾏。**

- 如果时间⽚⽤完，进程还在运⾏，那么将会把此进程从  CPU  释放出来，并把  CPU 分配给另外⼀个进程；
- 如果该进程在时间⽚结束前阻塞或结束，则 CPU ⽴即进⾏切换； 另外，时间⽚的⻓度就是⼀个很关键的点：
- 如果时间⽚设得太短会导致过多的进程上下⽂切换，降低了  CPU  效率；
- 如果设得太⻓⼜可能引起对短作业进程的响应时间变⻓。

一般来说，时间片设为20ms~50ms 通常是一个比较合理的折中值。

###### 05 最⾼优先级调度算法

​	前⾯的「时间⽚轮转算法」做了个假设，即让所有的进程同等重要，也不偏袒谁，⼤家的运⾏时间都⼀  样。但是，对于多⽤户计算机系统就有不同的看法了，它们希望调度是有优先级的，即希望调度程序能从就绪 队列中选择最⾼优先级的进程进⾏运⾏，这称为**最⾼优先级（Highest  Priority  First，HPF）调度算法**。

进程的优先级可以分为，静态优先级和动态优先级：

- **静态优先级**：创建进程时候，就已经确定了优先级了，然后整个运⾏时间优先级都不会变化；
- **动态优先级**： 根据进程的动态变化调整优先级，⽐如如果进程运⾏时间增加，则降低其优先级，如果 进程等待时间（就绪队列的等待时间）增加，则升⾼其优先级，也就是随着时间的推移增加等待进程 的优先级。

该算法也有两种处理优先级⾼的⽅法，⾮抢占式和抢占式：

- **非抢占式**：当就绪队列中出现优先级⾼的进程，运⾏完当前进程，再选择优先级⾼的进程。
- **抢占式**： 当就绪队列中出现优先级⾼的进程，当前进程挂起，调度优先级⾼的进程运⾏。 但是依然有缺点，可能会导致低优先级的进程永远不会运⾏。

###### 06 多级反馈队列调度算法

​	**多级反馈队列（Multilevel Feedback Queue）调度算法**是「时间⽚轮转算法」和「最⾼优先级算法」的 综合和发展。顾名思义：

- **多级**：表示有多个队列，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越短。
- **反馈**： 表示如果有新的进程加⼊优先级⾼的队列时，⽴刻停⽌当前正在运⾏的进程，转⽽去运⾏优 先级⾼的队列；

![image-20211211092956540](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110929622.png)

来看看，它是如何⼯作的：

- 设置了多个队列，赋予每个队列不同的优先级，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越 短；
- 新的进程会被放⼊到第⼀级队列的末尾，按先来先服务的原则排队等待被调度，如果在第⼀级队列规 定的时间⽚没运⾏完成，则将其转⼊到第⼆级队列的末尾，以此类推，直⾄完成；
- 当较⾼优先级的队列为空，才调度较低优先级的队列中的进程运⾏。如果进程运⾏时，有新进程进⼊ 较⾼优先级的队列，则停⽌当前运⾏的进程并将其移⼊到原队列末尾，接着让较⾼优先级的进程运⾏；

​	可以发现，对于短作业可能可以在第⼀级队列很快被处理完。对于⻓作业，如果在第⼀级队列处理不完， 可以移⼊下次队列等待被执⾏，虽然等待的时间变⻓了，但是运⾏时间也变更⻓了，所以该算法很好的**兼 顾了⻓短作业，同时有较好的响应时间**。

###### 举例说明

​	**办理业务的客户相当于进程，银⾏窗⼝⼯作⼈员相当于  CPU。**现在，假设这个银⾏只有⼀个窗⼝（单核  CPU ），那么⼯作⼈员⼀次只能处理⼀个业务。

![image-20211211093433092](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110934177.png)

​	那么最简单的处理⽅式，就是先来的先处理，后⾯来的就乖乖排队，这就是**先来先服务（FCFS）调度算 法**。但是万⼀先来的这位⽼哥是来贷款的，这⼀谈就好⼏个⼩时，⼀直占⽤着窗⼝，这样后⾯的⼈只能⼲ 等，或许后⾯的⼈只是想简单的取个钱，⼏分钟就能搞定，却因为前⾯⽼哥办⻓业务⽽要等⼏个⼩时，你 说⽓不⽓⼈？

 ![image-20211211093520063](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110935154.png)

​	有客户抱怨了，那我们就要改进，我们⼲脆优先给那些⼏分钟就能搞定的⼈办理业务，这就是**短作业优先（SJF）调度算法**。听起来不错，但是依然还是有个极端情况，万⼀办理短业务的⼈⾮常的多，这会导致⻓业务的⼈⼀直得不到服务，万⼀这个⻓业务是个⼤客户，那不就捡了芝麻丢了⻄⽠。

![image-20211211093634049](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110936141.png)

​	那就公平起⻅，现在窗⼝⼯作⼈员规定，每个⼈我只处理  10  分钟。如果  10 分钟之内处理完，就⻢上换下⼀个⼈。如果没处理完，依然换下⼀个⼈，但是客户⾃⼰得记住办理到哪个步骤了。这个也就是**时间⽚轮 转（RR）调度算法**。但是如果时间⽚设置过短，那么就会造成⼤量的上下⽂切换，增⼤了系统开销。如果 时间⽚过⻓，相当于退化成  FCFS 算法了。

![image-20211211093723530](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110937613.png)

​	既然公平也可能存在问题，那银⾏就对客户分等级，分为普通客户、VIP 客户、SVIP 客户。只要⾼优先级 的客户⼀来，就第⼀时间处理这个客户，这就是**最⾼优先级（HPF）调度算法**。但依然也会有极端的问 题，万⼀当天来的全是⾼级客户，那普通客户不是没有被服务的机会，不把普通客户当⼈是吗？那我们把 优先级改成动态的，如果客户办理业务时间增加，则降低其优先级，如果客户等待时间增加，则升⾼其优先级。

![image-20211211093759666](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110937747.png)

​	那有没有兼顾到公平和效率的⽅式呢？这⾥介绍⼀种算法，考虑的还算充分的，多级反馈队列（MFQ）调 度算法，它是时间⽚轮转算法和优先级算法的综合和发展。它的⼯作⽅式：

![image-20211211093831388](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110938472.png)

- 银行设置了多个排队（就绪）队列，每个队列都有不同的优先级，各个队列优先级从⾼到低，同时每 个队列执⾏时间⽚的⻓度也不同，优先级越⾼的时间⽚越短。
- 新客户（进程）来了，先进⼊第⼀级队列的末尾，按先来先服务原则排队等待被叫号（运⾏）。如果 时间⽚⽤完客户的业务还没办理完成，则让客户进⼊到下⼀级队列的末尾，以此类推，直⾄客户业务 办理完成。
- 当第⼀级队列没⼈排队时，就会叫号⼆级队列的客户。如果客户办理业务过程中，有新的客户加⼊到 较⾼优先级的队列，那么此时办理中的客户需要停⽌办理，回到原队列的末尾等待再次叫号，因为要 把窗⼝让给刚进⼊较⾼优先级队列的客户。

​	可以发现，对于要办理短业务的客户来说，可以很快的轮到并解决。对于要办理⻓业务的客户，⼀下⼦解 决不了，就可以放到下⼀个队列，虽然等待的时间稍微变⻓了，但是轮到⾃⼰的办理时间也变⻓了，也可 以接受，不会造成极端的现象，可以说是综合上⾯⼏种算法的优点。



### 4.2 进程间通信

![image-20211211094601116](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110946742.png)

​	每个进程的用户地址空间都是独立的，一般而言是不能互相访问的，但内核空间是每个进程都共享的，所以进程之间要通信必须通过内核。Linux  内核提供了不少进程间通信的机制，我们来⼀起瞧瞧有哪些？

#### 4.2.1 管道

如果你学过 Linux 命令，那你肯定很熟悉「`|`」这个竖线。

```
$ ps auxf | grep mysql
```

上面命令行里的「`|`」竖线就是一个**管道**，它的功能是将前一个命令（`ps auxf`）的输出，作为后一个命令（`grep mysql`）的输入，从这功能描述，可以看出**管道传输数据是单向的**，如果想相互通信，我们需要创建两个管道才行。

同时，我们得知上面这种管道是没有名字，所以「`|`」表示的管道称为**匿名管道**，用完了就销毁。

管道还有另外一个类型是**命名管道**，也被叫做 `FIFO`，因为数据是先进先出的传输方式。

在使用命名管道前，先需要通过 `mkfifo` 命令来创建，并且指定管道名字：

```shell
# 创建名为myPipe的管道
$ mkfifo myPipe
```

myPipe 就是这个管道的名称，基于 Linux 一切皆文件的理念，所以管道也是以文件的方式存在，我们可以用 ls 看一下，这个文件的类型是 p，也就是 pipe（管道） 的意思：

```shell
$ ls -l
prw-r--r--. 1 root    root         0 Jul 17 02:45 myPipe
```

接下来，我们往 myPipe 这个管道写入数据：

```
$ echo "hello" > myPipe  // 将数据写进管道
                         // 停住了 ...
```

​	你操作了后，你会发现命令执行后就停在这了，这是因为管道里的内容没有被读取，只有当管道里的数据被读完后，命令才可以正常退出。

​	于是，我们执行另外一个命令来读取这个管道里的数据：

```
$ cat < myPipe  // 读取管道里的数据
hello
```

可以看到，管道里的内容被读取出来了，并打印在了终端上，另外一方面，echo 那个命令也正常退出了。我们可以看出，**管道这种通信方式效率低，不适合进程间频繁地交换数据**。当然，它的好处，自然就是简单，同时也我们很容易得知管道里的数据已经被另一个进程读取了。

> 那管道如何创建呢，背后原理是什么？

匿名管道的创建，需要通过下面这个系统调用：

```shell
int pipe(int fd[2])
```

这里表示创建一个匿名管道，并返回了两个描述符，一个是管道的读取端描述符 `fd[0]`，另一个是管道的写入端描述符 `fd[1]`。注意，这个匿名管道是特殊的文件，只存在于内存，不存于文件系统中。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161511319.webp)

其实，**所谓的管道，就是内核里面的一串缓存**。从管道的一段写入的数据，实际上是缓存在内核中的，另一端读取，也就是从内核中读取这段数据。另外，管道传输的数据是无格式的流且大小受限。看到这，你可能会有疑问了，这两个描述符都是在一个进程里面，并没有起到进程间通信的作用，怎么样才能使得管道是跨过两个进程的呢？

我们可以使用 `fork` 创建子进程，**创建的子进程会复制父进程的文件描述符**，这样就做到了两个进程各有两个「 `fd[0]` 与 `fd[1]`」，两个进程就可以通过各自的 fd 写入和读取同一个管道文件实现跨进程通信了。

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZckxn1SzJ697nE1m1wJzmPQVOgyU702gpwGJppjZCBXI4XDFNwBYR2wxG2MgKvfJvfjzfmKicjg01A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​	管道只能一端写入，另一端读出，所以上面这种模式容易造成混乱，因为父进程和子进程都可以同时写入，也都可以读出。那么，为了避免这种情况，通常的做法是：

- 父进程关闭读取的 fd[0]，只保留写入的 fd[1]；
- 子进程关闭写入的 fd[1]，只保留读取的 fd[0]；



![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZckxn1SzJ697nE1m1wJzmPQgD8dzOZUnAfmVVndTmtGgZRNZsBFEYghLPBjicziam2E1iapicANMYRXbg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​	所以说如果需要双向通信，则应该创建两个管道。到这里，我们仅仅解析了使用管道进行父进程与子进程之间的通信，但是在我们 shell 里面并不是这样的。在 shell 里面执行 `A | B`命令的时候，A 进程和 B 进程都是 shell 创建出来的子进程，A 和 B 之间不存在父子关系，它俩的父进程都是 shell。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112161511421.webp)

​	所以说，在 shell 里通过「`|`」匿名管道将多个命令连接在一起，实际上也就是创建了多个子进程，那么在我们编写 shell 脚本时，能使用一个管道搞定的事情，就不要多用一个管道，这样可以减少创建子进程的系统开销。我们可以得知，**对于匿名管道，它的通信范围是存在父子关系的进程**。因为管道没有实体，也就是没有管道文件，只能通过 fork 来复制父进程 fd 文件描述符，来达到通信的目的。另外，**对于命名管道，它可以在不相关的进程间也能相互通信**。因为命令管道，提前创建了一个类型为管道的设备文件，在进程里只要使用这个设备文件，就可以相互通信。不管是匿名管道还是命名管道，进程写入的数据都是缓存在内核中，另一个进程读取数据时候自然也是从内核中获取，同时通信数据都遵循**先进先出**原则，不支持 lseek 之类的文件定位操作。



#### 4.2.2 消息队列

前面说到管道的通信方式是效率低的，因此管道不适合进程间频繁地交换数据。

对于这个问题，**消息队列**的通信模式就可以解决。比如，A 进程要给 B 进程发送消息，A 进程把数据放在对应的消息队列后就可以正常返回了，B 进程需要的时候再去读取数据就可以了。同理，B 进程要给 A 进程发送消息也是如此。

再来，**消息队列是保存在内核中的消息链表**，在发送数据时，会分成一个一个独立的数据单元，也就是消息体（数据块），消息体是用户自定义的数据类型，消息的发送方和接收方要约定好消息体的数据类型，所以每个消息体都是固定大小的存储块，不像管道是无格式的字节流数据。如果进程从消息队列中读取了消息体，内核就会把这个消息体删除。

消息队列生命周期随内核，如果没有释放消息队列或者没有关闭操作系统，消息队列会一直存在，而前面提到的匿名管道的生命周期，是随进程的创建而建立，随进程的结束而销毁。

消息这种模型，两个进程之间的通信就像平时发邮件一样，你来一封，我回一封，可以频繁沟通了。

但邮件的通信方式存在不足的地方有两点，**一是通信不及时，二是附件也有大小限制**，这同样也是消息队列通信不足的点。

**消息队列不适合比较大数据的传输**，因为在内核中每个消息体都有一个最大长度的限制，同时所有队列所包含的全部消息体的总长度也是有上限。在 Linux 内核中，会有两个宏定义 `MSGMAX` 和 `MSGMNB`，它们以字节为单位，分别定义了一条消息的最大长度和一个队列的最大长度。

**消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销**，因为进程写入数据到内核中的消息队列时，会发生从用户态拷贝数据到内核态的过程，同理另一进程读取内核中的消息数据时，会发生从内核态拷贝数据到用户态的过程。



#### 4.2.3 共享内存

消息队列的读取和写入的过程，都会有发生用户态与内核态之间的消息拷贝过程。那**共享内存**的方式，就很好的解决了这一问题。

现代操作系统，对于内存管理，采用的是虚拟内存技术，也就是每个进程都有自己独立的虚拟内存空间，不同进程的虚拟内存映射到不同的物理内存中。所以，即使进程 A 和 进程 B 的虚拟地址是一样的，其实访问的是不同的物理内存地址，对于数据的增删查改互不影响。

**共享内存的机制，就是拿出一块虚拟地址空间来，映射到相同的物理内存中**。这样这个进程写入的东西，另外一个进程马上就能看到了，都不需要拷贝来拷贝去，传来传去，大大提高了进程间通信的速度。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250907750.webp)

#### 4.2.4 信息量

用了共享内存通信方式，带来新的问题，那就是如果多个进程同时修改同一个共享内存，很有可能就冲突了。例如两个进程都同时写一个地址，那先写的那个进程会发现内容被别人覆盖了。

为了防止多进程竞争共享资源，而造成的数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被一个进程访问。正好，**信号量**就实现了这一保护机制。

**信号量其实是一个整型的计数器，主要用于实现进程间的互斥与同步，而不是用于缓存进程间通信的数据**。

信号量表示资源的数量，控制信号量的方式有两种原子操作：

- 一个是 **P 操作**，这个操作会把信号量减去 -1，相减后如果信号量 < 0，则表明资源已被占用，进程需阻塞等待；相减后如果信号量 >= 0，则表明还有资源可使用，进程可正常继续执行。
- 另一个是 **V 操作**，这个操作会把信号量加上 1，相加后如果信号量 <= 0，则表明当前有阻塞中的进程，于是会将该进程唤醒运行；相加后如果信号量 > 0，则表明当前没有阻塞中的进程；

P 操作是用在进入共享资源之前，V 操作是用在离开共享资源之后，这两个操作是必须成对出现的。

接下来，举个例子，如果要使得两个进程互斥访问共享内存，我们可以初始化信号量为 `1`。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250913774.webp)

具体的过程如下：

- 进程 A 在访问共享内存前，先执行了 P 操作，由于信号量的初始值为 1，故在进程 A 执行 P 操作后信号量变为 0，表示共享资源可用，于是进程 A 就可以访问共享内存。
- 若此时，进程 B 也想访问共享内存，执行了 P 操作，结果信号量变为了 -1，这就意味着临界资源已被占用，因此进程 B 被阻塞。
- 直到进程 A 访问完共享内存，才会执行 V 操作，使得信号量恢复为 0，接着就会唤醒阻塞中的线程 B，使得进程 B 可以访问共享内存，最后完成共享内存的访问后，执行 V 操作，使信号量恢复到初始值 1。

可以发现，信号初始化为 `1`，就代表着是**互斥信号量**，它可以保证共享内存在任何时刻只有一个进程在访问，这就很好的保护了共享内存。

另外，在多进程里，每个进程并不一定是顺序执行的，它们基本是以各自独立的、不可预知的速度向前推进，但有时候我们又希望多个进程能密切合作，以实现一个共同的任务。

例如，进程 A 是负责生产数据，而进程 B 是负责读取数据，这两个进程是相互合作、相互依赖的，进程 A 必须先生产了数据，进程 B 才能读取到数据，所以执行是有前后顺序的。

那么这时候，就可以用信号量来实现多进程同步的方式，我们可以初始化信号量为 `0`。

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZckxn1SzJ697nE1m1wJzmPQlO6zu8K0xlLpDBbew0jVibibhVm59TQy4ibJSZKxqKsWOrcLIibZE6RAVg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

具体过程：

- 如果进程 B 比进程 A 先执行了，那么执行到 P 操作时，由于信号量初始值为 0，故信号量会变为 -1，表示进程 A 还没生产数据，于是进程 B 就阻塞等待；
- 接着，当进程 A 生产完数据后，执行了 V 操作，就会使得信号量变为 0，于是就会唤醒阻塞在 P 操作的进程 B；
- 最后，进程 B 被唤醒后，意味着进程 A 已经生产了数据，于是进程 B 就可以正常读取数据了。

可以发现，信号初始化为 `0`，就代表着是**同步信号量**，它可以保证进程 A 应在进程 B 之前执行。

#### 4.2.5 信号

上面说的进程间通信，都是常规状态下的工作模式。**对于异常情况下的工作模式，就需要用「信号」的方式来通知进程。**

信号跟信号量虽然名字相似度 66.66%，但两者用途完全不一样，就好像 Java 和 JavaScript 的区别。

在 Linux 操作系统中， 为了响应各种各样的事件，提供了几十种信号，分别代表不同的意义。我们可以通过 `kill -l` 命令，查看所有的信号：

```shell
$ kill -l
 1) SIGHUP       2) SIGINT       3) SIGQUIT      4) SIGILL       5) SIGTRAP
 6) SIGABRT      7) SIGBUS       8) SIGFPE       9) SIGKILL     10) SIGUSR1
11) SIGSEGV     12) SIGUSR2     13) SIGPIPE     14) SIGALRM     15) SIGTERM
16) SIGSTKFLT   17) SIGCHLD     18) SIGCONT     19) SIGSTOP     20) SIGTSTP
21) SIGTTIN     22) SIGTTOU     23) SIGURG      24) SIGXCPU     25) SIGXFSZ
26) SIGVTALRM   27) SIGPROF     28) SIGWINCH    29) SIGIO       30) SIGPWR
31) SIGSYS      34) SIGRTMIN    35) SIGRTMIN+1  36) SIGRTMIN+2  37) SIGRTMIN+3
38) SIGRTMIN+4  39) SIGRTMIN+5  40) SIGRTMIN+6  41) SIGRTMIN+7  42) SIGRTMIN+8
43) SIGRTMIN+9  44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13
48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12
53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9  56) SIGRTMAX-8  57) SIGRTMAX-7
58) SIGRTMAX-6  59) SIGRTMAX-5  60) SIGRTMAX-4  61) SIGRTMAX-3  62) SIGRTMAX-2
63) SIGRTMAX-1  64) SIGRTMAX
```

运行在 shell 终端的进程，我们可以通过键盘输入某些组合键的时候，给进程发送信号。例如

- Ctrl+C 产生 `SIGINT` 信号，表示终止该进程；
- Ctrl+Z 产生 `SIGTSTP` 信号，表示停止该进程，但还未结束；

如果进程在后台运行，可以通过 `kill` 命令的方式给进程发送信号，但前提需要知道运行中的进程 PID 号，例如：

- kill -9 1050 ，表示给 PID 为 1050 的进程发送 `SIGKILL` 信号，用来立即结束该进程；

所以，信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令）。

信号是进程间通信机制中**唯一的异步通信机制**，因为可以在任何时候发送信号给某一进程，一旦有信号产生，我们就有下面这几种，用户进程对信号的处理方式。

**1.执行默认操作**。Linux 对每种信号都规定了默认操作，例如，上面列表中的 SIGTERM 信号，就是终止进程的意思。Core 的意思是 Core Dump，也即终止进程后，通过 Core Dump 将当前进程的运行状态保存在文件里面，方便程序员事后进行分析问题在哪里。

**2.捕捉信号**。我们可以为信号定义一个信号处理函数。当信号发生时，我们就执行相应的信号处理函数。

**3.忽略信号**。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。有两个信号是应用进程无法捕捉和忽略的，即 `SIGKILL` 和 `SEGSTOP`，它们用于在任何时候中断或结束某一进程。

#### 4.2.6 Socket

前面提到的管道、消息队列、共享内存、信号量和信号都是在同一台主机上进行进程间通信，那要想**跨网络与不同主机上的进程之间通信，就需要 Socket 通信了。**

实际上，Socket 通信不仅可以跨网络与不同主机的进程间通信，还可以在同主机上进程间通信。

我们来看看创建 socket 的系统调用：

```shell
int socket(int domain, int type, int protocal)
```

三个参数分别代表：

- domain 参数用来指定协议族，比如 AF_INET 用于 IPV4、AF_INET6 用于 IPV6、AF_LOCAL/AF_UNIX 用于本机；
- type 参数用来指定通信特性，比如 SOCK_STREAM 表示的是字节流，对应 TCP、SOCK_DGRAM  表示的是数据报，对应 UDP、SOCK_RAW 表示的是原始套接字；
- protocal 参数原本是用来指定通信协议的，但现在基本废弃。因为协议已经通过前面两个参数指定完成，protocol 目前一般写成 0 即可；

根据创建 socket 类型的不同，通信的方式也就不同：

- 实现 TCP 字节流通信：socket 类型是 AF_INET 和 SOCK_STREAM；
- 实现 UDP 数据报通信：socket 类型是 AF_INET 和 SOCK_DGRAM；
- 实现本地进程间通信：「本地字节流 socket 」类型是 AF_LOCAL 和 SOCK_STREAM，「本地数据报 socket 」类型是 AF_LOCAL 和 SOCK_DGRAM。另外，AF_UNIX 和 AF_LOCAL 是等价的，所以 AF_UNIX 也属于本地 socket；

接下来，简单说一下这三种通信的编程模式。

> 针对 TCP 协议通信的 socket 编程模型

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250922629.webp)

- 服务端和客户端初始化 `socket`，得到文件描述符；
- 服务端调用 `bind`，将绑定在 IP 地址和端口;
- 服务端调用 `listen`，进行监听；
- 服务端调用 `accept`，等待客户端连接；
- 客户端调用 `connect`，向服务器端的地址和端口发起连接请求；
- 服务端 `accept` 返回用于传输的 `socket` 的文件描述符；
- 客户端调用 `write` 写入数据；服务端调用 `read` 读取数据；
- 客户端断开连接时，会调用 `close`，那么服务端 `read` 读取数据的时候，就会读取到了 `EOF`，待处理完数据后，服务端调用 `close`，表示连接关闭。

这里需要注意的是，服务端调用 `accept` 时，连接成功了会返回一个已完成连接的 socket，后续用来传输数据。

所以，监听的 socket 和真正用来传送数据的 socket，是「**两个**」 socket，一个叫作**监听 socket**，一个叫作**已完成连接 socket**。

成功连接建立之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。

> 针对 UDP 协议通信的 socket 编程模型

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250927477.webp)

UDP 是没有连接的，所以不需要三次握手，也就不需要像 TCP 调用 listen 和 connect，但是 UDP 的交互仍然需要 IP 地址和端口号，因此也需要 bind。

对于 UDP 来说，不需要要维护连接，那么也就没有所谓的发送方和接收方，甚至都不存在客户端和服务端的概念，只要有一个 socket 多台机器就可以任意通信，因此每一个 UDP 的 socket 都需要 bind。

另外，每次通信时，调用 sendto 和 recvfrom，都要传入目标主机的 IP 地址和端口。

> 针对本地进程间通信的 socket 编程模型

本地 socket  被用于在**同一台主机上进程间通信**的场景：

- 本地 socket 的编程接口和 IPv4 、IPv6 套接字编程接口是一致的，可以支持「字节流」和「数据报」两种协议；
- 本地 socket 的实现效率大大高于 IPv4 和 IPv6 的字节流、数据报 socket 实现；

对于本地字节流 socket，其 socket 类型是 AF_LOCAL 和 SOCK_STREAM。

对于本地数据报 socket，其 socket 类型是 AF_LOCAL 和 SOCK_DGRAM。

本地字节流 socket 和 本地数据报 socket 在 bind 的时候，不像 TCP 和 UDP 要绑定 IP 地址和端口，而是**绑定一个本地文件**，这也就是它们之间的最大区别。



#### 4.2.7 总结

由于每个进程的用户空间都是独立的，不能相互访问，这时就需要借助内核空间来实现进程间通信，原因很简单，每个进程都是共享一个内核空间。

Linux 内核提供了不少进程间通信的方式，其中最简单的方式就是管道，管道分为「匿名管道」和「命名管道」。

**匿名管道**顾名思义，它没有名字标识，匿名管道是特殊文件只存在于内存，没有存在于文件系统中，shell 命令中的「`|`」竖线就是匿名管道，通信的数据是**无格式的流并且大小受限**，通信的方式是**单向**的，数据只能在一个方向上流动，如果要双向通信，需要创建两个管道，再来**匿名管道是只能用于存在父子关系的进程间通信**，匿名管道的生命周期随着进程创建而建立，随着进程终止而消失。

**命名管道**突破了匿名管道只能在亲缘关系进程间的通信限制，因为使用命名管道的前提，需要在文件系统创建一个类型为 p 的设备文件，那么毫无关系的进程就可以通过这个设备文件进行通信。另外，不管是匿名管道还是命名管道，进程写入的数据都是**缓存在内核**中，另一个进程读取数据时候自然也是从内核中获取，同时通信数据都遵循**先进先出**原则，不支持 lseek 之类的文件定位操作。

**消息队列**克服了管道通信的数据是无格式的字节流的问题，消息队列实际上是保存在内核的「消息链表」，消息队列的消息体是可以用户自定义的数据类型，发送数据时，会被分成一个一个独立的消息体，当然接收数据时，也要与发送方发送的消息体的数据类型保持一致，这样才能保证读取的数据是正确的。消息队列通信的速度不是最及时的，毕竟**每次数据的写入和读取都需要经过用户态与内核态之间的拷贝过程。**

**共享内存**可以解决消息队列通信中用户态与内核态之间数据拷贝过程带来的开销，**它直接分配一个共享空间，每个进程都可以直接访问**，就像访问进程自己的空间一样快捷方便，不需要陷入内核态或者系统调用，大大提高了通信的速度，享有**最快**的进程间通信方式之名。但是便捷高效的共享内存通信，**带来新的问题，多进程竞争同个共享资源会造成数据的错乱。**

那么，就需要**信号量**来保护共享资源，以确保任何时刻只能有一个进程访问共享资源，这种方式就是互斥访问。**信号量不仅可以实现访问的互斥性，还可以实现进程间的同步**，信号量其实是一个计数器，表示的是资源个数，其值可以通过两个原子操作来控制，分别是 **P 操作和 V 操作**。

与信号量名字很相似的叫**信号**，它俩名字虽然相似，但功能一点儿都不一样。信号是进程间通信机制中**唯一的异步通信机制**，信号可以在应用进程和内核之间直接交互，内核也可以利用信号来通知用户空间的进程发生了哪些系统事件，信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令），一旦有信号发生，**进程有三种方式响应信号 1. 执行默认操作、2. 捕捉信号、3. 忽略信号**。有两个信号是应用进程无法捕捉和忽略的，即 `SIGKILL` 和 `SEGSTOP`，这是为了方便我们能在任何时候结束或停止某个进程。

前面说到的通信机制，都是工作于同一台主机，如果**要与不同主机的进程间通信，那么就需要 Socket 通信了**。Socket 实际上不仅用于不同的主机进程间通信，还可以用于本地主机进程间通信，可根据创建 Socket 的类型不同，分为三种常见的通信方式，一个是基于 TCP 协议的通信方式，一个是基于 UDP 协议的通信方式，一个是本地进程间通信方式。

以上，就是进程间通信的主要机制了。你可能会问了，那线程通信间的方式呢？

同个进程下的线程之间都是共享进程的资源，只要是共享变量都可以做到线程间通信，比如全局变量，所以对于线程间关注的不是通信方式，而是关注多线程竞争共享资源的问题，信号量也同样可以在线程间实现互斥与同步：

- 互斥的方式，可保证任意时刻只有一个线程访问共享资源；
- 同步的方式，可保证线程 A 应在线程 B 之前执行；





### 4.3 多线程同步与异步

#### 4.3.1 竞争与协作

在单核 CPU 系统里，为了实现多个程序同时运行的假象，操作系统通常以时间片调度的方式，让每个进程执行每次执行一个时间片，时间片用完了，就切换下一个进程运行，由于这个时间片的时间很短，于是就造成了「并发」的现象。

![并发](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250930339.webp)

另外，操作系统也为每个进程创建巨大、私有的虚拟内存的假象，这种地址空间的抽象让每个程序好像拥有自己的内存，而实际上操作系统在背后秘密地让多个地址空间「复用」物理内存或者磁盘。

![虚拟内存管理-换入换出](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250931936.webp)

如果一个程序只有一个执行流程，也代表它是单线程的。当然一个程序可以有多个执行流程，也就是所谓的多线程程序，线程是调度的基本单位，进程则是资源分配的基本单位。

所以，线程之间是可以共享进程的资源，比如代码段、堆空间、数据段、打开的文件等资源，但每个线程都有自己独立的栈空间。

![多线程](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250932321.webp)

那么问题就来了，多个线程如果竞争共享资源，如果不采取有效的措施，则会造成共享数据的混乱。

我们做个小实验，创建两个线程，它们分别对共享变量 `i` 自增 `1` 执行 `10000` 次，如下代码（虽然说是 C++ 代码，但是没学过 C++ 的同学也是看到懂的）：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250933949.webp)

按理来说，`i` 变量最后的值应该是 `20000`，但很不幸，并不是如此。我们对上面的程序执行一下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250933390.webp)

运行了两次，发现出现了 i 值的结果是 `15173`，也会出现 `20000` 的 i 值结果。

每次运行不但会产生错误，而且得到不同的结果。在计算机里是不能容忍的，虽然是小概率出现的错误，但是小概率事件它一定是会发生的，「墨菲定律」大家都懂吧。

> 为什么会发生这种情况？

为了理解为什么会发生这种情况，我们必须了解编译器为更新计数器 `i` 变量生成的代码序列，也就是要了解汇编指令的执行顺序。

在这个例子中，我们只是想给 `i` 加上数字 1，那么它对应的汇编指令执行过程是这样的：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250934495.webp)

可以发现，只是单纯给 `i` 加上数字 1，在 CPU 运行的时候，实际上要执行 `3` 条指令。

设想我们的线程 1 进入这个代码区域，它将 i 的值（假设此时是 50 ）从内存加载到它的寄存器中，然后它向寄存器加 1，此时在寄存器中的 i 值是 51。

现在，一件不幸的事情发生了：**时钟中断发生**。因此，操作系统将当前正在运行的线程的状态保存到线程的线程控制块 TCP。

现在更糟的事情发生了，线程 2 被调度运行，并进入同一段代码。它也执行了第一条指令，从内存获取 i 值并将其放入到寄存器中，此时内存中 i 的值仍为 50，因此线程 2 寄存器中的 i 值也是 50。假设线程 2 执行接下来的两条指令，将寄存器中的 i 值 + 1，然后将寄存器中的 i 值保存到内存中，于是此时全局变量 i 值是 51。

最后，又发生一次上下文切换，线程 1 恢复执行。还记得它已经执行了两条汇编指令，现在准备执行最后一条指令。回忆一下， 线程 1 寄存器中的 i 值是51，因此，执行最后一条指令后，将值保存到内存，全局变量 i 的值再次被设置为 51。

简单来说，增加 i （值为 50 ）的代码被运行两次，按理来说，最后的 i 值应该是 52，但是由于**不可控的调度**，导致最后 i 值却是 51。

针对上面线程 1 和线程 2 的执行过程，我画了一张流程图，会更明确一些：

![蓝色表示线程1，红色表示线程2](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250934498.webp)

##### 4.3.1.1 互斥的概念

上面展示的情况称为**竞争条件（race condition）**，当多线程相互竞争操作共享变量时，由于运气不好，即在执行过程中发生了上下文切换，我们得到了错误的结果，事实上，每次运行都可能得到不同的结果，因此输出的结果存在**不确定性（indeterminate）**。

由于多线程执行操作共享变量的这段代码可能会导致竞争状态，因此我们将此段代码称为**临界区（critical section），它是访问共享资源的代码片段，一定不能给多线程同时执行。**

我们希望这段代码是**互斥（mutualexclusion）的，也就说保证一个线程在临界区执行时，其他线程应该被阻止进入临界区**，说白了，就是这段代码执行过程中，最多只能出现一个线程。

![互斥](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250940312.webp)

另外，说一下互斥也并不是只针对多线程。在多进程竞争共享资源的时候，也同样是可以使用互斥的方式来避免资源竞争造成的资源混乱。

##### 4.3.1.2 同步的概念

互斥解决了并发进程/线程对临界区的使用问题。这种基于临界区控制的交互作用是比较简单的，只要一个进程/线程进入了临界区，其他试图想进入临界区的进程/线程都会被阻塞着，直到第一个进程/线程离开了临界区。

我们都知道在多线程里，每个线程并一定是顺序执行的，它们基本是以各自独立的、不可预知的速度向前推进，但有时候我们又希望多个线程能密切合作，以实现一个共同的任务。

例子，线程 1 是负责读入数据的，而线程 2 是负责处理数据的，这两个线程是相互合作、相互依赖的。线程 2 在没有收到线程 1 的唤醒通知时，就会一直阻塞等待，当线程 1 读完数据需要把数据传给线程 2 时，线程 1 会唤醒线程 2，并把数据交给线程 2 处理。

**所谓同步，就是并发进程/线程在一些关键点上可能需要互相等待与互通消息，这种相互制约的等待与互通信息称为进程/线程同步**。

举个生活的同步例子，你肚子饿了想要吃饭，你叫妈妈早点做菜，妈妈听到后就开始做菜，但是在妈妈没有做完饭之前，你必须阻塞等待，等妈妈做完饭后，自然会通知你，接着你吃饭的事情就可以进行了。

![吃饭和做菜的同步关系](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250942099.webp)

注意，同步与互斥是两种不同的概念：

- 同步就好比：「操作 A 应在操作 B 之前执行」，「操作 C 必须在操作 A 和操作 B 都完成之后才能执行」等；
- 互斥就好比：「操作 A 和操作 B 不能在同一时刻执行」；

#### 4.3.2 互斥与同步的实现

在进程/线程并发执行的过程中，进程/线程之间存在协作的关系，例如有互斥、同步的关系。

为了实现进程/线程间正确的协作，操作系统必须提供实现进程协作的措施和方法，主要的方法有两种：

- *锁*：加锁、解锁操作；
- *信号量*：P、V 操作；

这两个都可以方便地实现进程/线程互斥，而信号量比锁的功能更强一些，它还可以方便地实现进程/线程同步。

##### 4.3.2.1 锁

使用加锁操作和解锁操作可以解决并发线程/进程的互斥问题。

任何想进入临界区的线程，必须先执行加锁操作。若加锁操作顺利通过，则线程可进入临界区；在完成对临界资源的访问后再执行解锁操作，以释放该临界资源。

![加锁-解锁](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250944243.webp)

根据锁的实现不同，可以分为「忙等待锁」和「无忙等待锁」。

> 我们先来看看「忙等待锁」的实现

在说明「忙等待锁」的实现之前，先介绍现代 CPU 体系结构提供的特殊**原子操作指令 —— 测试和置位（\*Test-and-Set\*）指令**。

如果用 C 代码表示 Test-and-Set 指令，形式如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250945728.webp)

测试并设置指令做了下述事情:

- 把 `old_ptr` 更新为 `new` 的新值
- 返回 `old_ptr` 的旧值；

当然，**关键是这些代码是原子执行**。因为既可以测试旧值，又可以设置新值，所以我们把这条指令叫作「测试并设置」。

那什么是原子操作呢？**原子操作就是要么全部执行，要么都不执行，不能出现执行到一半的中间状态**

我们可以运用 Test-and-Set 指令来实现「忙等待锁」，代码如下：

![忙等待锁的实现](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250945807.webp)

我们来确保理解为什么这个锁能工作：

- 第一个场景是，首先假设一个线程在运行，调用 `lock()`，没有其他线程持有锁，所以 `flag` 是 0。当调用 `TestAndSet(flag, 1)` 方法，返回 0，线程会跳出 while 循环，获取锁。同时也会原子的设置 flag 为1，标志锁已经被持有。当线程离开临界区，调用 `unlock()` 将 `flag` 清理为 0。
- 第二种场景是，当某一个线程已经持有锁（即 `flag` 为1）。本线程调用 `lock()`，然后调用 `TestAndSet(flag, 1)`，这一次返回 1。只要另一个线程一直持有锁，`TestAndSet()` 会重复返回 1，本线程会一直**忙等**。当 `flag` 终于被改为 0，本线程会调用 `TestAndSet()`，返回 0 并且原子地设置为 1，从而获得锁，进入临界区。

很明显，当获取不到锁时，线程就会一直 wile 循环，不做任何事情，所以就被称为「忙等待锁」，也被称为**自旋锁（\*spin lock\*）**。

这是最简单的一种锁，一直自旋，利用 CPU 周期，直到锁可用。在单处理器上，需要抢占式的调度器（即不断通过时钟中断一个线程，运行其他线程）。否则，自旋锁在单 CPU 上无法使用，因为一个自旋的线程永远不会放弃 CPU。

> 再来看看[ 无等待锁 ]的实现

无等待锁顾明思议就是获取不到锁的时候，不用自旋。

既然不想自旋，那当没获取到锁的时候，就把当前线程放入到锁的等待队列，然后执行调度程序，把 CPU 让给其他线程执行。

![无等待锁的实现](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250946603.webp)

本次只是提出了两种简单锁的实现方式。当然，在具体操作系统实现中，会更复杂，但也离不开本例子两个基本元素。

如果你想要对锁的更进一步理解，推荐大家可以看《操作系统导论》第 28 章锁的内容，这本书在「微信读书」就可以免费看。

##### 4.3.2.2 信息量

信号量是操作系统提供的一种协调共享资源访问的方法。

通常**信号量表示资源的数量**，对应的变量是一个整型（`sem`）变量。

另外，还有**两个原子操作的系统调用函数来控制信号量的**，分别是：

- *P 操作*：将 `sem` 减 `1`，相减后，如果 `sem < 0`，则进程/线程进入阻塞等待，否则继续，表明 P 操作可能会阻塞；
- *V 操作*：将 `sem` 加 `1`，相加后，如果 `sem <= 0`，唤醒一个等待中的进程/线程，表明 V 操作不会阻塞；

P 操作是用在进入临界区之前，V 操作是用在离开临界区之后，这两个操作是必须成对出现的。

举个类比，2 个资源的信号量，相当于 2 条火车轨道，PV 操作如下图过程：

![信号量与火车轨道](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250947943.webp)

> 操作系统是如何实现 PV 操作的呢？

信号量数据结构与 PV 操作的算法描述如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250948638.webp)

PV 操作的函数是由操作系统管理和实现的，所以操作系统已经使得执行 PV 函数时是具有原子性的。

> PV 操作如何使用的呢？

信号量不仅可以实现临界区的互斥访问控制，还可以线程间的事件同步。

我们先来说说如何使用**信号量实现临界区的互斥访问**。

为每类共享资源设置一个信号量 `s`，其初值为 `1`，表示该临界资源未被占用。

只要把进入临界区的操作置于 `P(s)` 和 `V(s)` 之间，即可实现进程/线程互斥：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250948446.webp)

此时，任何想进入临界区的线程，必先在互斥信号量上执行 P 操作，在完成对临界资源的访问后再执行 V 操作。由于互斥信号量的初始值为 1，故在第一个线程执行 P 操作后 s 值变为 0，表示临界资源为空闲，可分配给该线程，使之进入临界区。

若此时又有第二个线程想进入临界区，也应先执行 P 操作，结果使 s 变为负值，这就意味着临界资源已被占用，因此，第二个线程被阻塞。

并且，直到第一个线程执行 V 操作，释放临界资源而恢复 s 值为 0 后，才唤醒第二个线程，使之进入临界区，待它完成临界资源的访问后，又执行 V 操作，使 s 恢复到初始值 1。

对于两个并发线程，互斥信号量的值仅取 1、0 和 -1 三个值，分别表示：

- 如果互斥信号量为 1，表示没有线程进入临界区；
- 如果互斥信号量为 0，表示有一个线程进入临界区；
- 如果互斥信号量为 -1，表示一个线程进入临界区，另一个线程等待进入。

通过互斥信号量的方式，就能保证临界区任何时刻只有一个线程在执行，就达到了互斥的效果。

再来，我们说说如何使用**信号量实现事件同步**。

同步的方式是设置一个信号量，其初值为 `0`。

我们把前面的「吃饭-做饭」同步的例子，用代码的方式实现一下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250948554.webp)

妈妈一开始询问儿子要不要做饭时，执行的是 `P(s1)` ，相当于询问儿子需不需要吃饭，由于 `s1` 初始值为 0，此时 `s1` 变成 -1，表明儿子不需要吃饭，所以妈妈线程就进入等待状态。

当儿子肚子饿时，执行了 `V(s1)`，使得 `s1` 信号量从 -1 变成 0，表明此时儿子需要吃饭了，于是就唤醒了阻塞中的妈妈线程，妈妈线程就开始做饭。

接着，儿子线程执行了 `P(s2)`，相当于询问妈妈饭做完了吗，由于 `s2` 初始值是 0，则此时 `s2` 变成 -1，说明妈妈还没做完饭，儿子线程就等待状态。

最后，妈妈终于做完饭了，于是执行 `V(s2)`，`s2` 信号量从 -1 变回了 0，于是就唤醒等待中的儿子线程，唤醒后，儿子线程就可以进行吃饭了。

##### 4.3.2.3 生产者-消费者问题

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250949671.webp)

生产者-消费者问题描述：

- **生产者**在生成数据后，放在一个缓冲区中；
- **消费者**从缓冲区取出数据处理；
- 任何时刻，**只能有一个**生产者或消费者可以访问缓冲区；

我们对问题分析可以得出：

- 任何时刻只能有一个线程操作缓冲区，说明操作缓冲区是临界代码，**需要互斥**；
- 缓冲区空时，消费者必须等待生产者生成数据；缓冲区满时，生产者必须等待消费者取出数据。说明生产者和消费者**需要同步**。

那么我们需要三个信号量，分别是：

- 互斥信号量 `mutex`：用于互斥访问缓冲区，初始化值为 1；
- 资源信号量 `fullBuffers`：用于消费者询问缓冲区是否有数据，有数据则读取数据，初始化值为 0（表明缓冲区一开始为空）；
- 资源信号量 `emptyBuffers`：用于生产者询问缓冲区是否有空位，有空位则生成数据，初始化值为 n （缓冲区大小）；

具体的实现代码：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250949511.webp)

如果消费者线程一开始执行 `P(fullBuffers)`，由于信号量 `fullBuffers` 初始值为 0，则此时 `fullBuffers` 的值从 0 变为 -1，说明缓冲区里没有数据，消费者只能等待。

接着，轮到生产者执行 `P(emptyBuffers)`，表示减少 1 个空槽，如果当前没有其他生产者线程在临界区执行代码，那么该生产者线程就可以把数据放到缓冲区，放完后，执行 `V(fullBuffers)` ，信号量 `fullBuffers` 从 -1 变成 0，表明有「消费者」线程正在阻塞等待数据，于是阻塞等待的消费者线程会被唤醒。

消费者线程被唤醒后，如果此时没有其他消费者线程在读数据，那么就可以直接进入临界区，从缓冲区读取数据。最后，离开临界区后，把空槽的个数 + 1。

#### 4.3.3 经典同步问题

##### 4.3.3.1 哲学假就餐问题

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250950430.webp)

先来看看哲学家就餐的问题描述：

- `5` 个老大哥哲学家，闲着没事做，围绕着一张圆桌吃面；
- 巧就巧在，这个桌子只有 `5` 支叉子，每两个哲学家之间放一支叉子；
- 哲学家围在一起先思考，思考中途饿了就会想进餐；
- **奇葩的是，这些哲学家要两支叉子才愿意吃面，也就是需要拿到左右两边的叉子才进餐**；
- **吃完后，会把两支叉子放回原处，继续思考**；

那么问题来了，如何保证哲学家们的动作有序进行，而不会出现有人永远拿不到叉子呢？

> 方案一

我们用信号量的方式，也就是 PV 操作来尝试解决它，代码如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250951760.webp)

上面的程序，好似很自然。拿起叉子用 P 操作，代表有叉子就直接用，没有叉子时就等待其他哲学家放回叉子。

![方案一的问题](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250952552.webp)

不过，这种解法存在一个极端的问题：**假设五位哲学家同时拿起左边的叉子，桌面上就没有叉子了， 这样就没有人能够拿到他们右边的叉子，也就说每一位哲学家都会在 `P(fork[(i + 1) % N ])` 这条语句阻塞了，很明显这发生了死锁的现象**。

> 方案二

既然「方案一」会发生同时竞争左边叉子导致死锁的现象，那么我们就在拿叉子前，加个互斥信号量，代码如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250953091.webp)

上面程序中的互斥信号量的作用就在于，**只要有一个哲学家进入了「临界区」，也就是准备要拿叉子时，其他哲学家都不能动，只有这位哲学家用完叉子了，才能轮到下一个哲学家进餐。**

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250953717.webp)

方案二虽然能让哲学家们按顺序吃饭，但是每次进餐只能有一位哲学家，而桌面上是有 5 把叉子，按道理是能可以有两个哲学家同时进餐的，所以从效率角度上，这不是最好的解决方案。

> 方案三

那既然方案二使用互斥信号量，会导致只能允许一个哲学家就餐，那么我们就不用它。

另外，方案一的问题在于，会出现所有哲学家同时拿左边刀叉的可能性，那我们就避免哲学家可以同时拿左边的刀叉，采用分支结构，根据哲学家的编号的不同，而采取不同的动作。

**即让偶数编号的哲学家「先拿左边的叉子后拿右边的叉子」，奇数编号的哲学家「先拿右边的叉子后拿左边的叉子」。**

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250959496.webp)

上面的程序，在 P 操作时，根据哲学家的编号不同，拿起左右两边叉子的顺序不同。另外，V 操作是不需要分支的，因为 V 操作是不会阻塞的。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112250959769.webp)

方案三即不会出现死锁，也可以两人同时进餐

> 方案四

在这里再提出另外一种可行的解决方案，我们**用一个数组 state 来记录每一位哲学家在进程、思考还是饥饿状态（正在试图拿叉子）。**

那么，**一个哲学家只有在两个邻居都没有进餐时，才可以进入进餐状态。**

第 `i` 个哲学家的左邻右舍，则由宏 `LEFT` 和 `RIGHT` 定义：

- *LEFT* : ( i + 5  - 1 ) % 5
- *RIGHT* : ( i + 1 ) % 5

比如 i 为 2，则 `LEFT` 为 1，`RIGHT` 为 3。

具体代码实现如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251000668.webp)

上面的程序使用了一个信号量数组，每个信号量对应一位哲学家，这样在所需的叉子被占用时，想进餐的哲学家就被阻塞。

注意，每个进程/线程将 `smart_person` 函数作为主代码运行，而其他 `take_forks`、`put_forks` 和 `test` 只是普通的函数，而非单独的进程/线程。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251000626.webp)

方案四同样不会出现死锁，也可以两人同时进餐。



##### 4.3.3.2 读者-写者问题

前面的「哲学家进餐问题」对于互斥访问有限的竞争问题（如 I/O 设备）一类的建模过程十分有用。

另外，还有个著名的问题是「读者-写者」，它为数据库访问建立了一个模型。

读者只会读取数据，不会修改数据，而写者即可以读也可以修改数据。

读者-写者的问题描述：

- 「读-读」允许：同一时刻，允许多个读者同时读
- 「读-写」互斥：没有写者时读者才能读，没有读者时写者才能写
- 「写-写」互斥：没有其他写者时，写者才能写

接下来，提出几个解决方案来分析分析。

> 方案一

使用信号量的方式来尝试解决：

- 信号量 `wMutex`：控制写操作的互斥信号量，初始值为 1 ；
- 读者计数 `rCount`：正在进行读操作的读者个数，初始化为 0；
- 信号量 `rCountMutex`：控制对 rCount 读者计数器的互斥修改，初始值为 1；

接下来看看代码的实现：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251000461.webp)

上面的这种实现，是读者优先的策略，因为只要有读者正在读的状态，后来的读者都可以直接进入，如果读者持续不断进入，则写者会处于饥饿状态。

> 方案二

那既然有读者优先策略，自然也有写者优先策略：

- 只要有写者准备要写入，写者应尽快执行写操作，后来的读者就必须阻塞；
- 如果有写者持续不断写入，则读者就处于饥饿；

在方案一的基础上新增如下变量：

- 信号量 `rMutex`：控制读者进入的互斥信号量，初始值为 1；
- 信号量 `wDataMutex`：控制写者写操作的互斥信号量，初始值为 1；
- 写者计数 `wCount`：记录写者数量，初始值为 0；
- 信号量 `wCountMutex`：控制 wCount 互斥修改，初始值为 1；

具体实现如下代码：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251001383.webp)

注意，这里 `rMutex` 的作用，开始有多个读者读数据，它们全部进入读者队列，此时来了一个写者，执行了 `P(rMutex)` 之后，后续的读者由于阻塞在 `rMutex` 上，都不能再进入读者队列，而写者到来，则可以全部进入写者队列，因此保证了写者优先。

同时，第一个写者执行了 `P(rMutex)` 之后，也不能马上开始写，必须等到所有进入读者队列的读者都执行完读操作，通过 `V(wDataMutex)` 唤醒写者的写操作。

> 方案三

既然读者优先策略和写者优先策略都会造成饥饿的现象，那么我们就来实现一下公平策略。

公平策略：

- 优先级相同；
- 写者、读者互斥访问；
- 只能一个写者访问临界区；
- 可以有多个读者同时访问临街资源；

具体代码实现：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251000456.webp)

看完代码不知你是否有这样的疑问，为什么加了一个信号量 `flag`，就实现了公平竞争？

对比方案一的读者优先策略，可以发现，读者优先中只要后续有读者到达，读者就可以进入读者队列， 而写者必须等待，直到没有读者到达。

没有读者到达会导致读者队列为空，即 `rCount==0`，此时写者才可以进入临界区执行写操作。

而这里 `flag` 的作用就是阻止读者的这种特殊权限（特殊权限是只要读者到达，就可以进入读者队列）。

比如：开始来了一些读者读数据，它们全部进入读者队列，此时来了一个写者，执行 `P(falg)` 操作，使得后续到来的读者都阻塞在 `flag` 上，不能进入读者队列，这会使得读者队列逐渐为空，即 `rCount` 减为 0。

这个写者也不能立马开始写（因为此时读者队列不为空），会阻塞在信号量 `wDataMutex` 上，读者队列中的读者全部读取结束后，最后一个读者进程执行 `V(wDataMutex)`，唤醒刚才的写者，写者则继续开始进行写操作。



### 4.4 死锁

#### 4.4.1 死锁的概念

在多线程编程中，我们为了防止多线程竞争共享资源而导致数据错乱，都会在操作共享资源之前加上互斥锁，只有成功获得到锁的线程，才能操作共享资源，获取不到锁的线程就只能等待，直到锁被释放。

那么，当两个线程为了保护两个不同的共享资源而使用了两个互斥锁，那么这两个互斥锁应用不当的时候，可能会造成**两个线程都在等待对方释放锁**，在没有外力的作用下，这些线程会一直相互等待，就没办法继续运行，这种情况就是发生了**死锁**。

举个例子，小林拿了小美房间的钥匙，而小林在自己的房间里，小美拿了小林房间的钥匙，而小美也在自己的房间里。如果小林要从自己的房间里出去，必须拿到小美手中的钥匙，但是小美要出去，又必须拿到小林手中的钥匙，这就形成了死锁。

死锁只有**同时满足**以下四个条件才会发生：

- 互斥条件；
- 持有并等待条件；
- 不可剥夺条件；
- 环路等待条件；

##### 互斥条件

互斥条件是指**多个线程不能同时使用同一个资源**。

比如下图，如果线程 A 已经持有的资源，不能再同时被线程 B 持有，如果线程 B 请求获取线程 A 已经占用的资源，那线程 B 只能等待，直到线程 A 释放了资源。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251002225.webp)

##### 持有并等待条件

持有并等待条件是指，当线程 A 已经持有了资源 1，又想申请资源 2，而资源 2 已经被线程 C 持有了，所以线程  A 就会处于等待状态，但是**线程  A 在等待资源 2 的同时并不会释放自己已经持有的资源 1**。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251002222.webp)

##### 不可剥夺条件

不可剥夺条件是指，当线程已经持有了资源 ，**在自己使用完之前不能被其他线程获取**，线程 B 如果也想使用此资源，则只能在线程 A 使用完并释放后才能获取。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251002792.webp)

##### 环路等待条件

环路等待条件指都是，在死锁发生的时候，**两个线程获取资源的顺序构成了环形链**。

比如，线程 A 已经持有资源 2，而想请求资源 1， 线程 B 已经获取了资源 1，而想请求资源 2，这就形成资源请求等待的环形图。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251002712.webp)



#### 4.4.2 模拟死锁问题的产生

Talk is cheap. Show me the code.

下面，我们用代码来模拟死锁问题的产生。

首先，我们先创建 2 个线程，分别为线程 A 和 线程 B，然后有两个互斥锁，分别是 mutex_A 和 mutex_B，代码如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251003463.webp)

接下来，我们看下线程 A 函数做了什么。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251003623.webp)

可以看到，线程 A 函数的过程：

- 先获取互斥锁 A，然后睡眠 1 秒；
- 再获取互斥锁 B，然后释放互斥锁 B；
- 最后释放互斥锁 A；



然后，我们看看线程 B。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251003298.webp)

可以看到，线程 B 函数的过程：

- 先获取互斥锁 B，然后睡眠 1 秒；
- 再获取互斥锁 A，然后释放互斥锁 A；
- 最后释放互斥锁 B；

然后，我们运行这个程序，运行结果如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251003948.webp)

可以看到线程 B 在等待互斥锁 A 的释放，线程 A 在等待互斥锁 B 的释放，双方都在等待对方资源的释放，很明显，产生了死锁问题。

#### 4.4.3 利用工具排查死锁问题



如果你想排查你的 Java 程序是否死锁，则可以使用 `jstack` 工具，它是 jdk 自带的线程堆栈分析工具。

由于小林的死锁代码例子是 C 写的，在 Linux 下，我们可以使用 `pstack` + `gdb` 工具来定位死锁问题。

pstack 命令可以显示每个线程的栈跟踪信息（函数调用过程），它的使用方式也很简单，只需要 `pstack <pid>` 就可以了。

那么，在定位死锁问题时，我们可以多次执行 pstack 命令查看线程的函数调用过程，多次对比结果，确认哪几个线程一直没有变化，且是因为在等待锁，那么大概率是由于死锁问题导致的。

我用 pstack 输出了我前面模拟死锁问题的进程的所有线程的情况，我多次执行命令后，其结果都一样，如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251003526.webp)

可以看到，Thread 2 和 Thread 3 一直阻塞获取锁（*pthread_mutex_lock*）的过程，而且 pstack 多次输出信息都没有变化，那么可能大概率发生了死锁。

但是，还不能够确认这两个线程是在互相等待对方的锁的释放，因为我们看不到它们是等在哪个锁对象，于是我们可以使用 gdb 工具进一步确认。

整个 gdb 调试过程，如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251003556.webp)

我来解释下，上面的调试过程：

1. 通过 `info thread` 打印了所有的线程信息，可以看到有 3 个线程，一个是主线程（LWP 87746），另外两个都是我们自己创建的线程（LWP 87747 和 87748）；
2. 通过 `thread 2`，将切换到第 2 个线程（LWP 87748）；
3. 通过 `bt`，打印线程的调用栈信息，可以看到有 threadB_proc 函数，说明这个是线程 B 函数，也就说 LWP 87748 是线程 B;
4. 通过 `frame 3`，打印调用栈中的第三个帧的信息，可以看到线程 B 函数，在获取互斥锁 A 的时候阻塞了；
5. 通过 `p mutex_A`，打印互斥锁 A 对象信息，可以看到它被 LWP 为 87747（线程 A） 的线程持有着；
6. 通过 `p mutex_B`，打印互斥锁 A 对象信息，可以看到他被 LWP 为 87748 （线程 B） 的线程持有着；

因为线程 B 在等待线程 A 所持有的 mutex_A, 而同时线程 A 又在等待线程 B 所拥有的mutex_B, 所以可以断定该程序发生了死锁。



#### 4.4.4 避免死锁问题的发生

前面我们提到，产生死锁的四个必要条件是：互斥条件、持有并等待条件、不可剥夺条件、环路等待条件。

那么避免死锁问题就只需要破环其中一个条件就可以，最常见的并且可行的就是**使用资源有序分配法，来破环环路等待条件**。

那什么是资源有序分配法呢？

线程 A 和 线程 B 获取资源的顺序要一样，当线程 A 是先尝试获取资源 A，然后尝试获取资源  B 的时候，线程 B 同样也是先尝试获取资源 A，然后尝试获取资源 B。也就是说，线程 A 和 线程 B 总是以相同的顺序申请自己想要的资源。

我们使用资源有序分配法的方式来修改前面发生死锁的代码，我们可以不改动线程 A 的代码。

我们先要清楚线程 A 获取资源的顺序，它是先获取互斥锁 A，然后获取互斥锁 B。

所以我们只需将线程 B 改成以相同顺序的获取资源，就可以打破死锁了。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251005082.webp)

线程 B 函数改进后的代码如下：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251005460.webp)

执行结果如下，可以看，没有发生死锁。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251005205.webp)





#### 4.4.5 总结

简单来说，死锁问题的产生是由两个或者以上线程并行执行的时候，争夺资源而互相等待造成的。

死锁只有同时满足互斥、持有并等待、不可剥夺、环路等待这四个条件的时候才会发生。

所以要避免死锁问题，就是要破坏其中一个条件即可，最常用的方法就是使用资源有序分配法来破坏环路等待条件。

### 4.5 锁

多线程访问共享资源的时候，避免不了资源竞争而导致数据错乱的问题，所以我们通常为了解决这一问题，都会在访问共享资源之前加锁。

最常用的就是互斥锁，当然还有很多种不同的锁，比如自旋锁、读写锁、乐观锁等，不同种类的锁自然适用于不同的场景。

如果选择了错误的锁，那么在一些高并发的场景下，可能会降低系统的性能，这样用户体验就会非常差了。

所以，为了选择合适的锁，我们不仅需要清楚知道加锁的成本开销有多大，还需要分析业务场景中访问的共享资源的方式，再来还要考虑并发访问共享资源时的冲突概率。

对症下药，才能减少锁对高并发性能的影响。

那接下来，针对不同的应用场景，谈一谈「**互斥锁、自旋锁、读写锁、乐观锁、悲观锁**」的选择和使用。

#### 4.5.1 互斥锁与自旋锁：谁更轻松自如？

最底层的两种就是会「互斥锁和自旋锁」，有很多高级的锁都是基于它们实现的，你可以认为它们是各种锁的地基，所以我们必须清楚它俩之间的区别和应用。

加锁的目的就是保证共享资源在任意时间里，只有一个线程访问，这样就可以避免多线程导致共享数据错乱的问题。

当已经有一个线程加锁后，其他线程加锁则就会失败，互斥锁和自旋锁对于加锁失败后的处理方式是不一样的：

- **互斥锁**加锁失败后，线程会**释放 CPU** ，给其他线程；
- **自旋锁**加锁失败后，线程会**忙等待**，直到它拿到锁；

互斥锁是一种「独占锁」，比如当线程 A 加锁成功后，此时互斥锁已经被线程 A 独占了，只要线程 A 没有释放手中的锁，线程 B 加锁就会失败，于是就会释放 CPU 让给其他线程，**既然线程 B 释放掉了 CPU，自然线程 B 加锁的代码就会被阻塞**。

**对于互斥锁加锁失败而阻塞的现象，是由操作系统内核实现的**。当加锁失败时，内核会将线程置为「睡眠」状态，等到锁被释放后，内核会在合适的时机唤醒线程，当这个线程成功获取到锁后，于是就可以继续执行。如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251011443.webp)

所以，互斥锁加锁失败时，会从用户态陷入到内核态，让内核帮我们切换线程，虽然简化了使用锁的难度，但是存在一定的性能开销成本。

那这个开销成本是什么呢？会有**两次线程上下文切换的成本**：

- 当线程加锁失败时，内核会把线程的状态从「运行」状态设置为「睡眠」状态，然后把 CPU 切换给其他线程运行；
- 接着，当锁被释放时，之前「睡眠」状态的线程会变为「就绪」状态，然后内核会在合适的时间，把 CPU 切换给该线程运行。

线程的上下文切换的是什么？当两个线程是属于同一个进程，**因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。**

上下切换的耗时有大佬统计过，大概在几十纳秒到几微秒之间，如果你锁住的代码执行时间比较短，那可能上下文切换的时间都比你锁住的代码执行时间还要长。

所以，**如果你能确定被锁住的代码执行时间很短，就不应该用互斥锁，而应该选用自旋锁，否则使用互斥锁。**

自旋锁是通过 CPU 提供的 `CAS` 函数（*Compare And Swap*），在「用户态」完成加锁和解锁操作，不会主动产生线程上下文切换，所以相比互斥锁来说，会快一些，开销也小一些。

一般加锁的过程，包含两个步骤：

- 第一步，查看锁的状态，如果锁是空闲的，则执行第二步；
- 第二步，将锁设置为当前线程持有；

CAS 函数就把这两个步骤合并成一条硬件级指令，形成**原子指令**，这样就保证了这两个步骤是不可分割的，要么一次性执行完两个步骤，要么两个步骤都不执行。

使用自旋锁的时候，当发生多线程竞争锁的情况，加锁失败的线程会「忙等待」，直到它拿到锁。这里的「忙等待」可以用 `while` 循环等待实现，不过最好是使用 CPU 提供的 `PAUSE` 指令来实现「忙等待」，因为可以减少循环等待时的耗电量。

自旋锁是最比较简单的一种锁，一直自旋，利用 CPU 周期，直到锁可用。**需要注意，在单核 CPU 上，需要抢占式的调度器（即不断通过时钟中断一个线程，运行其他线程）。否则，自旋锁在单 CPU 上无法使用，因为一个自旋的线程永远不会放弃 CPU。**

自旋锁开销少，在多核系统下一般不会主动产生线程切换，适合异步、协程等在用户态切换请求的编程方式，但如果被锁住的代码执行时间过长，自旋的线程会长时间占用 CPU 资源，所以自旋的时间和被锁住的代码执行的时间是成「正比」的关系，我们需要清楚的知道这一点。

自旋锁与互斥锁使用层面比较相似，但实现层面上完全不同：**当加锁失败时，互斥锁用「线程切换」来应对，自旋锁则用「忙等待」来应对**。

它俩是锁的最基本处理方式，更高级的锁都会选择其中一个来实现，比如读写锁既可以选择互斥锁实现，也可以基于自旋锁实现。



#### 4.5.2 读写锁：读和写还有优先级区分？

读写锁从字面意思我们也可以知道，它由「读锁」和「写锁」两部分构成，如果只读取共享资源用「读锁」加锁，如果要修改共享资源则用「写锁」加锁。

所以，**读写锁适用于能明确区分读操作和写操作的场景**。

读写锁的工作原理是：

- 当「写锁」没有被线程持有时，多个线程能够并发地持有读锁，这大大提高了共享资源的访问效率，因为「读锁」是用于读取共享资源的场景，所以多个线程同时持有读锁也不会破坏共享资源的数据。
- 但是，一旦「写锁」被线程持有后，读线程的获取读锁的操作会被阻塞，而且其他写线程的获取写锁的操作也会被阻塞。

所以说，写锁是独占锁，因为任何时刻只能有一个线程持有写锁，类似互斥锁和自旋锁，而读锁是共享锁，因为读锁可以被多个线程同时持有。

知道了读写锁的工作原理后，我们可以发现，**读写锁在读多写少的场景，能发挥出优势**。

另外，根据实现的不同，读写锁可以分为「读优先锁」和「写优先锁」。

读优先锁期望的是，读锁能被更多的线程持有，以便提高读线程的并发性，它的工作方式是：当读线程 A 先持有了读锁，写线程 B 在获取写锁的时候，会被阻塞，并且在阻塞过程中，后续来的读线程 C 仍然可以成功获取读锁，最后直到读线程 A 和 C 释放读锁后，写线程 B 才可以成功获取读锁。如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251026495.webp)

而写优先锁是优先服务写线程，其工作方式是：当读线程 A 先持有了读锁，写线程 B 在获取写锁的时候，会被阻塞，并且在阻塞过程中，后续来的读线程 C 获取读锁时会失败，于是读线程 C 将被阻塞在获取读锁的操作，这样只要读线程 A 释放读锁后，写线程 B 就可以成功获取读锁。如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251026905.webp)

读优先锁对于读线程并发性更好，但也不是没有问题。我们试想一下，如果一直有读线程获取读锁，那么写线程将永远获取不到写锁，这就造成了写线程「饥饿」的现象。

写优先锁可以保证写线程不会饿死，但是如果一直有写线程获取写锁，读线程也会被「饿死」。

既然不管优先读锁还是写锁，对方可能会出现饿死问题，那么我们就不偏袒任何一方，搞个「公平读写锁」。

**公平读写锁比较简单的一种方式是：用队列把获取锁的线程排队，不管是写线程还是读线程都按照先进先出的原则加锁即可，这样读线程仍然可以并发，也不会出现「饥饿」的现象。**

互斥锁和自旋锁都是最基本的锁，读写锁可以根据场景来选择这两种锁其中的一个进行实现。

#### 4.5.3 乐观锁与悲观锁：做事心态有何不同？

前面提到的互斥锁、自旋锁、读写锁，都是属于悲观锁。

悲观锁做事比较悲观，它认为**多线程同时修改共享资源的概率比较高，于是很容易出现冲突，所以访问共享资源前，先要上锁**。

那相反的，如果多线程同时修改共享资源的概率比较低，就可以采用乐观锁。

乐观锁做事比较乐观，它假定冲突的概率很低，它的工作方式是：**先修改完共享资源，再验证这段时间内有没有发生冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作**。

放弃后如何重试，这跟业务场景息息相关，虽然重试的成本很高，但是冲突的概率足够低的话，还是可以接受的。

可见，乐观锁的心态是，不管三七二十一，先改了资源再说。另外，你会发现**乐观锁全程并没有加锁，所以它也叫无锁编程**。

这里举一个场景例子：在线文档。

我们都知道在线文档可以同时多人编辑的，如果使用了悲观锁，那么只要有一个用户正在编辑文档，此时其他用户就无法打开相同的文档了，这用户体验当然不好了。

那实现多人同时编辑，实际上是用了乐观锁，它允许多个用户打开同一个文档进行编辑，编辑完提交之后才验证修改的内容是否有冲突。

怎么样才算发生冲突？这里举个例子，比如用户 A 先在浏览器编辑文档，之后用户 B 在浏览器也打开了相同的文档进行编辑，但是用户 B 比用户 A 提交改动，这一过程用户 A 是不知道的，当 A 提交修改完的内容时，那么 A 和 B 之间并行修改的地方就会发生冲突。

服务端要怎么验证是否冲突了呢？通常方案如下：

- 由于发生冲突的概率比较低，所以先让用户编辑文档，但是浏览器在下载文档时会记录下服务端返回的文档版本号；
- 当用户提交修改时，发给服务端的请求会带上原始文档版本号，服务器收到后将它与当前版本号进行比较，如果版本号一致则修改成功，否则提交失败。

实际上，我们常见的 SVN 和 Git 也是用了乐观锁的思想，先让用户编辑代码，然后提交的时候，通过版本号来判断是否产生了冲突，发生了冲突的地方，需要我们自己修改后，再重新提交。

乐观锁虽然去除了加锁解锁的操作，但是一旦发生冲突，重试的成本非常高，所以**只有在冲突概率非常低，且加锁成本非常高的场景时，才考虑使用乐观锁。**

#### 总结

开发过程中，最常见的就是互斥锁的了，互斥锁加锁失败时，会用「线程切换」来应对，当加锁失败的线程再次加锁成功后的这一过程，会有两次线程上下文切换的成本，性能损耗比较大。

如果我们明确知道被锁住的代码的执行时间很短，那我们应该选择开销比较小的自旋锁，因为自旋锁加锁失败时，并不会主动产生线程切换，而是一直忙等待，直到获取到锁，那么如果被锁住的代码执行时间很短，那这个忙等待的时间相对应也很短。

如果能区分读操作和写操作的场景，那读写锁就更合适了，它允许多个读线程可以同时持有读锁，提高了读的并发性。根据偏袒读方还是写方，可以分为读优先锁和写优先锁，读优先锁并发性很强，但是写线程会被饿死，而写优先锁会优先服务写线程，读线程也可能会被饿死，那为了避免饥饿的问题，于是就有了公平读写锁，它是用队列把请求锁的线程排队，并保证先入先出的原则来对线程加锁，这样便保证了某种线程不会被饿死，通用性也更好点。

互斥锁和自旋锁都是最基本的锁，读写锁可以根据场景来选择这两种锁其中的一个进行实现。

另外，互斥锁、自旋锁、读写锁都属于悲观锁，悲观锁认为并发访问共享资源时，冲突概率可能非常高，所以在访问共享资源前，都需要先加锁。

相反的，如果并发访问共享资源时，冲突概率非常低的话，就可以使用乐观锁，它的工作方式是，在访问共享资源时，不用先加锁，修改完共享资源后，再验证这段时间内有没有发生冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作。

但是，一旦冲突概率上升，就不适合使用乐观锁了，因为它解决冲突的重试成本非常高。

不管使用的哪种锁，我们的加锁的代码范围应该尽可能的小，也就是加锁的粒度要小，这样执行速度会比较快。再来，使用上了合适的锁，就会快上加快了。



## 五、调度算法

### 5.1 进程调度算法

进程调度算法也称 CPU 调度算法，毕竟进程是由 CPU 调度的。

当 CPU 空闲时，操作系统就选择内存中的某个「就绪状态」的进程，并给其分配 CPU。

什么时候会发生 CPU 调度呢？通常有以下情况：

1. 当进程从运行状态转到等待状态；
2. 当进程从运行状态转到就绪状态；
3. 当进程从等待状态转到就绪状态；
4. 当进程从运行状态转到终止状态；

其中发生在 1 和 4 两种情况下的调度称为「非抢占式调度」，2 和 3 两种情况下发生的调度称为「抢占式调度」。

非抢占式的意思就是，当进程正在运行时，它就会一直运行，直到该进程完成或发生某个事件而被阻塞时，才会把 CPU 让给其他进程。

而抢占式调度，顾名思义就是进程正在运行的时，可以被打断，使其把 CPU 让给其他进程。那抢占的原则一般有三种，分别是时间片原则、优先权原则、短作业优先原则。

你可能会好奇为什么第 3 种情况也会发生 CPU 调度呢？假设有一个进程是处于等待状态的，但是它的优先级比较高，如果该进程等待的事件发生了，它就会转到就绪状态，一旦它转到就绪状态，如果我们的调度算法是以优先级来进行调度的，那么它就会立马抢占正在运行的进程，所以这个时候就会发生 CPU 调度。

那第 2 种状态通常是时间片到的情况，因为时间片到了就会发生中断，于是就会抢占正在运行的进程，从而占用 CPU。

调度算法影响的是等待时间（进程在就绪队列中等待调度的时间总和），而不能影响进程真在使用 CPU 的时间和 I/O 时间。

接下来，说说常见的调度算法：

- 先来先服务调度算法
- 最短作业优先调度算法
- 高响应比优先调度算法
- 时间片轮转调度算法
- 最高优先级调度算法
- 多级反馈队列调度算法

#### 1. 先来先服务调度算法

最简单的一个调度算法，就是非抢占式的**先来先服务（\*First Come First Severd, FCFS\*）算法**了。

![FCFS算法](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112241926588.webp)

顾名思义，先来后到，**每次从就绪队列选择最先进入队列的进程，然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。**

这似乎很公平，但是当一个长作业先运行了，那么后面的短作业等待的时间就会很长，不利于短作业。

FCFS 对长作业有利，适用于 CPU 繁忙型作业的系统，而不适用于 I/O 繁忙型作业的系统。

#### 2. 最短作业优先调度算法

**最短作业优先（\*Shortest Job First, SJF\*）调度算法**同样也是顾名思义，它会**优先选择运行时间最短的进程来运行**，这有助于提高系统的吞吐量。

![SJF算法](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112241929321.webp)

这显然对长作业不利，很容易造成一种极端现象。

比如，一个长作业在就绪队列等待运行，而这个就绪队列有非常多的短作业，那么就会使得长作业不断的往后推，周转时间变长，致使长作业长期不会被运行。

#### 3. 高响应比优先算法

前面的「先来先服务调度算法」和「最短作业优先调度算法」都没有很好的权衡短作业和长作业。

那么，**高响应比优先 （\*Highest Response Ratio Next, HRRN\*）调度算法**主要是权衡了短作业和长作业。

**每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行**，「响应比优先级」的计算公式：

![](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112241930474.webp)

从上面的公式，可以发现：

- 如果两个进程的「等待时间」相同时，「要求的服务时间」越短，「响应比」就越高，这样短作业的进程容易被选中运行；
- 如果两个进程「要求的服务时间」相同时，「等待时间」越长，「响应比」就越高，这就兼顾到了长作业进程，因为进程的响应比可以随时间等待的增加而提高，当其等待时间足够长时，其响应比便可以升到很高，从而获得运行的机会；

#### 4. 时间片轮转算法

最古老、最简单、最公平且使用最广的算法就是**时间片轮转（\*Round Robin, RR\*）调度算法**。

![RR调度算法](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112241931828.webp)

**每个进程被分配一个时间段，称为时间片（\*Quantum\*），即允许该进程在该时间段中运行。**

- 如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配另外一个进程；
- 如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；

另外，时间片的长度就是一个很关键的点：

- 如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率；
- 如果设得太长又可能引起对短作业进程的响应时间变长；

通常时间片设为 `20ms~50ms` 通常是一个比较合理的折中值。

#### 5. 最高优先算法

前面的「时间片轮转算法」做了个假设，即让所有的进程同等重要，也不偏袒谁，大家的运行时间都一样。

但是，对于多用户计算机系统就有不同的看法了，它们希望调度是有优先级的，即希望调度程序能**从就绪队列中选择最高优先级的进程进行运行，这称为最高优先级（\*Highest Priority First，HPF\*）调度算法**。

进程的优先级可以分为，静态优先级或动态优先级：

- 静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化；
- 动态优先级：根据进程的动态变化调整优先级，比如如果进程运行时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升高其优先级，也就是**随着时间的推移增加等待进程的优先级**。

该算法也有两种处理优先级高的方法，非抢占式和抢占式：

- 非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程。
- 抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行。

但是依然有缺点，可能会导致低优先级的进程永远不会运行。

#### 6. 多级反馈队列调度算法

**多级反馈队列（\*Multilevel Feedback Queue\*）调度算法**是「时间片轮转算法」和「最高优先级算法」的综合和发展。

顾名思义：

- 「多级」表示有多个队列，每个队列优先级从高到低，同时优先级越高时间片越短。
- 「反馈」表示如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列；

![多级反馈队列](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112241932170.webp)

来看看，它是如何工作的：

- 设置了多个队列，赋予每个队列不同的优先级，每个**队列优先级从高到低**，同时**优先级越高时间片越短**；
- 新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度，如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾，以此类推，直至完成；
- 当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行；

可以发现，对于短作业可能可以在第一级队列很快被处理完。对于长作业，如果在第一级队列处理不完，可以移入下次队列等待被执行，虽然等待的时间变长了，但是运行时间也会更长了，所以该算法很好的**兼顾了长短作业，同时有较好的响应时间。**



### 5.2 页面置换算法

在了解内存页面置换算法前，我们得先谈一下**缺页异常（缺页中断）**。

当 CPU 访问的页面不在物理内存时，便会产生一个缺页中断，请求操作系统将所缺页调入到物理内存。那它与一般中断的主要区别在于：

- 缺页中断在指令执行「期间」产生和处理中断信号，而一般中断在一条指令执行「完成」后检查和处理中断信号。
- 缺页中断返回到该指令的开始重新执行「该指令」，而一般中断返回回到该指令的「下一个指令」执行。

我们来看一下缺页中断的处理流程，如下图：

![缺页中断的处理流程](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112241956148.webp)

1. 在 CPU 里访问一条 Load M 指令，然后 CPU 会去找 M 所对应的页表项。
2. 如果该页表项的状态位是「有效的」，那 CPU 就可以直接去访问物理内存了，如果状态位是「无效的」，则 CPU 则会发送缺页中断请求。
3. 操作系统收到了缺页中断，则会执行缺页中断处理函数，先会查找该页面在磁盘中的页面的位置。
4. 找到磁盘中对应的页面后，需要把该页面换入到物理内存中，但是在换入前，需要在物理内存中找空闲页，如果找到空闲页，就把页面换入到物理内存中。
5. 页面从磁盘换入到物理内存完成后，则把页表项中的状态位修改为「有效的」。
6. 最后，CPU 重新执行导致缺页异常的指令。

上面所说的过程，第 4 步是能在物理内存找到空闲页的情况，那如果找不到呢？

找不到空闲页的话，就说明此时内存已满了，这时候，就需要「页面置换算法」选择一个物理页，如果该物理页有被修改过（脏页），则把它换出到磁盘，然后把该被置换出去的页表项的状态改成「无效的」，最后把正在访问的页面装入到这个物理页中。

这里提一下，页表项通常有如下图的字段：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112241956843.webp)

那其中：

- *状态位*：用于表示该页是否有效，也就是说是否在物理内存中，供程序访问时参考。
- *访问字段*：用于记录该页在一段时间被访问的次数，供页面置换算法选择出页面时参考。
- *修改位*：表示该页在调入内存后是否有被修改过，由于内存中的每一页都在磁盘上保留一份副本，因此，如果没有修改，在置换该页时就不需要将该页写回到磁盘上，以减少系统的开销；如果已经被修改，则将该页重写到磁盘上，以保证磁盘中所保留的始终是最新的副本。
- *硬盘地址*：用于指出该页在硬盘上的地址，通常是物理块号，供调入该页时使用。

这里我整理了虚拟内存的管理整个流程，你可以从下面这张图看到：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112242006814.webp)

所以，页面置换算法的功能是，**当出现缺页异常，需调入新页面而内存已满时，选择被置换的物理页面**，也就是说选择一个物理页面换出到磁盘，然后把需要访问的页面换入到物理页。

那其算法目标则是，尽可能减少页面的换入换出的次数，常见的页面置换算法有如下几种：

- 最佳页面置换算法（*OPT*）
- 先进先出置换算法（*FIFO*）
- 最近最久未使用的置换算法（*LRU*）
- 时钟页面置换算法（*Lock*）
- 最不常用置换算法（*LFU*）



#### 1. 最佳页面置换算法

最佳页面置换算法基本思路是，**置换在「未来」最长时间不访问的页面**。

所以，该算法实现需要计算内存中每个逻辑页面的「下一次」访问时间，然后比较，选择未来最长时间不访问的页面。

我们举个例子，假设一开始有 3 个空闲的物理页，然后有请求的页面序列，那它的置换过程如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251029682.webp)

在这个请求的页面序列中，缺页共发生了 `7` 次（空闲页换入 3 次 + 最优页面置换 4 次），页面置换共发生了 `4` 次。

这很理想，但是实际系统中无法实现，因为程序访问页面时是动态的，我们是无法预知每个页面在「下一次」访问前的等待时间。

所以，最佳页面置换算法作用是为了衡量你的算法的效率，你的算法效率越接近该算法的效率，那么说明你的算法是高效的。

#### 2. 先进先出置换算法

既然我们无法预知页面在下一次访问前所需的等待时间，那我们可以**选择在内存驻留时间很长的页面进行中置换**，这个就是「先进先出置换」算法的思想。

还是以前面的请求的页面序列作为例子，假设使用先进先出置换算法，则过程如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251029277.webp)

在这个请求的页面序列中，缺页共发生了 `10` 次，页面置换共发生了 `7` 次，跟最佳页面置换算法比较起来，性能明显差了很多。

#### 3. 最近最久未使用的置换算法

最近最久未使用（*LRU*）的置换算法的基本思路是，发生缺页时，**选择最长时间没有被访问的页面进行置换**，也就是说，该算法假设已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用。

这种算法近似最优置换算法，最优置换算法是通过「未来」的使用情况来推测要淘汰的页面，而 LRU 则是通过「历史」的使用情况来推测要淘汰的页面。

还是以前面的请求的页面序列作为例子，假设使用最近最久未使用的置换算法，则过程如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251029047.webp)

在这个请求的页面序列中，缺页共发生了 `9` 次，页面置换共发生了 `6` 次，跟先进先出置换算法比较起来，性能提高了一些。

虽然 LRU 在理论上是可以实现的，但代价很高。为了完全实现 LRU，需要在内存中维护一个所有页面的链表，最近最多使用的页面在表头，最近最少使用的页面在表尾。

困难的是，在每次访问内存时都必须要更新「整个链表」。在链表中找到一个页面，删除它，然后把它移动到表头是一个非常费时的操作。

所以，LRU 虽然看上去不错，但是由于开销比较大，实际应用中比较少使用。

#### 4. 时钟页面置换算法

那有没有一种即能优化置换的次数，也能方便实现的算法呢？

时钟页面置换算法就可以两者兼得，它跟 LRU 近似，又是对 FIFO 的一种改进。

该算法的思路是，把所有的页面都保存在一个类似钟面的「环形链表」中，一个表针指向最老的页面。

当发生缺页中断时，算法首先检查表针指向的页面：

- 如果它的访问位位是 0 就淘汰该页面，并把新的页面插入这个位置，然后把表针前移一个位置；
- 如果访问位是 1 就清除访问位，并把表针前移一个位置，重复这个过程直到找到了一个访问位为 0 的页面为止；

我画了一副时钟页面置换算法的工作流程图，你可以在下方看到：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251030015.webp)

了解了这个算法的工作方式，就明白为什么它被称为时钟（*Clock*）算法了。

#### 5. 最不常用算法

最不常用（*LFU*）算法，这名字听起来很调皮，但是它的意思不是指这个算法不常用，而是**当发生缺页中断时，选择「访问次数」最少的那个页面，并将其淘汰**。

它的实现方式是，对每个页面设置一个「访问计数器」，每当一个页面被访问时，该页面的访问计数器就累加 1。在发生缺页中断时，淘汰计数器值最小的那个页面。

看起来很简单，每个页面加一个计数器就可以实现了，但是在操作系统中实现的时候，我们需要考虑效率和硬件成本的。

要增加一个计数器来实现，这个硬件成本是比较高的，另外如果要对这个计数器查找哪个页面访问次数最小，查找链表本身，如果链表长度很大，是非常耗时的，效率不高。

但还有个问题，LFU 算法只考虑了频率问题，没考虑时间的问题，比如有些页面在过去时间里访问的频率很高，但是现在已经没有访问了，而当前频繁访问的页面由于没有这些页面访问的次数高，在发生缺页中断时，就会可能会误伤当前刚开始频繁访问，但访问次数还不高的页面。

那这个问题的解决的办法还是有的，可以定期减少访问的次数，比如当发生时间中断时，把过去时间访问的页面的访问次数除以 2，也就说，随着时间的流失，以前的高访问次数的页面会慢慢减少，相当于加大了被置换的概率。

### 5.3 磁盘调度算法

我们来看看磁盘的结构，如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251033741.webp)磁盘的结构

常见的机械磁盘是上图左边的样子，中间圆的部分是磁盘的盘片，一般会有多个盘片，每个盘面都有自己的磁头。右边的图就是一个盘片的结构，盘片中的每一层分为多个磁道，每个磁道分多个扇区，每个扇区是 `512` 字节。那么，多个具有相同编号的磁道形成一个圆柱，称之为磁盘的柱面，如上图里中间的样子。

磁盘调度算法的目的很简单，就是为了提高磁盘的访问性能，一般是通过优化磁盘的访问请求顺序来做到的。

寻道的时间是磁盘访问最耗时的部分，如果请求顺序优化的得当，必然可以节省一些不必要的寻道时间，从而提高磁盘的访问性能。

假设有下面一个请求序列，每个数字代表磁道的位置：

98，183，37，122，14，124，65，67

初始磁头当前的位置是在第 `53` 磁道。

接下来，分别对以上的序列，作为每个调度算法的例子，那常见的磁盘调度算法有：

- 先来先服务算法
- 最短寻道时间优先算法
- 扫描算法算法
- 循环扫描算法
- LOOK 与 C-LOOK 算法

#### 1. 先来先服务

先来先服务（*First-Come，First-Served，FCFS*），顾名思义，先到来的请求，先被服务。

那按照这个序列的话：

98，183，37，122，14，124，65，67

那么，磁盘的写入顺序是从左到右，如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251033215.webp)先来先服务

先来先服务算法总共移动了 `640` 个磁道的距离，这么一看这种算法，比较简单粗暴，但是如果大量进程竞争使用磁盘，请求访问的磁道可能会很分散，那先来先服务算法在性能上就会显得很差，因为寻道时间过长。

#### 2. 最短寻道时间优先

最短寻道时间优先（*Shortest Seek First，SSF*）算法的工作方式是，优先选择从当前磁头位置所需寻道时间最短的请求，还是以这个序列为例子：

98，183，37，122，14，124，65，67

那么，那么根据距离磁头（ 53 位置）最近的请求的算法，具体的请求则会是下列从左到右的顺序：

65，67，37，14，98，122，124，183

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251029297.webp)

磁头移动的总距离是 `236` 磁道，相比先来先服务性能提高了不少。

但这个算法可能存在某些请求的**饥饿**，因为本次例子我们是静态的序列，看不出问题，假设是一个动态的请求，如果后续来的请求都是小于 183 磁道的，那么 183 磁道可能永远不会被响应，于是就产生了饥饿现象，这里**产生饥饿的原因是磁头在一小块区域来回移动**。

#### 3. 扫描算法

最短寻道时间优先算法会产生饥饿的原因在于：磁头有可能再一个小区域内来回得移动。

为了防止这个问题，可以规定：**磁头在一个方向上移动，访问所有未完成的请求，直到磁头到达该方向上的最后的磁道，才调换方向，这就是扫描（\*Scan\*）算法**。

这种算法也叫做电梯算法，比如电梯保持按一个方向移动，直到在那个方向上没有请求为止，然后改变方向。

还是以这个序列为例子，磁头的初始位置是 53：

98，183，37，122，14，124，65，67

那么，假设扫描调度算先朝磁道号减少的方向移动，具体请求则会是下列从左到右的顺序：

37，14，`0`，65，67，98，122，124，183

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251033108.webp)

磁头先响应左边的请求，直到到达最左端（ 0 磁道）后，才开始反向移动，响应右边的请求。

扫描调度算法性能较好，不会产生饥饿现象，但是存在这样的问题，中间部分的磁道会比较占便宜，中间部分相比其他部分响应的频率会比较多，也就是说每个磁道的响应频率存在差异。

#### 4. 循环扫描算法

扫描算法使得每个磁道响应的频率存在差异，那么要优化这个问题的话，可以总是按相同的方向进行扫描，使得每个磁道的响应频率基本一致。

循环扫描（*Circular Scan, CSCAN* ）规定：只有磁头朝某个特定方向移动时，才处理磁道访问请求，而返回时直接快速移动至最靠边缘的磁道，也就是复位磁头，这个过程是很快的，并且**返回中途不处理任何请求**，该算法的特点，就是**磁道只响应一个方向上的请求**。

还是以这个序列为例子，磁头的初始位置是 53：

98，183，37，122，14，124，65，67

那么，假设循环扫描调度算先朝磁道增加的方向移动，具体请求会是下列从左到右的顺序：

65，67，98，122，124，183，`199`，`0`，14，37

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251029790.webp)循环扫描算法

磁头先响应了右边的请求，直到碰到了最右端的磁道 199，就立即回到磁盘的开始处（磁道 0），但这个返回的途中是不响应任何请求的，直到到达最开始的磁道后，才继续顺序响应右边的请求。

循环扫描算法相比于扫描算法，对于各个位置磁道响应频率相对比较平均。

#### 5. LOOK 与 C-LOOK算法

我们前面说到的扫描算法和循环扫描算法，都是磁头移动到磁盘「最始端或最末端」才开始调换方向。

那这其实是可以优化的，优化的思路就是**磁头在移动到「最远的请求」位置，然后立即反向移动。**

那针对 SCAN 算法的优化则叫 LOOK 算法，它的工作方式，磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，**反向移动的途中会响应请求**。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251034319.webp)

而针 C-SCAN 算法的优化则叫 C-LOOK，它的工作方式，磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，**反向移动的途中不会响应请求**。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251034063.webp)





## 六、文件系统



### 6.1 文件系统的组成

​	⽂件系统是操作系统中负责管理持久数据的⼦系统，说简单点，就是负责把⽤户的⽂件存到磁盘硬件中， 因为即使计算机断电了，磁盘⾥的数据并不会丢失，所以可以持久化的保存⽂件。⽂件系统的基本数据单位是⽂件，它的⽬的是对磁盘上的⽂件进⾏组织管理，那组织的⽅式不同，就会形 成不同的⽂件系统。

​	Linux 最经典的⼀句话是：「**⼀切皆⽂件**」，不仅普通的⽂件和⽬录，就连块设备、管道、socket 等，也 都是统⼀交给⽂件系统管理的。Linux ⽂件系统会为每个⽂件分配两个数据结构：**索引节点（index node）**和**⽬录项（directory entry）**，它们主要⽤来记录⽂件的元信息和⽬录层次结构。

- **索引节点**：也就是 inode，⽤来记录⽂件的元信息，⽐如 inode 编号、⽂件⼤⼩、访问权限、创建时 间、修改时间、数据在磁盘的位置等等。索引节点是⽂件的唯⼀标识，它们之间⼀⼀对应，也同样都会被存储在硬盘中，所以索引节点同样占⽤磁盘空间。
- **⽬录项**：也就是 dentry，⽤来记录⽂件的名字、索引节点指针以及与其他⽬录项的层级关联关系。多 个⽬录项关联起来，就会形成⽬录结构，但它与索引节点不同的是，⽬录项是由内核维护的⼀个数据 结构，不存放于磁盘，⽽是缓存在内存。

​	由于索引节点唯⼀标识⼀个⽂件，⽽⽬录项记录着⽂件的名，所以⽬录项和索引节点的关系是多对⼀，也 就是说，⼀个⽂件可以有多个别字。⽐如，硬链接的实现就是多个⽬录项中的索引节点指向同⼀个⽂件。注意，⽬录也是⽂件，也是⽤索引节点唯⼀标识，和普通⽂件不同的是，普通⽂件在磁盘⾥⾯保存的是⽂件数据，⽽⽬录⽂件在磁盘⾥⾯保存⼦⽬录或⽂件。

> 目录项和目录是一个东西吗？

​	虽然名字很相近，但是它们不是⼀个东⻄，⽬录是个⽂件，持久化存储在磁盘，⽽⽬录项是内核⼀个数据 结构，缓存在内存。如果查询⽬录频繁从磁盘读，效率会很低，所以内核会把已经读过的⽬录⽤⽬录项这个数据结构缓存在内 存，下次再次读到相同的⽬录时，只需从内存读就可以，⼤⼤提⾼了⽂件系统的效率。注意，⽬录项这个数据结构不只是表示⽬录，也是可以表示⽂件的。

> 那文件数据是如何存储在磁盘的呢？

​	磁盘读写的最小单位是扇区，扇区的大小只有512B大小，很明显，如果每次读写都以这么小的单位，那这读写的效率会非常低。所以，文件系统把多个扇区组成一个逻辑块，每次读写的最小单位就是逻辑块（数据块），Linux中的逻辑块大小`4KB`，也就是一次性读写8个扇区，这将大大提高了磁盘的读写的效率。

![image-20211210161113136](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101611234.png)

​	索引节点是存储在硬盘上的数据，那么为了加速⽂件的访问，通常会把索引节点加载到内存中。另外，磁盘进⾏格式化的时候，会被分成三个存储区域，分别是超级块、索引节点区和数据块区。

- **超级块**：用来存储文件系统的详细信息，比如块个数、块大小、空闲块等等。
- **索引节点区**：用来存储索引节点；
- **数据块区**：用来存储文件或目录数据

​	我们不可能把超级块和索引节点区全部加载到内存，这样内存肯定撑不住，所以只有当需要使⽤的时候， 才将其加载进内存，它们加载进内存的时机是不同的：

- **超级块**： 当文件系统挂载时进入内存
- **索引节点区**： 当文件被访问时进入内存



### 6.2 虚拟文件系统

​	⽂件系统的种类众多，⽽操作系统希望**对⽤户提供⼀个统⼀的接⼝**，于是在⽤户层与⽂件系统层引⼊了中 间层，这个中间层就称为虚拟⽂件系统（`Virtual  File System，VFS`）。VFS 定义了⼀组所有⽂件系统都⽀持的数据结构和标准接⼝，这样程序员不需要了解⽂件系统的⼯作原 理，只需要了解  VFS 提供的统⼀接⼝即可。

> 图：在 Linux ⽂件系统中，⽤户空间、系统调⽤、虚拟机⽂件系统、缓存、⽂件系统以及存储之间的关系 

![image-20211210161917723](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101619826.png)



Linux  ⽀持的⽂件系统也不少，根据存储位置的不同，可以把⽂件系统分为三类：

- **磁盘的文件系统**： 直接把数据存储在磁盘，⽐如  Ext  2/3/4、XFS 等都是这类⽂件系统。
- **内存的文件系统**： 这类⽂件系统的数据不是存储在硬盘的，⽽是占⽤内存空间，我们经常⽤到的`/proc`和`/sys`文件系统都属于这一类，读写这类文件，实际上是读写内核中相关的数据。
- **网络的文件系统**： 用来访问其他计算机主机数据的文件系统，比如NFS、SMB等等。

​	⽂件系统⾸先要先挂载到某个⽬录才可以正常使⽤，⽐如 Linux 系统在启动时，会把⽂件系统挂载到根⽬ 录。

### 6.3 文件的使用

1. 我们从用户角度来看文件的话，就是我们要怎么使用文件？首先，我们得通过系统调用来打开一个文件。

   ![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251056133.webp)

   ```
   fd = open(name, flag); # 打开文件
   ...
   write(fd,...);         # 写数据
   ...
   close(fd);             # 关闭文件
   ```

   上面简单的代码是读取一个文件的过程：

   - 首先用 `open` 系统调用打开文件，`open` 的参数中包含文件的路径名和文件名。
   - 使用 `write` 写数据，其中 `write` 使用 `open` 所返回的**文件描述符**，并不使用文件名作为参数。
   - 使用完文件后，要用 `close` 系统调用关闭文件，避免资源的泄露。

   我们打开了一个文件后，操作系统会跟踪进程打开的所有文件，所谓的跟踪呢，就是操作系统为每个进程维护一个打开文件表，文件表里的每一项代表「**文件描述符**」，所以说文件描述符是打开文件的标识。

   ![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251056057.webp)打开文件表

   操作系统在打开文件表中维护着打开文件的状态和信息：

   - 文件指针：系统跟踪上次读写位置作为当前文件位置指针，这种指针对打开文件的某个进程来说是唯一的；
   - 文件打开计数器：文件关闭时，操作系统必须重用其打开文件表条目，否则表内空间不够用。因为多个进程可能打开同一个文件，所以系统在删除打开文件条目之前，必须等待最后一个进程关闭文件，该计数器跟踪打开和关闭的数量，当该计数为 0 时，系统关闭文件，删除该条目；
   - 文件磁盘位置：绝大多数文件操作都要求系统修改文件数据，该信息保存在内存中，以免每个操作都从磁盘中读取；
   - 访问权限：每个进程打开文件都需要有一个访问模式（创建、只读、读写、添加等），该信息保存在进程的打开文件表中，以便操作系统能允许或拒绝之后的 I/O 请求；

   在用户视角里，文件就是一个持久化的数据结构，但操作系统并不会关心你想存在磁盘上的任何的数据结构，操作系统的视角是如何把文件数据和磁盘块对应起来。

   所以，用户和操作系统对文件的读写操作是有差异的，用户习惯以字节的方式读写文件，而操作系统则是以数据块来读写文件，那屏蔽掉这种差异的工作就是文件系统了。

   我们来分别看一下，读文件和写文件的过程：

   - 当用户进程从文件读取 1 个字节大小的数据时，文件系统则需要获取字节所在的数据块，再返回数据块对应的用户进程所需的数据部分。
   - 当用户进程把 1 个字节大小的数据写进文件时，文件系统则找到需要写入数据的数据块的位置，然后修改数据块中对应的部分，最后再把数据块写回磁盘。

   所以说，**文件系统的基本操作单位是数据块**。

### 6.4 文件存储

文件的数据是要存储在硬盘上面的，数据在磁盘上的存放方式，就像程序在内存中存放的方式那样，有以下两种：

- 连续空间存放方式
- 非连续空间存放方式

其中，非连续空间存放方式又可以分为「链表方式」和「索引方式」。

不同的存储方式，有各自的特点，重点是要分析它们的存储效率和读写性能，接下来分别对每种存储方式说一下。

#### 6.4.1 连续空间存储方式

连续空间存放方式顾名思义，**文件存放在磁盘「连续的」物理空间中**。这种模式下，文件的数据都是紧密相连，**读写效率很高**，因为一次磁盘寻道就可以读出整个文件。

使用连续存放的方式有一个前提，必须先知道一个文件的大小，这样文件系统才会根据文件的大小在磁盘上找到一块连续的空间分配给文件。

所以，**文件头里需要指定「起始块的位置」和「长度」**，有了这两个信息就可以很好的表示文件存放方式是一块连续的磁盘空间。

注意，此处说的文件头，就类似于 Linux 的 inode。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251054037.webp)

连续空间存放的方式虽然读写效率高，**但是有「磁盘空间碎片」和「文件长度不易扩展」的缺陷。**

如下图，如果文件 B 被删除，磁盘上就留下一块空缺，这时，如果新来的文件小于其中的一个空缺，我们就可以将其放在相应空缺里。但如果该文件的大小大于所有的空缺，但却小于空缺大小之和，则虽然磁盘上有足够的空缺，但该文件还是不能存放。当然了，我们可以通过将现有文件进行挪动来腾出空间以容纳新的文件，但是这个在磁盘挪动文件是非常耗时，所以这种方式不太现实。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251054161.webp)

另外一个缺陷是文件长度扩展不方便，例如上图中的文件 A 要想扩大一下，需要更多的磁盘空间，唯一的办法就只能是挪动的方式，前面也说了，这种方式效率是非常低的。

那么有没有更好的方式来解决上面的问题呢？答案当然有，既然连续空间存放的方式不太行，那么我们就改变存放的方式，使用非连续空间存放方式来解决这些缺陷。



#### 6.4.2 非连续空间存储

非连续空间存放方式分为「链表方式」和「索引方式」。

> 我们先来看看链表的方式。

链表的方式存放是**离散的，不用连续的**，于是就可以**消除磁盘碎片**，可大大提高磁盘空间的利用率，同时**文件的长度可以动态扩展**。根据实现的方式的不同，链表可分为「**隐式链表**」和「**显式链接**」两种形式。

文件要以「**隐式链表**」的方式存放的话，**实现的方式是文件头要包含「第一块」和「最后一块」的位置，并且每个数据块里面留出一个指针空间，用来存放下一个数据块的位置**，这样一个数据块连着一个数据块，从链头开是就可以顺着指针找到所有的数据块，所以存放的方式可以是不连续的。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251055327.webp)隐式链表

隐式链表的存放方式的**缺点在于无法直接访问数据块，只能通过指针顺序访问文件，以及数据块指针消耗了一定的存储空间**。隐式链接分配的**稳定性较差**，系统在运行过程中由于软件或者硬件错误**导致链表中的指针丢失或损坏，会导致文件数据的丢失。**

如果取出每个磁盘块的指针，把它放在内存的一个表中，就可以解决上述隐式链表的两个不足。那么，这种实现方式是「**显式链接**」，它指**把用于链接文件各数据块的指针，显式地存放在内存的一张链接表中**，该表在整个磁盘仅设置一张，**每个表项中存放链接指针，指向下一个数据块号**。

对于显式链接的工作方式，我们举个例子，文件 A 依次使用了磁盘块 4、7、2、10 和 12 ，文件 B 依次使用了磁盘块 6、3、11 和 14 。利用下图中的表，可以从第 4 块开始，顺着链走到最后，找到文件 A 的全部磁盘块。同样，从第 6 块开始，顺着链走到最后，也能够找出文件 B 的全部磁盘块。最后，这两个链都以一个不属于有效磁盘编号的特殊标记（如 -1 ）结束。内存中的这样一个表格称为**文件分配表（\*File Allocation Table，FAT\*）**。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251054962.webp)

由于查找记录的过程是在内存中进行的，因而不仅显著地**提高了检索速度**，而且**大大减少了访问磁盘的次数**。但也正是整个表都存放在内存中的关系，它的主要的缺点是**不适用于大磁盘**。

比如，对于 200GB 的磁盘和 1KB 大小的块，这张表需要有 2 亿项，每一项对应于这 2 亿个磁盘块中的一个块，每项如果需要 4 个字节，那这张表要占用 800MB 内存，很显然 FAT 方案对于大磁盘而言不太合适。

> 接下来，我们来看看索引的方式。

链表的方式解决了连续分配的磁盘碎片和文件动态扩展的问题，但是不能有效支持直接访问（FAT除外），索引的方式可以解决这个问题。

索引的实现是为每个文件创建一个「**索引数据块**」，里面存放的是**指向文件数据块的指针列表**，说白了就像书的目录一样，要找哪个章节的内容，看目录查就可以。

另外，**文件头需要包含指向「索引数据块」的指针**，这样就可以通过文件头知道索引数据块的位置，再通过索引数据块里的索引信息找到对应的数据块。

创建文件时，索引块的所有指针都设为空。当首次写入第 i 块时，先从空闲空间中取得一个块，再将其地址写到索引块的第 i 个条目。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251055857.webp)

索引的方式优点在于：

- 文件的创建、增大、缩小很方便；
- 不会有碎片的问题；
- 支持顺序读写和随机读写；

由于索引数据也是存放在磁盘块的，如果文件很小，明明只需一块就可以存放的下，但还是需要额外分配一块来存放索引数据，所以缺陷之一就是存储索引带来的开销。

如果文件很大，大到一个索引数据块放不下索引信息，这时又要如何处理大文件的存放呢？我们可以通过组合的方式，来处理大文件的存。

先来看看链表 + 索引的组合，这种组合称为「**链式索引块**」，它的实现方式是**在索引数据块留出一个存放下一个索引数据块的指针**，于是当一个索引数据块的索引信息用完了，就可以通过指针的方式，找到下一个索引数据块的信息。那这种方式也会出现前面提到的链表方式的问题，万一某个指针损坏了，后面的数据也就会无法读取了。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251054917.webp)

还有另外一种组合方式是索引 + 索引的方式，这种组合称为「**多级索引块**」，实现方式是**通过一个索引块来存放多个索引数据块**，一层套一层索引，像极了俄罗斯套娃是吧。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251055588.webp)



### 6.5 空闲空间管理

前面说到的文件的存储是针对已经被占用的数据块组织和管理，接下来的问题是，如果我要保存一个数据块，我应该放在硬盘上的哪个位置呢？难道需要将所有的块扫描一遍，找个空的地方随便放吗？

那这种方式效率就太低了，所以针对磁盘的空闲空间也是要引入管理的机制，接下来介绍几种常见的方法：

- 空闲表法
- 空闲链表法
- 位图法

#### 6.5.1 空闲表法

空闲表法就是为所有空闲空间建立一张表，表内容包括空闲区的第一个块号和该空闲区的块个数，注意，这个方式是连续分配的。如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251051900.webp)

当请求分配磁盘空间时，系统依次扫描空闲表里的内容，直到找到一个合适的空闲区域为止。当用户撤销一个文件时，系统回收文件空间。这时，也需顺序扫描空闲表，寻找一个空闲表条目并将释放空间的第一个物理块号及它占用的块数填到这个条目中。

这种方法仅当有少量的空闲区时才有较好的效果。因为，如果存储空间中有着大量的小的空闲区，则空闲表变得很大，这样查询效率会很低。另外，这种分配技术适用于建立连续文件。

#### 6.5.2 空闲链表法

我们也可以使用「链表」的方式来管理空闲空间，每一个空闲块里有一个指针指向下一个空闲块，这样也能很方便的找到空闲块并管理起来。如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251051057.webp)

当创建文件需要一块或几块时，就从链头上依次取下一块或几块。反之，当回收空间时，把这些空闲块依次接到链头上。

这种技术只要在主存中保存一个指针，令它指向第一个空闲块。其特点是简单，但不能随机访问，工作效率低，因为每当在链上增加或移动空闲块时需要做很多 I/O 操作，同时数据块的指针消耗了一定的存储空间。

空闲表法和空闲链表法都不适合用于大型文件系统，因为这会使空闲表或空闲链表太大。

#### 6.5.3 位图法

位图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。

当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。它形式如下：

```
1111110011111110001110110111111100111 ...
```

在 Linux 文件系统就采用了位图的方式来管理空闲空间，不仅用于数据空闲块的管理，还用于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，自然也要有对其管理。



### 6.6 文件系统结构

前面提到 Linux 是用位图的方式管理空闲空间，用户在创建一个新文件时，Linux 内核会通过 inode 的位图找到空闲可用的 inode，并进行分配。要存储数据时，会通过块的位图找到空闲的块，并分配，但仔细计算一下还是有问题的。

数据块的位图是放在磁盘块里的，假设是放在一个块里，一个块 4K，每位表示一个数据块，共可以表示 `4 * 1024 * 8 = 2^15` 个空闲块，由于 1 个数据块是 4K 大小，那么最大可以表示的空间为 `2^15 * 4 * 1024 = 2^27` 个 byte，也就是 128M。

也就是说按照上面的结构，如果采用「一个块的位图 + 一系列的块」，外加「一个块的 inode 的位图 + 一系列的 inode 的结构」能表示的最大空间也就 128M，这太少了，现在很多文件都比这个大。

在 Linux 文件系统，把这个结构称为一个**块组**，那么有 N 多的块组，就能够表示 N 大的文件。

下图给出了 Linux Ext2 整个文件系统的结构和块组的内容，文件系统都由大量块组组成，在硬盘上相继排布：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251050204.webp)

最前面的第一个块是引导块，在系统启动时用于启用引导，接着后面就是一个一个连续的块组了，块组的内容如下：

- *超级块*，包含的是文件系统的重要信息，比如 inode 总个数、块总个数、每个块组的 inode 个数、每个块组的块个数等等。
- *块组描述符*，包含文件系统中各个块组的状态，比如块组中空闲块和 inode 的数目等，每个块组都包含了文件系统中「所有块组的组描述符信息」。
- *数据位图和 inode 位图*， 用于表示对应的数据块或 inode 是空闲的，还是被使用中。
- *inode 列表*，包含了块组中所有的 inode，inode 用于保存文件系统中与各个文件和目录相关的所有元数据。
- *数据块*，包含文件的有用数据。

你可以会发现每个块组里有很多重复的信息，比如**超级块和块组描述符表，这两个都是全局信息，而且非常的重要**，这么做是有两个原因：

- 如果系统崩溃破坏了超级块或块组描述符，有关文件系统结构和内容的所有信息都会丢失。如果有冗余的副本，该信息是可能恢复的。
- 通过使文件和管理数据尽可能接近，减少了磁头寻道和旋转，这可以提高文件系统的性能。

不过，Ext2 的后续版本采用了稀疏技术。该做法是，超级块和块组描述符表不再存储到文件系统的每个块组中，而是只写入到块组 0、块组 1 和其他 ID 可以表示为 3、 5、7 的幂的块组中。

### 6.7 目录的存储

在前面，我们知道了一个普通文件是如何存储的，但还有一个特殊的文件，经常用到的目录，它是如何保存的呢？

基于 Linux 一切皆文件的设计思想，目录其实也是个文件，你甚至可以通过 `vim` 打开它，它也有 inode，inode 里面也是指向一些块。

和普通文件不同的是，**普通文件的块里面保存的是文件数据，而目录文件的块里面保存的是目录里面一项一项的文件信息。**

在目录文件的块中，最简单的保存格式就是**列表**，就是一项一项地将目录下的文件信息（如文件名、文件 inode、文件类型等）列在表里。

列表中每一项就代表该目录下的文件的文件名和对应的 inode，通过这个 inode，就可以找到真正的文件。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251049299.webp)

通常，第一项是「`.`」，表示当前目录，第二项是「`..`」，表示上一级目录，接下来就是一项一项的文件名和 inode。

如果一个目录有超级多的文件，我们要想在这个目录下找文件，按照列表一项一项的找，效率就不高了。

于是，保存目录的格式改成**哈希表**，对文件名进行哈希计算，把哈希值保存起来，如果我们要查找一个目录下面的文件名，可以通过名称取哈希。如果哈希能够匹配上，就说明这个文件的信息在相应的块里面。

Linux 系统的 ext 文件系统就是采用了哈希表，来保存目录的内容，这种方法的优点是查找非常迅速，插入和删除也较简单，不过需要一些预备措施来避免哈希冲突。

目录查询是通过在磁盘上反复搜索完成，需要不断地进行 I/O 操作，开销较大。所以，为了减少 I/O 操作，把当前使用的文件目录缓存在内存，以后要使用该文件时只要在内存中操作，从而降低了磁盘操作次数，提高了文件系统的访问速度。



### 6.8 软链接和硬链接

有时候我们希望给某个文件取个别名，那么在 Linux 中可以通过**硬链接（\*Hard Link\*）** 和**软链接（\*Symbolic Link\*）** 的方式来实现，它们都是比较特殊的文件，但是实现方式也是不相同的。

硬链接是**多个目录项中的「索引节点」指向一个文件**，也就是指向同一个 inode，但是 inode 是不可能跨越文件系统的，每个文件系统都有各自的 inode 数据结构和列表，所以**硬链接是不可用于跨文件系统的**。由于多个目录项都是指向一个 inode，那么**只有删除文件的所有硬链接以及源文件时，系统才会彻底删除该文件。**

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251048206.webp)

软链接相当于重新创建一个文件，这个文件有**独立的 inode**，但是这个**文件的内容是另外一个文件的路径**，所以访问软链接的时候，实际上相当于访问到了另外一个文件，所以**软链接是可以跨文件系统的**，甚至**目标文件被删除了，链接文件还是在的，只不过指向的文件找不到了而已。**

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251048564.webp)

### 6.9 文件I/O

文件的读写方式各有千秋，对于文件的 I/O 分类也非常多，常见的有

- 缓冲与非缓冲 I/O
- 直接与非直接 I/O
- 阻塞与非阻塞 I/O VS 同步与异步 I/O

接下来，分别对这些分类讨论讨论。

#### 6.9.1 缓冲与非缓冲I/O

文件操作的标准库是可以实现数据的缓存，那么**根据「是否利用标准库缓冲」，可以把文件 I/O 分为缓冲 I/O 和非缓冲 I/O**：

- 缓冲 I/O，利用的是标准库的缓存实现文件的加速访问，而标准库再通过系统调用访问文件。
- 非缓冲 I/O，直接通过系统调用访问文件，不经过标准库缓存。

这里所说的「缓冲」特指标准库内部实现的缓冲。

比方说，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来，这样做的目的是，减少系统调用的次数，毕竟系统调用是有 CPU 上下文切换的开销的。

#### 6.9.2 直接与非直接I/O

我们都知道磁盘 I/O 是非常慢的，所以 Linux 内核为了减少磁盘 I/O 次数，在系统调用后，会把用户数据拷贝到内核中缓存起来，这个内核缓存空间也就是「页缓存」，只有当缓存满足某些条件的时候，才发起磁盘 I/O 的请求。

那么，**根据是「否利用操作系统的缓存」，可以把文件 I/O 分为直接 I/O 与非直接 I/O**：

- 直接 I/O，不会发生内核缓存和用户程序之间数据复制，而是直接经过文件系统访问磁盘。
- 非直接 I/O，读操作时，数据从内核缓存中拷贝给用户程序，写操作时，数据从用户程序拷贝给内核缓存，再由内核决定什么时候写入数据到磁盘。

如果你在使用文件操作类的系统调用函数时，指定了 `O_DIRECT` 标志，则表示使用直接 I/O。如果没有设置过，默认使用的是非直接 I/O。

> 如果用了非直接 I/O 进行写数据操作，内核什么情况下才会把缓存数据写入到磁盘？

以下几种场景会触发内核缓存的数据写入磁盘：

- 在调用 `write` 的最后，当发现内核缓存的数据太多的时候，内核会把数据写到磁盘上；
- 用户主动调用 `sync`，内核缓存会刷到磁盘上；
- 当内存十分紧张，无法再分配页面时，也会把内核缓存的数据刷到磁盘上；
- 内核缓存的数据的缓存时间超过某个时间时，也会把数据刷到磁盘上；

#### 6.9.3 阻塞与非阻塞IO VS 异步与同步I/O

为什么把阻塞 / 非阻塞与同步与异步放一起说的呢？因为它们确实非常相似，也非常容易混淆，不过它们之间的关系还是有点微妙的。

先来看看**阻塞 I/O**，当用户程序执行 `read` ，线程会被阻塞，一直等到内核数据准备好，并把数据从内核缓冲区拷贝到应用程序的缓冲区中，当拷贝过程完成，`read` 才会返回。

注意，**阻塞等待的是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程**。过程如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251047804.webp)阻塞 I/O

知道了阻塞 I/O ，来看看**非阻塞 I/O**，非阻塞的 read 请求在数据未准备好的情况下立即返回，可以继续往下执行，此时应用程序不断轮询内核，直到数据准备好，内核将数据拷贝到应用程序缓冲区，`read` 调用才可以获取到结果。过程如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251047713.webp)

注意，**这里最后一次 read 调用，获取数据的过程，是一个同步的过程，是需要等待的过程。这里的同步指的是内核态的数据拷贝到用户程序的缓存区这个过程。**

举个例子，访问管道或 socket 时，如果设置了 `O_NONBLOCK` 标志，那么就表示使用的是非阻塞 I/O 的方式访问，而不做任何设置的话，默认是阻塞 I/O。

应用程序每次轮询内核的 I/O 是否准备好，感觉有点傻乎乎，因为轮询的过程中，应用程序啥也做不了，只是在循环。

为了解决这种傻乎乎轮询方式，于是 **I/O 多路复用**技术就出来了，如 select、poll，它是通过 I/O 事件分发，当内核数据准备好时，再以事件通知应用程序进行操作。

这个做法大大改善了应用进程对 CPU 的利用率，在没有被通知的情况下，应用进程可以使用 CPU 做其他的事情。

下图是使用 select I/O 多路复用过程。注意，`read` 获取数据的过程（数据从内核态拷贝到用户态的过程），也是一个**同步的过程**，需要等待：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251047957.webp)

实际上，无论是阻塞 I/O、非阻塞 I/O，还是基于非阻塞 I/O 的多路复用**都是同步调用。因为它们在 read 调用时，内核将数据从内核空间拷贝到应用程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷贝效率不高，read 调用就会在这个同步过程中等待比较长的时间。**

而真正的**异步 I/O** 是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程都不用等待。

当我们发起 `aio_read` 之后，就立即返回，内核自动将数据从内核空间拷贝到应用程序空间，这个拷贝过程同样是异步的，内核自动完成的，和前面的同步操作不一样，应用程序并不需要主动发起拷贝动作。过程如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251047944.webp)

下面这张图，总结了以上几种 I/O 模型：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251047743.webp)

在前面我们知道了，I/O 是分为两个过程的：

1. 数据准备的过程
2. 数据从内核空间拷贝到用户进程缓冲区的过程

阻塞 I/O 会阻塞在「过程 1 」和「过程 2」，而非阻塞 I/O 和基于非阻塞 I/O 的多路复用只会阻塞在「过程 2」，所以这三个都可以认为是同步 I/O。

异步 I/O 则不同，「过程 1 」和「过程 2 」都不会阻塞。

> 用故事去理解这几种 I/O 模型

举个你去饭堂吃饭的例子，你好比用户程序，饭堂好比操作系统。

阻塞 I/O 好比，你去饭堂吃饭，但是饭堂的菜还没做好，然后你就一直在那里等啊等，等了好长一段时间终于等到饭堂阿姨把菜端了出来（数据准备的过程），但是你还得继续等阿姨把菜（内核空间）打到你的饭盒里（用户空间），经历完这两个过程，你才可以离开。

非阻塞 I/O 好比，你去了饭堂，问阿姨菜做好了没有，阿姨告诉你没，你就离开了，过几十分钟，你又来饭堂问阿姨，阿姨说做好了，于是阿姨帮你把菜打到你的饭盒里，这个过程你是得等待的。

基于非阻塞的 I/O 多路复用好比，你去饭堂吃饭，发现有一排窗口，饭堂阿姨告诉你这些窗口都还没做好菜，等做好了再通知你，于是等啊等（`select` 调用中），过了一会阿姨通知你菜做好了，但是不知道哪个窗口的菜做好了，你自己看吧。于是你只能一个一个窗口去确认，后面发现 5 号窗口菜做好了，于是你让 5 号窗口的阿姨帮你打菜到饭盒里，这个打菜的过程你是要等待的，虽然时间不长。打完菜后，你自然就可以离开了。

异步 I/O 好比，你让饭堂阿姨将菜做好并把菜打到饭盒里后，把饭盒送到你面前，整个过程你都不需要任何等待。







## 七、设备管理

### 7.1 设备控制器

我们的电脑设备可以接非常多的输入输出设备，比如键盘、鼠标、显示器、网卡、硬盘、打印机、音响等等，每个设备的用法和功能都不同，那操作系统是如何把这些输入输出设备统一管理的呢?

为了屏蔽设备之间的差异，每个设备都有一个叫**设备控制器（*Device Control*）** 的组件，比如硬盘有硬盘控制器、显示器有视频控制器等。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251038618.webp)

因为这些控制器都很清楚的知道对应设备的用法和功能，所以 CPU 是通过设备控制器来和设备打交道的。

设备控制器里有芯片，它可执行自己的逻辑，也有自己的寄存器，用来与 CPU 进行通信，比如：

- 通过写入这些寄存器，操作系统可以命令设备发送数据、接收数据、开启或关闭，或者执行某些其他操作。
- 通过读取这些寄存器，操作系统可以了解设备的状态，是否准备好接收一个新的命令等。

实际上，控制器是有三类寄存器，它们分别是**状态寄存器（*Status Register*）**、 **命令寄存器（*Command Register*）**以及**数据寄存器（*Data Register*）**，如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251039063.webp)

这三个寄存器的作用：

- *数据寄存器*，CPU 向 I/O 设备写入需要传输的数据，比如要打印的内容是「Hello」，CPU 就要先发送一个 H 字符给到对应的 I/O 设备。
- *命令寄存器*，CPU 发送一个命令，告诉 I/O 设备，要进行输入/输出操作，于是就会交给 I/O 设备去工作，任务完成后，会把状态寄存器里面的状态标记为完成。
- *状态寄存器*，目的是告诉 CPU ，现在已经在工作或工作已经完成，如果已经在工作状态，CPU 再发送数据或者命令过来，都是没有用的，直到前面的工作已经完成，状态寄存标记成已完成，CPU 才能发送下一个字符和命令。

CPU 通过读、写设备控制器中的寄存器来控制设备，这可比 CPU 直接控制输入输出设备，要方便和标准很多。

另外， 输入输出设备可分为两大类 ：**块设备（*Block Device*）**和**字符设备（*Character Device*）**。

- *块设备*，把数据存储在固定大小的块中，每个块有自己的地址，硬盘、USB 是常见的块设备。
- *字符设备*，以字符为单位发送或接收一个字符流，字符设备是不可寻址的，也没有任何寻道操作，鼠标是常见的字符设备。

块设备通常传输的数据量会非常大，于是控制器设立了一个可读写的**数据缓冲区**。

- CPU 写入数据到控制器的缓冲区时，当缓冲区的数据囤够了一部分，才会发给设备。
- CPU 从控制器的缓冲区读取数据时，也需要缓冲区囤够了一部分，才拷贝到内存。

这样做是为了，减少对设备的操作次数。

那 CPU 是如何与设备的控制寄存器和数据缓冲区进行通信的？存在两个方法：

- *端口 I/O*，每个控制寄存器被分配一个 I/O 端口，可以通过特殊的汇编指令操作这些寄存器，比如 `in/out` 类似的指令。
- *内存映射 I/O*，将所有控制寄存器映射到内存空间中，这样就可以像读写内存一样读写数据缓冲区。



### 7.2 I/O控制方式

在前面我知道，每种设备都有一个设备控制器，控制器相当于一个小 CPU，它可以自己处理一些事情，但有个问题是，当 CPU 给设备发送了一个指令，让设备控制器去读设备的数据，它读完的时候，要怎么通知 CPU 呢？

控制器的寄存器一般会有状态标记位，用来标识输入或输出操作是否完成。于是，我们想到第一种**轮询等待**的方法，让 CPU 一直查寄存器的状态，直到状态标记为完成，很明显，这种方式非常的傻瓜，它会占用 CPU 的全部时间。

那我们就想到第二种方法 —— **中断**，通知操作系统数据已经准备好了。我们一般会有一个硬件的**中断控制器**，当设备完成任务后触发中断到中断控制器，中断控制器就通知 CPU，一个中断产生了，CPU 需要停下当前手里的事情来处理中断。

另外，中断有两种，一种**软中断**，例如代码调用 `INT` 指令触发，一种是**硬件中断**，就是硬件通过中断控制器触发的。

但中断的方式对于频繁读写数据的磁盘，并不友好，这样 CPU 容易经常被打断，会占用 CPU 大量的时间。对于这一类设备的问题的解决方法是使用 **DMA（\*Direct Memory Access\*）** 功能，它可以使得设备在 CPU 不参与的情况下，能够自行完成把设备 I/O 数据放入到内存。那要实现 DMA 功能要有 「DMA 控制器」硬件的支持。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251042409.webp)

DMA 的工作方式如下：

- CPU 需对 DMA 控制器下发指令，告诉它想读取多少数据，读完的数据放在内存的某个地方就可以了；
- 接下来，DMA 控制器会向磁盘控制器发出指令，通知它从磁盘读数据到其内部的缓冲区中，接着磁盘控制器将缓冲区的数据传输到内存；
- 当磁盘控制器把数据传输到内存的操作完成后，磁盘控制器在总线上发出一个确认成功的信号到 DMA 控制器；
- DMA 控制器收到信号后，DMA 控制器发中断通知 CPU 指令完成，CPU 就可以直接用内存里面现成的数据了；

可以看到， CPU 当要读取磁盘数据的时候，只需给 DMA 控制器发送指令，然后返回去做其他事情，当磁盘数据拷贝到内存后，DMA 控制机器通过中断的方式，告诉 CPU 数据已经准备好了，可以从内存读数据了。仅仅在传送开始和结束时需要 CPU 干预。

### 7.3 设备驱动程序

虽然设备控制器屏蔽了设备的众多细节，但每种设备的控制器的寄存器、缓冲区等使用模式都是不同的，所以为了屏蔽「设备控制器」的差异，引入了**设备驱动程序**。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251043383.webp)

设备控制器不属于操作系统范畴，它是属于硬件，而设备驱动程序属于操作系统的一部分，操作系统的内核代码可以像本地调用代码一样使用设备驱动程序的接口，而设备驱动程序是面向设备控制器的代码，它发出操控设备控制器的指令后，才可以操作设备控制器。

不同的设备控制器虽然功能不同，但是**设备驱动程序会提供统一的接口给操作系统**，这样不同的设备驱动程序，就可以以相同的方式接入操作系统。如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251043534.webp)

前面提到了不少关于中断的事情，设备完成了事情，则会发送中断来通知操作系统。那操作系统就需要有一个地方来处理这个中断，这个地方也就是在设备驱动程序里，它会及时响应控制器发来的中断请求，并根据这个中断的类型调用响应的**中断处理程序**进行处理。

通常，设备驱动程序初始化的时候，要先注册一个该设备的中断处理函数。

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251043720.webp)

我们来看看，中断处理程序的处理流程：

1. 在 I/O 时，设备控制器如果已经准备好数据，则会通过中断控制器向 CPU 发送中断请求；
2. 保护被中断进程的 CPU 上下文；
3. 转入相应的设备中断处理函数；
4. 进行中断处理；
5. 恢复被中断进程的上下文；

### 7.4 通用块层

对于块设备，为了减少不同块设备的差异带来的影响，Linux 通过一个统一的**通用块层**，来管理不同的块设备。

通用块层是处于文件系统和磁盘驱动中间的一个块设备抽象层，它主要有两个功能：

- 第一个功能，向上为文件系统和应用程序，提供访问块设备的标准接口，向下把各种不同的磁盘设备抽象为统一的块设备，并在内核层面，提供一个框架来管理这些设备的驱动程序；
- 第二功能，通用层还会给文件系统和应用程序发来的 I/O 请求排队，接着会对队列重新排序、请求合并等方式，也就是 I/O 调度，主要目的是为了提高磁盘读写的效率。

Linux 内存支持 5 种 I/O 调度算法，分别是：

- 没有调度算法
- 先入先出调度算法
- 完全公平调度算法
- 优先级调度
- 最终期限调度算法

第一种，没有调度算法，是的，你没听错，它不对文件系统和应用程序的 I/O 做任何处理，这种算法常用在虚拟机 I/O 中，此时磁盘 I/O 调度算法交由物理机系统负责。

第二种，先入先出调度算法，这是最简单的 I/O 调度算法，先进入 I/O 调度队列的 I/O 请求先发生。

第三种，完全公平调度算法，大部分系统都把这个算法作为默认的 I/O 调度器，它为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求。

第四种，优先级调度算法，顾名思义，优先级高的 I/O 请求先发生， 它适用于运行大量进程的系统，像是桌面环境、多媒体应用等。

第五种，最终期限调度算法，分别为读、写请求创建了不同的 I/O 队列，这样可以提高机械磁盘的吞吐量，并确保达到最终期限的请求被优先处理，适用于在 I/O 压力比较大的场景，比如数据库等。



### 7.5 储系统I/O软件分层

前面说到了不少东西，设备、设备控制器、驱动程序、通用块层，现在再结合文件系统原理，我们来看看 Linux 存储系统的 I/O 软件分层。

可以把 Linux 存储系统的 I/O 由上到下可以分为三个层次，分别是文件系统层、通用块层、设备层。他们整个的层次关系如下图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251044824.webp)

这三个层次的作用是：

- 文件系统层，包括虚拟文件系统和其他文件系统的具体实现，它向上为应用程序统一提供了标准的文件访问接口，向下会通过通用块层来存储和管理磁盘数据。
- 通用块层，包括块设备的 I/O 队列和 I/O 调度器，它会对文件系统的 I/O 请求进行排队，再通过 I/O 调度器，选择一个 I/O 发给下一层的设备层。
- 设备层，包括硬件设备、设备控制器和驱动程序，负责最终物理设备的 I/O 操作。

有了文件系统接口之后，不但可以通过文件系统的命令行操作设备，也可以通过应用程序，调用 `read`、`write` 函数，就像读写文件一样操作设备，所以说设备在 Linux 下，也只是一个特殊的文件。

但是，除了读写操作，还需要有检查特定于设备的功能和属性。于是，需要 `ioctl` 接口，它表示输入输出控制接口，是用于配置和修改特定设备属性的通用接口。

另外，存储系统的 I/O 是整个系统最慢的一个环节，所以 Linux 提供了不少缓存机制来提高 I/O 的效率。

- 为了提高文件访问的效率，会使用**页缓存、索引节点缓存、目录项缓存**等多种缓存机制，目的是为了减少对块设备的直接调用。
- 为了提高块设备的访问效率， 会使用**缓冲区**，来缓存块设备的数据。



### 7.6 键盘敲入字母时，期间发生了什么？

看完前面的内容，相信你对输入输出设备的管理有了一定的认识，那接下来就从操作系统的角度回答开头的问题「键盘敲入字母时，操作系统期间发生了什么？」

我们先来看看 CPU 的硬件架构图：

![图片](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112251044785.webp)

CPU 里面的内存接口，直接和系统总线通信，然后系统总线再接入一个 I/O 桥接器，这个 I/O 桥接器，另一边接入了内存总线，使得 CPU 和内存通信。再另一边，又接入了一个 I/O 总线，用来连接 I/O 设备，比如键盘、显示器等。

那当用户输入了键盘字符，**键盘控制器**就会产生扫描码数据，并将其缓冲在键盘控制器的寄存器中，紧接着键盘控制器通过总线给 CPU 发送**中断请求**。

CPU 收到中断请求后，操作系统会**保存被中断进程的 CPU 上下文**，然后调用键盘的**中断处理程序**。

键盘的中断处理程序是在**键盘驱动程序**初始化时注册的，那键盘**中断处理函数**的功能就是从键盘控制器的寄存器的缓冲区读取扫描码，再根据扫描码找到用户在键盘输入的字符，如果输入的字符是显示字符，那就会把扫描码翻译成对应显示字符的  ASCII 码，比如用户在键盘输入的是字母 A，是显示字符，于是就会把扫描码翻译成 A 字符的 ASCII 码。

得到了显示字符的 ASCII 码后，就会把 ASCII 码放到「读缓冲区队列」，接下来就是要把显示字符显示屏幕了，显示设备的驱动程序会定时从「读缓冲区队列」读取数据放到「写缓冲区队列」，最后把「写缓冲区队列」的数据一个一个写入到显示设备的控制器的寄存器中的数据缓冲区，最后将这些数据显示在屏幕里。

显示出结果后，**恢复被中断进程的上下文**。
