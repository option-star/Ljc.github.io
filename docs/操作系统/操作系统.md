## 一、硬件结构



### 1.1 CPU是如何执行程序的？

:::tip

列举一些小问题

1. a = 1 + 2这条代码是怎么被CPU执行的？
2. 软件的32位和64位之间的区别？
3. 32位操作系统可以运行在64位的电脑上吗？
4. 64位的操作系统可以运行在32位的电脑上吗？如果不行，原因是什么？
5. 64位相比32位CPU的优势在哪？
6. 64位CPU的计算性能一定比32位CPU高很多吗？

:::

#### 图灵的工作方式

​	要想知道程序执行的原理，我们可以先从「图灵机」说起，图灵的基本思想是用机器来模拟人们用纸笔进行数学运算的过程，而且还定义了计算机由哪些部分组成，程序又是如何执行的。

![image-20211207144158423](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071442578.png)

图灵机的基本组成如下：

- **纸带**，纸带由⼀个个连续的格子组成，每个格子可以写入字符，纸带就好比内存，而纸带上的格子的字符就好比内存中的数据或程序；
- **读写头**，读写头可以读取纸带上任意格子的字符，也可以把字符写入到纸带的格子；
- 读写头上有⼀些部件，比如存储单元、控制单元以及运算单元：
  1. 存储单元用于存放数据；
  2. 控制单元用于识别字符是数据还是指令，以及控制程序的流程等；
  3. 运算单元用于执行运算指令；

​	知道了图灵机的组成后，我们以简单数学运算的 `1 + 2 `作为例子，来看看它是怎么执行这行代码的。

1. 首先，用读写头把 「1、2、+」这 3 个字符分别写入到纸带上的 3 个格子，然后读写头先停在 1 字符对应的格子上；

![步骤1](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071446658.png)

2. 接着，读写头读入 1 到存储设备中，这个存储设备称为图灵机的状态；

![image-20211207144737532](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071447589.png)

3. 然后读写头向右移动⼀个格，用同样的方式把 2 读入到图灵机的状态，于是现在图灵机的状态中存储着两个连续的数字， 1 和 2；

![image-20211207144813045](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071448104.png)

4. 读写头再往右移动⼀个格，就会碰到 + 号，读写头读到 + 号后，将 + 号传输给「控制单元」，控制单元发现是⼀个 + 号而不是数字，所以没有存⼊到状态中，因为 + 号是运算符指令，作用是加和目前的状态，于是通知「运算单元」工作。运算单元收到要加和状态中的值的通知后，就会把状态中的1 和 2 读入并计算，再将计算的结果 3 存放到状态中；

![image-20211207144933276](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071449338.png)

5. 最后，运算单元将结果返回给控制单元，控制单元将结果传输给读写头，读写头向右移动，把结果 3写入到纸带的格子中；

![image-20211207145027094](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071450147.png)

​	通过上面的图灵机计算 1 + 2 的过程，可以发现图灵机主要功能就是读取纸带格子中的内容，然后交给控制单元识别字符是数字还是运算符指令，如果是数字则存入到图灵机状态中，如果是运算符，则通知运算符单元读取状态中的数值进行计算，计算结果最终返回给读写头，读写头把结果写⼊到纸带的格⼦中。

​	事实上，图灵机这个看起来很简单的工作方式，和我们今天的计算机是基本⼀样的。接下来，我们⼀同再看看当今计算机的组成以及工作方式。

#### 冯诺伊曼模型

​	在 1945 年冯诺依曼和其他计算机科学家们提出了计算机具体实现的报告，其遵循了图灵机的设计，而且还提出用电子元件构造计算机，并约定了用⼆进制进行计算和存储，还定义计算机基本结构为 5 个部分，分别是**中央处理器（CPU）、内存、输⼊设备、输出设备、总线**。

![image-20211207145327017](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071453072.png)

这 5 个部分也被称为冯诺依曼模型，接下来看看这 5 个部分的具体作用。

##### 内存

​	我们的程序和数据都是存储在内存，存储的区域是线性的。

​	数据存储的单位是一个**二进制位（bit）**，即0或1。最小一个地址为内存总字节数-1，这种结构好似我们程序里的数组，所以内存的读写任何一个数据的速度都是一样的。

##### 中央处理器

​	中央处理器也就是我们常说的 CPU，32 位和 64 位 CPU 最主要区别在于⼀次能计算多少字节数据：

- 32 位 CPU ⼀次可以计算 4 个字节；
- 64 位 CPU ⼀次可以计算 8 个字节；

这里的 32 位和 64 位，通常称为 CPU 的位宽。

​	之所以 CPU 要这样设计，是为了能**计算更大的数值**，如果是 8 位的 CPU，那么⼀次只能计算 1 个字节`0~255` 范围内的数值，这样就无法⼀次完成计算 `10000 * 500` ，于是为了能⼀次计算大数的运算，CPU 需要支持多个 byte ⼀起计算，所以 CPU 位宽越大，可以计算的数值就越大，比如如说 32 位 CPU 能计算的最大整数是 `4294967295` 。

​	CPU 内部还有⼀些组件，常见的有寄存器、控制单元和逻辑运算单元等。其中，控制单元负责控制 CPU ⼯作，逻辑运算单元负责计算，而寄存器可以分为多种类，每种寄存器的功能又不尽相同。

​	CPU 中的寄存器主要作用是存储计算时的数据，你可能好奇为什么有了内存还需要寄存器？原因很简单， 因为内存离 CPU 太远了，而寄存器就在 CPU 里，还紧挨着控制单元和逻辑运算单元，自然计算时速度会很快。

常见的寄存器种类：

- **通用寄存器**：用来存放需要进行运算的数据，比如需要进行加和运算的两个数据
- **程序计数器**： 用来存储CPU要执行下一条指令[所在的内存地址]，注意不是存储了下一条要执行的指令，此时指令还在内存中，程序计数器只是存储了下一条指令的地址。
- **指令寄存器**：用来存放程序计数器的指向的指令，也就是指令本身，指令被执行完成之前，指令都存储在这里

##### 总线

总线式用于CPU和 内存以及其他设备之间的通信，总线可分为3种：

- **地址总线**： 用于指定CPU将要操作的内存地址；
- **数据总线**： 用于读写内存的数据；
- **控制总线**： 用于发送和接受信号，比如中断、设备复位等信号，CPU收到信号后自然进行相应，这时也需要控制总线；

当 CPU 要读写内存数据的时候，⼀般需要通过两个总线：

1. 首先要通过「地址总线」来指定内存的地址；
2. 再通过「数据总线」来传输数据；

##### 输入、输出设备

​	输⼊设备向计算机输⼊数据，计算机经过计算后，把数据输出给输出设备。期间，如果输⼊设备是键盘， 按下按键时是需要和 CPU 进行交互的，这时就需要用到控制总线了。

#### 线路位宽和CPU位宽

​	数据是如何通过线路传输的呢？其实是通过操作电压，低电压表示 0，高压电压则表示 1。

​	如果构造了高低高这样的信号，其实就是 101 ⼆进制数据，⼗进制则表示 5，如果只有⼀条线路，就意味 着每次只能传递 1 bit 的数据，即 0 或 1，那么传输 101 这个数据，就需要 3 次才能传输完成，这样的效 率非常低。

​	这样⼀位⼀位传输的方式，称为**串行**，下⼀个 bit 必须等待上⼀个 bit 传输完成才能进行传输。当然，想⼀次多传⼀些数据，增加线路即可，这时数据就可以**并行**传输。

​	为了避免低效率的串行传输的方式，线路的位宽最好⼀次就能访问到所有的内存地址。 CPU 要想操作的内存地址就需要地址总线，如果地址总线只有 1 条，那每次只能表示 「0 或 1」这两种情况，所以 CPU ⼀次 只能操作 2 个内存地址，如果想要 CPU 操作 4G 的内存，那么就需要 32 条地址总线，因为 2 ^ 32 = 4G 。

​	知道了线路位宽的意义后，我们再来看看 CPU 位宽。

​	CPU 的位宽最好不要小于线路位宽，比如 32 位 CPU 控制 40 位宽的地址总线和数据总线的话，工作起来就会非常复杂且麻烦，所以 32 位的 CPU 最好和 32 位宽的线路搭配，因为 32 位 CPU ⼀次最多只能操作32 位宽的地址总线和数据总线。

​	如果用32 位 CPU 去加和两个 64 位大小的数字，就需要把这 2 个 64 位的数字分成 2 个低位 32 位数字和 2 个⾼位 32 位数字来计算，先加个两个低位的 32 位数字，算出进位，然后加和两个高位的 32 位数字， 最后再加上进位，就能算出结果了，可以发现 32 位 CPU 并不能⼀次性计算出加和两个 64 位数字的结 果。

​	但是并不代表 64 位 CPU 性能比 32 位 CPU 高很多，很少应用需要算超过 32 位的数字，所以**如果计算的数额不超过32位数字的情况下，32位和64 位CPU 之间没什么区别的，只有当计算超过32位数字的情况下，64位的优势才能体现出来**。

​	另外，32 位 CPU 最大只能操作 4GB 内存，就算你装了 8 GB 内存条，也没用。而 64 位 CPU 寻址范围则很大，理论最大的寻址空间为 `2^64` 。

#### 程序执行的基本过程

​	在前面，我们知道了程序在图灵机的执行过程，接下来我们来看看程序在冯诺依曼模型上是怎么执行的。 程序实际上是⼀条⼀条指令，所以程序的运行过程就是把每⼀条指令⼀步⼀步的执行起来，负责执行指令 的就是 CPU 了。

![image-20211207152610066](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071526134.png)

CPU执行程序的过程如下：

1. CPU 读取「程序计数器」的值，这个值是指令的内存地址，然后 CPU 的「控制单元」操作「地址总线」指定需要访问的内存地址，接着通知内存设备准备数据，数据准备好后通过「数据总线」将指令数据传给 CPU，CPU 收到内存传来的数据后，将这个指令数据存⼊到「指令寄存器」。
2. 第⼆步，CPU 分析「指令寄存器」中的指令，确定指令的类型和参数，如果是计算类型的指令，就把指令交给「逻辑运算单元」运算；如果是存储类型的指令，则交由「控制单元」执行；
3. 第三步，CPU 执行完指令后，「程序计数器」的值自增，表示指向下⼀条指令。这个自增的大小，由CPU 的位宽决定，比如 32 位的 CPU，指令是 4 个字节，需要 4 个内存地址存放，因此「程序计数器」的值会自增 4；

简单总结⼀下就是，⼀个程序执行的时候，CPU 会根据程序计数器里的内存地址，从内存里面把需要执 的指令读取到指令寄存器里面执行，然后根据指令长度自增，开始顺序读取下⼀条指令。 CPU 从程序计数器读取指令、到执行、再到下⼀条指令，这个过程会不断循环，直到程序执行结束，这个不断循环的过程被称为 **CPU** **的指令周期**。

#### a = 1 + 2 执行具体过程

​	知道了基本的程序执行过程后，接下来用a = 1 + 2 的作为例子，进⼀步分析该程序在冯诺伊曼模型的执行过程。

​	CPU 是不认识 a = 1 + 2 这个字符串，这些字符串只是方便我们程序员认识，要想这段程序能跑起来，还需要把整个程序翻译成**汇编语言**的程序，这个过程称为编译成汇编代码。

​	针对汇编代码，我们还需要用汇编器翻译成机器码，这些机器码由 0 和 1 组成的机器语言，这⼀条条机器码，就是⼀条条的**计算机指令**，这个才是 CPU 能够真正认识的东西。

​	下面来看看 a = 1 + 2 在 32 位 CPU 的执行过程。

​	程序编译过程中，编译器通过分析代码，发现 1 和 2 是数据，于是程序运行时，内存会有个专门的区域来存放这些数据，这个区域就是「数据段」。如下图，数据 1 和 2 的区域位置：

- 数据 1 被存放到 0x100 位置；
- 数据 2 被存放到 0x104 位置；

注意，数据和指令是分开区域存放的，存放指令区域的地方称为「正文段」。

![image-20211207155323804](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071553874.png)

​	编译器会把 a = 1 + 2 翻译成 4 条指令，存放到正文段中。如图，这 4 条指令被存放到了 0x200 ~ 0x20c 的区域中：

- 0x200 的内容是 load 指令将 0x100 地址中的数据 1 装⼊到寄存器 R0 ；
-  0x204 的内容是 load 指令将 0x104 地址中的数据 2 装⼊到寄存器 R1 ； 
- 0x208 的内容是 add 指令将寄存器 R0 和 R1 的数据相加，并把结果存放到寄存器 R2 ；
- 0x20c 的内容是 store 指令将寄存器 R2 中的数据存回数据段中的 0x108 地址中，这个地址也就 是变量 a 内存中的地址；

​	编译完成后，具体执⾏程序的时候，程序计数器会被设置为 0x200 地址，然后依次执行这 4 条指令。

​	上面的例子中，由于是在 32 位 CPU 执⾏的，因此⼀条指令是占 32 位大小，所以你会发现每条指令间隔4 个字节。

​	而数据的大小是根据你在程序中指定的变量类型，比如 int 类型的数据则占 4 个字节， char 类型的数据则占 1 个字节。

##### 指令

​	上⾯的例⼦中，图中指令的内容我写的是简易的汇编代码，⽬的是为了⽅便理解指令的具体内容，事实上指令的内容是⼀串⼆进制数字的机器码，每条指令都有对应的机器码，CPU 通过解析机器码来知道指令的内容。

​	不同的 CPU 有不同的指令集，也就是对应着不同的汇编语⾔和不同的机器码，接下来选⽤最简单的 MIPS指集，来看看机器码是如何⽣成的，这样也能明⽩⼆进制的机器码的具体含义。

​	MIPS 的指令是⼀个 32 位的整数，高6 位代表着操作码，表示这条指令是⼀条什么样的指令，剩下的 26位不同指令类型所表示的内容也就不相同，主要有三种类型R、I 和 J。

![image-20211207170704938](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071707084.png)

⼀起具体看看这三种类型的含义：

- ***R* 指令**，⽤在算术和逻辑操作，⾥⾯由读取和写⼊数据的寄存器地址。如果是逻辑位移操作，后⾯还有位移操作的「位移量」，⽽最后的「功能码」则是再前⾯的操作码不够的时候，扩展操作码来表示对应的具体指令的；

- ***I* 指令**，⽤在数据传输、条件分⽀等。这个类型的指令，就没有了位移量和操作码，也没有了第三个寄存器，⽽是把这三部分直接合并成了⼀个地址值或⼀个常数；

- ***J* 指令**，⽤在跳转，⾼ 6 位之外的 26 位都是⼀个跳转后的地址；

​	接下来，我们把前⾯例⼦的这条指令：「 add 指令将寄存器 R0 和 R1 的数据相加，并把结果放⼊到R2 」，翻译成机器码。

![image-20211207170919918](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071709964.png)

加和运算 add 指令是属于 R 指令类型：

- add 对应的 MIPS 指令⾥操作码是 000000 ，以及最末尾的功能码是 100000 ，这些数值都是固定的，查⼀下 MIPS 指令集的⼿册就能知道的；

- rs 代表第⼀个寄存器 R0 的编号，即 00000 ；

- rt 代表第⼆个寄存器 R1 的编号，即 00001 ；

- rd 代表⽬标的临时寄存器 R2 的编号，即 00010 ；

- 因为不是位移操作，所以位移量是 00000

​	把上⾯这些数字拼在⼀起就是⼀条 32 位的 MIPS 加法指令了，那么⽤ 16 进制表示的机器码则是`0x00011020` 。

​	编译器在编译程序的时候，会构造指令，这个过程叫做指令的编码。CPU 执⾏程序的时候，就会解析指令，这个过程叫作**指令的解码**。

​	现代大多数 CPU 都使⽤来流水线的⽅式来执⾏指令，所谓的流⽔线就是把⼀个任务拆分成多个⼩任务，于是⼀条指令通常分为 4 个阶段，称为 4 级流⽔线，如下图：

![流水线](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071712013.png)

四个阶段的具体含义：

1.  CPU 通过程序计数器读取对应内存地址的指令，这个部分称为 **Fetch（取得指令）**；
1.  CPU 对指令进⾏解码，这个部分称为 **Decode（指令译码）**；
1.   CPU 执⾏指令，这个部分称为 **Execution（执⾏指令）**；
1.   CPU 将计算结果存回寄存器或者将寄存器的值存⼊内存，这个部分称为 **Store（数据回写）**

​	上⾯这 4 个阶段，我们称为**指令周期（Instrution Cycle）**，CPU 的⼯作就是⼀个周期接着⼀个周期，周⽽复始。

​	事实上，不同的阶段其实是由计算机中的不同组件完成的：

![image-20211207171427814](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071714863.png)

1. 取指令的阶段，我们的指令是存放在**存储器**⾥的，实际上，通过程序计数器和指令寄存器取出指令的过程，是由**控制器**操作的；
2. 指令的译码过程，也是由**控制器**进⾏的；
3. 指令执⾏的过程，⽆论是进⾏算术操作、逻辑操作，还是进⾏数据传输、条件分⽀操作，都是由**算术逻辑单元**操作的，也就是由**运算器**处理的。但是如果是⼀个简单的⽆条件地址跳转，则是直接在**控制器**⾥⾯完成的，不需要⽤到运算器。

##### 指令的类型

指令从功能⻆度划分，可以分为 5 大类：

- **数据传输类型的指令**，⽐如` store/load` 是寄存器与内存间数据传输的指令， `mov `是将⼀个内存地址的数据移动到另⼀个内存地址的指令；
- **运算类型的指令**，⽐如加减乘除、位运算、⽐较⼤⼩等等，它们最多只能处理两个寄存器中的数据；
- **跳转类型的指令**，通过修改程序计数器的值来达到跳转执⾏指令的过程，⽐如编程中常⻅的 `if-else` 、 `swtich-case` 、函数调⽤等。
- **信号类型的指令**，比如发生中断的指令`trap`
- **闲置类型的指令**，⽐如指令 `nop `，执⾏后 CPU 会空转⼀个周期；

##### 指令的执行速度

> 时钟周期

​	CPU 的硬件参数都会有 `GHz `这个参数，⽐如⼀个 1 `GHz` 的 CPU，指的是时钟频率是 1 G，代表着 1 秒会产⽣ 1G 次数的脉冲信号，每⼀次脉冲信号⾼低电平的转换就是⼀个周期，称为**时钟周期**。

​	对于 CPU 来说，在⼀个时钟周期内，CPU 仅能完成⼀个最基本的动作，时钟频率越⾼，时钟周期就越短，⼯作速度也就越快。

​	⼀个时钟周期⼀定能执⾏完⼀条指令吗？答案是不⼀定的，⼤多数指令不能在⼀个时钟周期完成，通常需要若⼲个时钟周期。不同的指令需要的时钟周期是不同的，加法和乘法都对应着⼀条 CPU 指令，但是乘法需要的时钟周期就要⽐加法多。

> 如何让程序跑得更快？

​	程序执⾏的时候，耗费的 CPU 时间少就说明程序是快的，对于程序的 CPU 执⾏时间，我们可以拆解成**CPU时钟周期数（CPU Cycles）和时钟周期时间（Clock Cycle Time）的乘积**

![image-20211207172354914](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071723967.png)

​	时钟周期时间就是我们前⾯提及的 CPU 主频，主频越⾼说明 CPU 的⼯作速度就越快，⽐如我⼿头上的电脑的 CPU 是 2.4 GHz 四核 Intel Core i5，这⾥的 2.4 GHz 就是电脑的主频，时钟周期时间就是 1/2.4G。

​	要想 CPU 跑的更快，⾃然缩短时钟周期时间，也就是提升 CPU 主频，但是今⾮彼⽇，摩尔定律早已失效，当今的 CPU 主频已经很难再做到翻倍的效果了。

​	另外，换⼀个更好的 CPU，这个也是我们软件⼯程师控制不了的事情，我们应该把⽬光放到另外⼀个乘法因⼦ —— CPU 时钟周期数，如果能减少程序所需的 CPU 时钟周期数量，⼀样也是能提升程序的性能的。

​	对于 CPU 时钟周期数我们可以进⼀步拆解成：「**指令数 x 每条指令的平均时钟周期数（Cycles Per Instruction，简称 CPI** **）**」，于是程序的 CPU 执⾏时间的公式可变成如下：

![image-20211207172638810](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071726854.png)

因此，要想程序跑的更快，优化这三者即可：

- **指令数**： 表示执⾏程序所需要多少条指令，以及哪些指令。这个层⾯是基本靠编译器来优化，毕竟同样的代码，在不同的编译器，编译出来的计算机指令会有各种不同的表示⽅式。
- **每条指令的平均时钟周期数CPI**： 表示⼀条指令需要多少个时钟周期数，现代⼤多数 CPU 通过流⽔线技术（Pipline），让⼀条指令需要的 CPU 时钟周期数尽可能的少；
- **时钟周期时间**，表示计算机主频，取决于计算机硬件。有的 CPU ⽀持超频技术，打开了超频意味着把 CPU 内部的时钟给调快了，于是 CPU ⼯作速度就变快了，但是也是有代价的，CPU 跑的越快，散热的压⼒就会越⼤，CPU 会很容易奔溃。

#### 总结

> 64 位相⽐ 32 位 CPU 的优势在哪吗？64 位 CPU 的计算性能⼀定⽐ 32 位 CPU ⾼很多吗？

64 位相⽐ 32 位 CPU 的优势主要体现在两个⽅⾯：

- 64 位 CPU 可以⼀次计算超过 32 位的数字，⽽ 32 位 CPU 如果要计算超过 32 位的数字，要分多步骤进⾏计算，效率就没那么⾼，但是⼤部分应⽤程序很少会计算那么⼤的数字，所以**只有运算大数字的时候，64位 CPU的优势才能体现出来，否则和 32位CPU的计算性能相差不⼤**。
- 64 位 CPU 可以**寻址更大的内存空间**，32 位 CPU 最⼤的寻址地址是 4G，即使你加了 8G ⼤⼩的内存，也还是只能寻址到 4G，⽽ 64 位 CPU 最⼤寻址地址是 2^64 ，远超于 32 位 CPU 最⼤寻址地址的 2^32 。

> 你知道软件的 32 位和 64 位之间的区别吗？再来 32 位的操作系统可以运⾏在 64 位的电脑上吗？64 位的操作系统可以运⾏在 32 位的电脑上吗？如果不⾏，原因是什么？

​	64 位和 32 位软件，实际上代表指令是 64 位还是 32 位的：

- 如果 32 位指令在 64 位机器上执⾏，需要⼀套兼容机制，就可以做到兼容运⾏了。但是 **如果64位指令在32位机器上执行，就比较困难了，因为32位寄存器存不下64位的指令**
- 操作系统其实也是⼀种程序，我们也会看到操作系统会分成 32 位操作系统、64 位操作系统，其代表意义就是操作系统中程序的指令是多少位，⽐如 64 位操作系统，指令也就是 64 位，因此不能装在32 位机器上。

​	总之，硬件的 64 位和 32 位指的是 CPU 的位宽，软件的 64 位和 32 位指的是指令的位宽。 

### 1.2 存储器金字塔

​	⼤家如果想⾃⼰组装电脑的话，肯定需要购买⼀个 CPU，但是存储器⽅⾯的设备，分类⽐较多，那我们肯

定不能只买⼀种存储器，⽐如你除了要买内存，还要买硬盘，⽽针对硬盘我们还可以选择是固态硬盘还是

机械硬盘。

​	相信⼤家都知道内存和硬盘都属于计算机的存储设备，断电后内存的数据是会丢失的，⽽硬盘则不会，因

为硬盘是持久化存储设备，同时也是⼀个 I/O 设备。

​	但其实 CPU 内部也有存储数据的组件，这个应该⽐较少⼈注意到，⽐如**寄存器**、**CPU L1/L2/L3 Cache**

也都是属于存储设备，只不过它们能存储的数据⾮常⼩，但是它们因为靠近 CPU 核⼼，所以访问速度都⾮

常快，快过硬盘好⼏个数量级别。

​	问题来了，**那机械硬盘、固态硬盘、内存这三个存储器，到底和** **CPU L1 Cache** **相⽐速度差多少倍呢？**

​	在回答这个问题之前，我们先来看看「**存储器的层次结构**」，好让我们对存储器设备有⼀个整体的认识。

![image-20211209145701608](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091457771.png)



#### 存储器的层次结构

​	我们想象中⼀个场景，⼤学期末准备考试了，你前去图书馆临时抱佛脚。那么，在看书的时候，我们的⼤

脑会思考问题，也会记忆知识点，另外我们通常也会把常⽤的书放在⾃⼰的桌⼦上，当我们要找⼀本不常

⽤的书，则会去图书馆的书架找。

​	就是这么⼀个⼩⼩的场景，已经把计算机的存储结构基本都涵盖了。

​	我们可以把 CPU ⽐喻成我们的⼤脑，⼤脑正在思考的东⻄，就好⽐ CPU 中的**寄存器**，处理速度是最快

的，但是能存储的数据也是最少的，毕竟我们也不能⼀下同时思考太多的事情，除⾮你练过。

​	我们⼤脑中的记忆，就好⽐ **CPU Cache**，中⽂称为 CPU ⾼速缓存，处理速度相⽐寄存器慢了⼀点，但是

能存储的数据也稍微多了⼀些。

​	CPU Cache 通常会分为**L1、L2、L3三层**，其中 L1 Cache 通常分成「数据缓存」和「指令缓存」，L1

是距离 CPU 最近的，因此它⽐ L2、L3 的读写速度都快、存储空间都⼩。我们⼤脑中短期记忆，就好⽐

L1 Cache，⽽⻓期记忆就好⽐ L2/L3 Cache。

​	寄存器和 CPU Cache 都是在 CPU 内部，跟 CPU 挨着很近，因此它们的读写速度都相当的快，但是能存

储的数据很少，毕竟 CPU 就这么丁点⼤。

​	当我们⼤脑记忆中没有资料的时候，可以从书桌或书架上拿书来阅读，那我们桌⼦上的书，就好⽐**内存**，

我们虽然可以⼀伸⼿就可以拿到，但读写速度肯定远慢于寄存器，那图书馆书架上的书，就好⽐**硬盘**，能

存储的数据⾮常⼤，但是读写速度相⽐内存差好⼏个数量级，更别说跟寄存器的差距了。

![image-20211209145937525](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091459596.png)

​	我们从图书馆书架取书，把书放到桌⼦上，再阅读书，我们⼤脑就会记忆知识点，然后再经过⼤脑思考，

这⼀系列过程相当于，数据从硬盘加载到内存，再从内存加载到 CPU 的寄存器和 Cache 中，然后再通过

CPU 进⾏处理和计算。

​	**对于存储器，它的速度越快、能耗会越⾼、⽽且材料的成本也是越贵的，以⾄于速度快的存储器的容量都⽐较⼩。**

​	CPU ⾥的寄存器和 Cache，是整个计算机存储器中价格最贵的，虽然存储空间很⼩，但是读写速度是极快

的，⽽相对⽐较便宜的内存和硬盘，速度肯定⽐不上 CPU 内部的存储器，但是能弥补存储空间的不⾜。

存储器通常可以分为这么⼏个级别：

![image-20211209150110914](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091501110.png)

- 寄存器
- CPU Cache
  - L1-Cache
  - L2-Cache
  - L3-Cache
- 内存
- SSD/HDD硬盘



##### 寄存器

​	最靠近 CPU 的控制单元和逻辑计算单元的存储器，就是寄存器了，它使⽤的材料速度也是最快的，因此价格也是最贵的，那么数量不能很多。

​	存储器的数量通常在⼏⼗到⼏百之间，每个寄存器可以⽤来存储⼀定的字节（byte）的数据。⽐如：

- 32 位 CPU 中⼤多数寄存器可以存储 `4` 个字节；
- 64 位 CPU 中⼤多数寄存器可以存储` 8` 个字节。

​	寄存器的访问速度⾮常快，⼀般要求在半个 CPU 时钟周期内完成读写，CPU 时钟周期跟 CPU 主频息息相关，⽐如 2 GHz 主频的 CPU，那么它的时钟周期就是 1/2G，也就是 0.5ns（纳秒）。
​	CPU 处理⼀条指令的时候，除了读写寄存器，还需要解码指令、控制指令执⾏和计算。如果寄存器的速度太慢，则会拉⻓指令的处理周期，从⽽给⽤户的感觉，就是电脑「很慢」。

##### CPU Cache

​	CPU Cache ⽤的是⼀种叫 `SRAM（Static Random-AccessMemory，静态随机存储器）` 的芯⽚。SRAM之所以叫“静态”存储器，是因为只要有电，数据就可以保持存在，⽽⼀旦断电，数据就会丢失了。在 SRAM ⾥⾯，⼀个 bit 的数据，通常需要 6 个晶体管，所以 SRAM 的存储密度不⾼，同样的物理空间下，能存储的数据是有限的，不过也因为 SRAM 的电路简单，所以访问速度⾮常快。

​	CPU 的⾼速缓存，通常可以分为 L1、L2、L3 这样的三层⾼速缓存，也称为⼀级缓存、⼆次缓存、三次缓存。

![image-20211209151050920](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091510993.png)

###### L1 高速缓存

​	L1 ⾼速缓存的访问速度⼏乎和寄存器⼀样快，通常只需要 2~4 个时钟周期，⽽⼤⼩在⼏⼗ KB 到⼏百

KB 不等。每个 CPU 核⼼都有⼀块属于⾃⼰的 L1 ⾼速缓存，指令和数据在 L1 是分开存放的，所以 L1 ⾼速缓存通常分成**指令缓存**和**数据缓存**。

​	在 Linux 系统，我们可以通过这条命令，查看 CPU ⾥的 L1 Cache 「数据」缓存的容量⼤⼩：

```shell
$ cat /sys/devices/system/cpu/cpu0/cache/index0/size
# 32K
```

⽽查看 L1 Cache 「指令」缓存的容量⼤⼩，则是：

```shell
$ cat /sys/devices/system/cpu/cpu0/cache/index1/size
# 32K
```



###### L2 高速缓存

​	L2 ⾼速缓存同样每个 CPU 核⼼都有，但是 L2 ⾼速缓存位置⽐ L1 ⾼速缓存距离 CPU 核⼼ 更远，它⼤⼩⽐ L1 ⾼速缓存更⼤，CPU 型号不同⼤⼩也就不同，通常⼤⼩在⼏百 KB 到⼏ MB 不等，访问速度则更慢，速度在 10~20 个时钟周期。

在 Linux 系统，我们可以通过这条命令，查看 CPU ⾥的 L2 Cache 的容量⼤⼩:

```shell
$ cat /sys/devices/system/cpu/cpu0/cache/index2/size
# 256K
```

###### L3 高速缓存

​	L3 ⾼速缓存通常是多个 CPU 核⼼共⽤的，位置⽐ L2 ⾼速缓存距离 CPU 核⼼ 更远，⼤⼩也会更⼤些，通常⼤⼩在⼏ MB 到⼏⼗ MB 不等，具体值根据 CPU 型号⽽定。访问速度相对也⽐较慢⼀些，访问速度在 20~60 个时钟周期。

在 Linux 系统，我们可以通过这条命令，查看 CPU ⾥的 L3 Cache 的容量⼤⼩：

```shell
$ cat /sys/devices/system/cpu/cpu0/cache/index3/size
# 3072K
```

##### 内存

​	内存⽤的芯⽚和 CPU Cache 有所不同，它使⽤的是⼀种叫作`DRAM（Dynamic Random Access Memory，动态随机存取存储器）`的芯⽚。相⽐ SRAM，DRAM 的密度更⾼，功耗更低，有更⼤的容量，⽽且造价⽐ SRAM 芯⽚便宜很多。DRAM 存储⼀个 bit 数据，只需要⼀个晶体管和⼀个电容就能存储，但是因为数据会被存储在电容⾥，电容会不断漏电，所以需要「定时刷新」电容，才能保证数据不会被丢失，这就是 DRAM 之所以被称为「动态」存储器的原因，只有不断刷新，数据才能被存储起来。DRAM 的数据访问电路和刷新电路都⽐ SRAM 更复杂，所以访问的速度会更慢，内存速度⼤概在200~300 个 时钟周期之间。

##### SSD/HDD硬盘

​	`SSD（Solid-state disk）` 就是我们常说的固体硬盘，结构和内存类似，但是它相⽐内存的优点是断电后数据还是存在的，⽽内存、寄存器、⾼速缓存断电后数据都会丢失。内存的读写速度⽐ SSD ⼤概快10~1000 倍。当然，还有⼀款传统的硬盘，也就是机械硬盘（*Hard Disk Drive, HDD*），它是通过物理读写的⽅式来访问数据的，因此它访问速度是⾮常慢的，它的速度⽐内存慢 10W 倍左右。由于 SSD 的价格快接近机械硬盘了，因此机械硬盘已经逐渐被 SSD 替代了。

#### 存储器的层次关系

​	现代的⼀台计算机，都⽤上了 CPU Cahce、内存、到 SSD 或 HDD 硬盘这些存储器设备了。其中，存储空间越⼤的存储器设备，其访问速度越慢，所需成本也相对越少。CPU 并不会直接和每⼀种存储器设备直接打交道，⽽是每⼀种存储器设备只和它相邻的存储器设备打交道。⽐如，CPU Cache 的数据是从内存加载过来的，写回数据的时候也只写回到内存，CPU Cache 不会直接把数据写到硬盘，也不会直接从硬盘加载数据，⽽是先加载到内存，再从内存加载到 CPU Cache 中。所以，**每个存储器只和相邻的⼀层存储器设备打交道，并且存储设备为了追求更快的速度，所需的材料成本必然也是更⾼，也正因为成本太⾼，所以CPU内部的寄存器、L1\L2\L3Cache只好⽤较⼩的容量，相反内存、硬盘则可⽤更⼤的容量，这就我们今天所说的存储器层次结构。**

![image-20211209153044869](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091530948.png)

​	另外，当 CPU 需要访问内存中某个数据的时候，如果寄存器有这个数据，CPU 就直接从寄存器取数据即 可，如果寄存器没有这个数据，CPU 就会查询 L1 ⾼速缓存，如果 L1 没有，则查询 L2 ⾼速缓存，L2 还 是没有的话就查询  L3  ⾼速缓存，L3 依然没有的话，才去内存中取数据。所以，存储层次结构也形成了**缓存**的体系。

![未命名文件](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091805012.png)



#### 存储器之间的实际价格和性能差距

​	前⾯我们知道了，速度越快的存储器，造价成本往往也越⾼，那我们就以实际的数据来看看，不同层级的存储器之间的性能和价格差异。下⾯这张表格是不同层级的存储器之间的成本对⽐图：

![image-20211209180638200](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091806268.png)

​	你可以看到 L1 Cache 的访问延时是 1 纳秒，⽽内存已经是 100 纳秒了，相⽐ L1 Cache 速度慢了 100倍。另外，机械硬盘的访问延时更是⾼达 10 毫秒，相⽐ L1 Cache 速度慢了 10000000 倍，差了好⼏个数量级别。在价格上，每⽣成 MB ⼤⼩的 L1 Cache 相⽐内存贵了 466 倍，相⽐机械硬盘那更是贵了 175000倍。我在某东逛了下各个存储器设备的零售价，8G 内存 + 1T 机械硬盘 + 256G 固态硬盘的总价格，都不及⼀块 Intle i5-10400 的 CPU 的价格，这款 CPU 的⾼速缓存的总⼤⼩也就⼗多 MB。

#### 总结

​	各种存储器之间的关系，可以⽤我们在图书馆学习这个场景来理解。CPU 可以⽐喻成我们的⼤脑，我们当前正在思考和处理的知识的过程，就好⽐ CPU 中的**寄存器**处理数据的过程，速度极快，但是容量很⼩。⽽ CPU 中的 **L1-L3 Cache** 好⽐我们⼤脑中的短期记忆和⻓期记忆，需要⼩⼩花费点时间来调取数据并处理。我们⾯前的桌⼦就相当于**内存**，能放下更多的书（数据），但是找起来和看起来就要花费⼀些时间，相⽐CPU Cache 慢不少。⽽图书馆的书架相当于**硬盘**，能放下⽐内存更多的数据，但找起来就更费时间了，可以说是最慢的存储器设备了。 

​	寄存器、CPU Cache，到内存、硬盘，这样⼀层层下来的存储器，访问速度越来越慢，存储容量越来越⼤，价格也越来越便宜，⽽且每个存储器只和相邻的⼀层存储器设备打交道，于是这样就形成了存储器的层次结构。再来回答，开头的问题：那机械硬盘、固态硬盘、内存这三个存储器，到底和 CPU L1 Cache 相⽐速度差多少倍呢？

​	CPU L1 Cache 随机访问延时是 1 纳秒，内存则是 100 纳秒，所以 **CPU L1 Cache⽐内存快100倍左右**。SSD 随机访问延时是 150 微秒，所以 **CPU L1 Cache⽐SSD快150000倍左右**。最慢的机械硬盘随机访问延时已经⾼达 10 毫秒，我们来看看机械硬盘到底有多「⻳速」：

- **SSD比机械硬盘快70倍左右**
- **内存比机械硬盘快100000倍左右**
- **CPU L1 Cache比机械硬盘快10000000倍左右 **

​	我们把上述的时间⽐例差异放⼤后，就能⾮常直观感受到它们的性能差异了。如果 CPU 访问 L1 Cache 的缓存时间是 1 秒，那访问内存则需要⼤约 2 分钟，随机访问 SSD ⾥的数据则需要 1.7 天，访问机械硬盘那更久，⻓达近 4 个⽉。

### 1.3 如何写出让CPU跑得更快的代码？

​	代码都是由 CPU 跑起来的，我们代码写的好与坏就决定了 CPU 的执⾏效率，特别是在编写计算密集型的程序，更要注重 CPU 的执⾏效率，否则将会⼤⼤影响系统性能。CPU 内部嵌⼊了 CPU Cache（⾼速缓存），它的存储容量很⼩，但是离 CPU 核⼼很近，所以缓存的读写速度是极快的，那么如果 CPU 运算时，直接从 CPU Cache 读取数据，⽽不是从内存的话，运算速度就会很快。但是，⼤多数⼈不知道 CPU Cache 的运⾏机制，以⾄于不知道如何才能够写出能够配合 CPU Cache ⼯作机制的代码，⼀旦你掌握了它，你写代码的时候，就有新的优化思路了。那么，接下来我们就来看看，CPU Cache 到底是什么样的，是如何⼯作的呢，⼜该写出让 CPU 执⾏更快的代码呢？

![image-20211209181454012](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091814083.png)

#### CPU Cache有多快？

​	你可能会好奇为什么有了内存，还需要 CPU Cache？根据摩尔定律，CPU 的访问速度每 18 个⽉就会翻倍，相当于每年增⻓ 60% 左右，内存的速度当然也会不断增⻓，但是增⻓的速度远⼩于 CPU，平均每年只增⻓ 7% 左右。于是，CPU 与内存的访问性能的差距不断拉⼤。到现在，⼀次内存访问所需时间是 200~300 多个时钟周期，这意味着 CPU 和内存的访问速度已经相差200~300 多倍了。为了弥补 CPU 与内存两者之间的性能差异，就在 CPU 内部引⼊了 CPU Cache，也称⾼速缓存。 CPU Cache 通常分为⼤⼩不等的三级缓存，分别是 **L1 Cache、L2 Cache、L3 Cache**。由于 CPU Cache 所使⽤的材料是 SRAM，价格⽐内存使⽤的 DRAM ⾼出很多，在当今每⽣产 1 MB ⼤⼩的 CPU Cache 需要 7 美⾦的成本，⽽内存只需要 0.015 美⾦的成本，成本⽅⾯相差了 466 倍，所以 CPU Cache 不像内存那样动辄以 GB 计算，它的⼤⼩是以 KB 或 MB 来计算的。

​	在 Linux 系统中，我们可以使⽤下图的⽅式来查看各级 CPU Cache 的⼤⼩，⽐如我这⼿上这台服务器，离 CPU 核⼼最近的 L1 Cache 是 32KB，其次是 L2 Cache 是 256KB，最⼤的 L3 Cache 则是 3MB。

![image-20211209181948173](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091819247.png)

​	其中，**L1 Cache** **通常会分为「数据缓存」和「指令缓存」**，这意味着数据和指令在 L1 Cache 这⼀层是分开缓存的，上图中的 index0 也就是数据缓存，⽽ index1 则是指令缓存，它两的⼤⼩通常是⼀样的。另外，你也会注意到，L3 Cache ⽐ L1 Cache 和 L2 Cache ⼤很多，这是因为 **L1 Cache和L2 Cache都是每个CPU核心特有的，而L3 Cache是多个CPU核心共享的。**程序执⾏时，会先将内存中的数据加载到共享的 L3 Cache 中，再加载到每个核⼼独有的 L2 Cache，最后进⼊到最快的 L1 Cache，之后才会被 CPU 读取。它们之间的层级关系，如下图：

![image-20211209182158677](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091821740.png)

​	越靠近 CPU 核⼼的缓存其访问速度越快，CPU 访问 L1 Cache 只需要 2~4 个时钟周期，访问 L2 Cache⼤约 10~20 个时钟周期，访问 L3 Cache ⼤约 20~60 个时钟周期，⽽访问内存速度⼤概在 200~300个 时钟周期之间。所以，CPU  从  L1  Cache 读取数据的速度，相⽐从内存读取的速度，会快多倍。如下表格：

![image-20211209182227877](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091822993.png)

#### CPU Cache的数据结构和读取过程是怎么样的？

​	CPU Cache 的数据是从内存中读取过来的，它是以⼀⼩块⼀⼩块读取数据的，⽽不是按照单个数组元素来读取数据的，在 CPU Cache 中的，这样⼀⼩块⼀⼩块的数据，称为 **Cache Line（缓存块）**。

:::tip

![image-20211209182731593](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091827652.png)

:::

​	⽐如，有⼀个 int array[100] 的数组，当载⼊ array[0] 时，由于这个数组元素的⼤⼩在内存只占 4 字节，不⾜ 64 字节，CPU 就会**顺序加载**数组元素到 array[15] ，意味着 array[0]~array[15] 数组元素都会被缓存在 CPU Cache 中了，因此当下次访问这些数组元素时，会直接从 CPU Cache 读取，⽽不⽤再从内存中读取，⼤⼤提⾼了 CPU 读取数据的性能。事实上，CPU 读取数据的时候，⽆论数据是否存放到 Cache 中，CPU 都是先访问 Cache，只有当 Cache中找不到数据时，才会去访问内存，并把内存中的数据读⼊到 Cache 中，CPU 再从 CPU Cache 读取数据。

![image-20211209182913147](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091829247.png)

​	

​	这样的访问机制，跟我们使⽤「内存作为硬盘的缓存」的逻辑是⼀样的，如果内存有缓存的数据，则直接返回，否则要访问⻳速⼀般的硬盘。那 CPU 怎么知道要访问的内存数据，是否在 Cache ⾥？如果在的话，如何找到 Cache 对应的数据呢？我们从最简单、基础的**直接映射Cache（Direct Mapped Cache）** 说起，来看看整个 CPU Cache 的数据结构和访问逻辑。

​	前⾯，我们提到 CPU 访问内存数据时，是⼀⼩块⼀⼩块数据读取的，具体这⼀⼩块数据的⼤⼩，取决于`coherency_line_size` 的值，⼀般 64 字节。在内存中，这⼀块的数据我们称为**内存块（Block）**，读取的时候我们要拿到数据所在内存块的地址。对于直接映射 Cache 采⽤的策略，就是把内存块的地址始终「映射」在⼀个 **`CPU Line`（缓存块）** 的地址，⾄于映射关系实现⽅式，则是使⽤「取模运算」，取模运算的结果就是内存块地址对应的 `CPU Line`（缓存块） 的地址。举个例⼦，内存共被划分为 32 个内存块，CPU Cache 共有 8 个 `CPU Line`，假设 CPU 想要访问第 15 号内存块，如果 15 号内存块中的数据已经缓存在 CPU Line 中的话，则是⼀定映射在 7 号 CPU Line 中，因为 15 % 8 的值是 7。机智的你肯定发现了，使⽤取模⽅式映射的话，就会出现多个内存块对应同⼀个 CPU Line，⽐如上⾯的例⼦，除了 15 号内存块是映射在 7 号 CPU Line 中，还有 7 号、23 号、31 号内存块都是映射到 7 号 CPULine 中。

![image-20211209183416918](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112091834024.png)

​	因此，为了区别不同的内存块，在对应的 CPU Line 中我们还会存储⼀个**组标记（Tag）**。这个组标记会记 录当前  CPU  Line 中存储的数据对应的内存块，我们可以⽤这个组标记来区分不同的内存块。除了组标记信息外，CPU Line 还有两个信息：

1. 从内存加载过来的实际存放**数据（Data）**
2. **有效位（Valid bit）**，它是用来标记对应的CPU Line中的数据是否有效的，如果有效位是0，无论CPU Line中是否有数据，CPU都会访问内存，重新加载数据。

​	CPU 在从 CPU Cache 读取数据的时候，并不是读取 CPU Line 中的整个数据块，⽽是读取 CPU 所需要的⼀个数据⽚段，这样的数据统称为⼀个**字（word）**。那怎么在对应的 CPU Line 中数据块中找到所需的字呢？答案是，需要⼀个**偏移量(offset)**。因此，⼀个内存的访问地址，包括**组标记、CPU Line索引、偏移量**这三种信息，于是 CPU 就能通过这些信息，在 CPU Cache 中找到缓存的数据。⽽对于 CPU Cache ⾥的数据结构，则是由**索引+有效位+组标记+数据块**组成。

![image-20211210085818864](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112100858387.png)

​	如果内存中的数据已经在 CPU Cahe 中了，那 CPU 访问⼀个内存地址的时候，会经历这 4 个步骤：

1. 根据内存地址中索引信息，计算在 CPU Cahe 中的索引，也就是找出对应的 CPU Line 的地址；

2. 找到对应 CPU Line 后，判断 CPU Line 中的有效位，确认 CPU Line 中数据是否是有效的，如果是⽆

效的，CPU 就会直接访问内存，并重新加载数据，如果数据有效，则往下执⾏；

3. 对⽐内存地址中组标记和 CPU Line 中的组标记，确认 CPU Line 中的数据是我们要访问的内存数

据，如果不是的话，CPU 就会直接访问内存，并重新加载数据，如果是的话，则往下执⾏；

4. 根据内存地址中偏移量信息，从 CPU Line 的数据块中，读取对应的字。

​		到这⾥，相信你对直接映射 Cache 有了⼀定认识，但其实除了直接映射 Cache 之外，还有其他通过内存地址找到 CPU Cache 中的数据的策略，⽐如全相连 Cache （*Fully Associative Cache*）、组相连 Cache（*Set Associative Cache*）等，这⼏种策策略的数据结构都⽐较相似，我们理解了直接映射 Cache 的⼯作⽅式，其他的策略如果你有兴趣去看，相信很快就能理解的了。

#### 如何写出让 CPU 跑得更快的代码？

​	我们知道 CPU 访问内存的速度，⽐访问 CPU Cache 的速度慢了 100 多倍，所以如果 CPU 所要操作的数据在 CPU Cache 中的话，这样将会带来很⼤的性能提升。访问的数据在 CPU Cache 中的话，意味着**缓存命中**，缓存命中率越⾼的话，代码的性能就会越好，CPU 也就跑的越快。于是，「如何写出让 CPU 跑得更快的代码？」这个问题，可以改成「如何写出 CPU 缓存命中率⾼的代码？」。在前⾯我也提到， L1 Cache 通常分为「数据缓存」和「指令缓存」，这是因为 CPU 会别处理数据和指令，⽐如 1+1=2 这个运算， + 就是指令，会被放在「指令缓存」中，⽽输⼊数字 1 则会被放在「数据缓存」⾥。因此，**我们要分开来看「数据缓存」和「指令缓存」的缓存命中率**。

###### 如何提升数据缓存的命中率？

​	假设要遍历⼆维数组，有以下两种形式，虽然代码执⾏结果是⼀样，但你觉得哪种形式效率最⾼呢？为什么⾼呢？

![image-20211210090453889](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112100904208.png)

​	经过测试，形式⼀` array[i][j] `执⾏时间⽐形式⼆ `array[j][i] `快好⼏倍。之所以有这么⼤的差距，是因为⼆维数组 array 所占⽤的内存是连续的，⽐如⻓度 N 的指是 2 的话，那么内存中的数组元素的布局顺序是这样的：

![image-20211210090635448](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112100906589.png)

​	形式⼀⽤` array[i][j] `访问数组元素的顺序，正是和内存中数组元素存放的顺序⼀致。当 CPU 访问`array[0][0]` 时，由于该数据不在 Cache 中，于是会「顺序」把跟随其后的 3 个元素从内存中加载到 CPU Cache，这样当 CPU 访问后⾯的 3 个数组元素时，就能在 CPU Cache 中成功地找到数据，这意味着缓存命中率很⾼，缓存命中的数据不需要访问内存，这便⼤⼤提⾼了代码的性能。

⽽如果⽤形式⼆的 `array[j][i] `来访问，则访问的顺序就是：

![image-20211210090836147](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112100908370.png)

​	你可以看到，访问的⽅式跳跃式的，⽽不是顺序的，那么如果 N 的数值很⼤，那么操作` array[j][i] `时，是没办法把 `array[j+1][i] `也读⼊到 CPU Cache 中的，既然 `array[j+1][i] `没有读取到 CPU Cache，那么就需要从内存读取该数据元素了。很明显，这种不连续性、跳跃式访问数据元素的⽅式，可能不能充分利⽤到了 CPU Cache 的特性，从⽽代码的性能不⾼。那访问` array[0][0] `元素时，CPU 具体会⼀次从内存中加载多少元素到 CPU Cache 呢？这个问题，在前⾯我们也提到过，这跟 CPU Cache Line 有关，它表示 **CPU Cache** **⼀次性能加载数据的⼤⼩**，可以在Linux ⾥通过 coherency_line_size 配置查看 它的⼤⼩，通常是 64 个字节。

![image-20211210091105571](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112100911803.png)

​	也就是说，当 CPU 访问内存数据时，如果数据不在 CPU Cache 中，则会⼀次性会连续加载 64 字节⼤⼩的数据到 CPU Cache，那么当访问` array[0][0] `时，由于该元素不⾜ 64 字节，于是就会往后**顺序**`array[0][0]~array[0][15] `到 CPU Cache 中。顺序访问的 `array[i][j] `因为利⽤了这⼀特点，所以就会⽐跳跃式访问的 array[j][i] 要快。**因此，遇到这种遍历数组的情况时，按照内存布局顺序访问，将可以有效的利⽤** **CPU Cache** **带来的好处，这样我们代码的性能就会得到很⼤的提升。**

###### 如何提升指令缓存的命中率？

​	提升数据的缓存命中率的⽅式，是按照内存布局顺序访问，那针对指令的缓存该如何提升呢？我们以⼀个例⼦来看看，有⼀个元素为 0 到 100 之间随机数字组成的⼀维数组：

```c
int array[N];
for (i = 0; i <　N; i++) {
	array[i] = rand() % 100;
}
```

接下来，对这个数组做两个操作：

```C
// 操作一： 数组遍历: 循环遍历数组，把小于50的数组元素置为0；
for(i=0;i<N;i++){
    if(array[i] < 50) {
    	array[i] = 0;
    }
}
// 操作二：排序: 将数组排序
sort(array, array + N);
```

​	那么问题来了，你觉得先遍历再排序速度快，还是先排序再遍历速度快呢？在回答这个问题之前，我们先了解 CPU 的**分⽀预测器**。对于 if 条件语句，意味着此时⾄少可以选择跳转到两段不同的指令执⾏，也就是 if 还是 else 中的指令。那么，**如果分⽀预测可以预测到接下来要执⾏if⾥的指令，还是else指令的话，就可以「提前」把这些指令放在指令缓存中，这样CPU可以直接从Cache读取到指令，于是执⾏速度就会很快**。

​	当数组中的元素是随机的，分⽀预测就⽆法有效⼯作，⽽当数组元素都是是顺序的，分⽀预测器会动态地根据历史命中数据对未来进⾏预测，这样命中率就会很⾼。因此，先排序再遍历速度会更快，这是因为排序之后，数字是从⼩到⼤的，那么前⼏次循环命中 if < 50的次数会⽐较多，于是分⽀预测就会缓存 if ⾥的 array[i] = 0 指令到 Cache 中，后续 CPU 执⾏该指令就只需要从 Cache 读取就好了。如果你肯定代码中的 if 中的表达式判断为 true 的概率⽐较⾼，我们可以使⽤显示分⽀预测⼯具，⽐如在 C/C++ 语⾔中编译器提供了 likely 和 unlikely 这两种宏，如果 if 条件为 ture 的概率⼤，则可以⽤ likely 宏把 if ⾥的表达式包裹起来，反之⽤ unlikely 宏。实际上，CPU ⾃身的动态分⽀预测已经是⽐较准的了，所以只有当⾮常确信 CPU 预测的不准，且能够知道实际的概率情况时，才建议使⽤这两种宏。

![image-20211210133740298](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101337506.png)

###### **如果提升多核** **CPU** **的缓存命中率？**

​	在单核 CPU，虽然只能执⾏⼀个进程，但是操作系统给每个进程分配了⼀个时间⽚，时间⽚⽤完了，就调度下⼀个进程，于是各个进程就按时间⽚交替地占⽤ CPU，从宏观上看起来各个进程同时在执⾏。⽽现代 CPU 都是多核⼼的，进程可能在不同 CPU 核⼼来回切换执⾏，这对 CPU Cache 不是有利的，虽然 L3 Cache 是多核⼼之间共享的，但是 L1 和 L2 Cache 都是每个核⼼独有的，**如果⼀个进程在不同核⼼来回切换，各个核⼼的缓存命中率就会受到影响**，相反如果进程都在同⼀个核⼼上执⾏，那么其数据的 L1和 L2 Cache 的缓存命中率可以得到有效提⾼，缓存命中率⾼就意味着 CPU 可以减少访问 内存的频率。当有多个同时执⾏「计算密集型」的线程，为了防⽌因为切换到不同的核⼼，⽽导致缓存命中率下降的问题，我们可以把**线程绑定在某⼀个** **CPU** **核⼼上**，这样性能可以得到⾮常可观的提升。在 Linux 上提供了 `sched_setaffinity` ⽅法，来实现将线程绑定到某个 CPU 核⼼这⼀功能。

![image-20211210133942144](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101339239.png)

###### 总结

​	由于随着计算机技术的发展，CPU 与 内存的访问速度相差越来越多，如今差距已经⾼达好⼏百倍了，所以CPU 内部嵌⼊了 CPU Cache 组件，作为内存与 CPU 之间的缓存层，CPU Cache 由于离 CPU 核⼼很近，所以访问速度也是⾮常快的，但由于所需材料成本⽐较⾼，它不像内存动辄⼏个 GB ⼤⼩，⽽是仅有⼏⼗ KB 到 MB ⼤⼩。当 CPU 访问数据的时候，先是访问 CPU Cache，如果缓存命中的话，则直接返回数据，就不⽤每次都从内存读取速度了。因此，缓存命中率越⾼，代码的性能越好。但需要注意的是，当 CPU 访问数据时，如果 CPU Cache 没有缓存该数据，则会从内存读取数据，但是并不是只读⼀个数据，⽽是⼀次性读取⼀块⼀块的数据存放到 CPU Cache 中，之后才会被 CPU 读取。内存地址映射到 CPU Cache 地址⾥的策略有很多种，其中⽐较简单是直接映射 Cache，它巧妙的把内存地址拆分成「索引 + 组标记 + 偏移量」的⽅式，使得我们可以将很⼤的内存地址，映射到很⼩的 CPU Cache 地址⾥。要想写出让 CPU 跑得更快的代码，就需要写出缓存命中率⾼的代码，CPU L1 Cache 分为数据缓存和指令缓存，因⽽需要分别提⾼它们的缓存命中率：

- 对于数据缓存，我们在遍历数据的时候，应该按照内存布局的顺序操作，这是因为 CPU Cache 是根据 CPU Cache Line 批量操作数据的，所以顺序地操作连续内存数据时，性能能得到有效的提升；

- 对于指令缓存，有规律的条件分⽀语句能够让 CPU 的分⽀预测器发挥作⽤，进⼀步提⾼执⾏的效率；

​	另外，对于多核 CPU 系统，线程可能在不同 CPU 核⼼来回切换，这样各个核⼼的缓存命中率就会受到影响，于是要想提⾼进程的缓存命中率，可以考虑把线程绑定 CPU 到某⼀个 CPU 核⼼。

### 1.4 CPU缓存一致性

![image-20211210134253734](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101342827.png)

#### CPU Cache的数据的写入

​	随着时间的推移，CPU 和内存的访问性能相差越来越⼤，于是就在 CPU 内部嵌⼊了 CPU Cache（⾼速缓存），CPU Cache 离 CPU 核⼼相当近，因此它的访问速度是很快的，于是它充当了 CPU 与内存之间的缓存⻆⾊。CPU Cache 通常分为三级缓存：L1 Cache、L2 Cache、L3 Cache，级别越低的离 CPU 核⼼越近，访问速度也快，但是存储容量相对就会越⼩。其中，在多核⼼的 CPU ⾥，每个核⼼都有各⾃的 L1/L2 Cache， ⽽ L3 Cache 是所有核⼼共享使⽤的。

![image-20211210134420407](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101344480.png)

​	我们先简单了解下 CPU Cache 的结构，CPU Cache 是由很多个 Cache Line 组成的，CPU Line 是 CPU从内存读取数据的基本单位，⽽ CPU Line 是由各种标志（Tag）+ 数据块（Data Block）组成，你可以在下图清晰的看到：

![image-20211210134457232](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101344332.png)

​	我们当然期望 CPU 读取数据的时候，都是尽可能地从 CPU Cache 中读取，⽽不是每⼀次都要从内存中获取数据。所以，身为程序员，我们要尽可能写出缓存命中率⾼的代码，这样就有效提⾼程序的性能，具体的做法，你可以参考我上⼀篇⽂章「如何写出让 CPU 跑得更快的代码？」。事实上，数据不光是只有读操作，还有写操作，那么如果数据写⼊ Cache 之后，内存与 Cache 相对应的数据将会不同，这种情况下 Cache 和内存数据都不⼀致了，于是我们肯定是要把 Cache 中的数据同步到内存⾥的。问题来了，那在什么时机才把 Cache 中的数据写回到内存呢？为了应对这个问题，下⾯介绍两种针对写⼊数据的⽅法：

- **写直达**（Write Through）
- **写回**（Write Back）

##### 写直达

​	保持内存与 Cache ⼀致性最简单的⽅式是，**把数据同时写⼊内存和 Cache** **中**，这种⽅法称为**写直达（Write Through）**。在这个⽅法⾥，写⼊前会先判断数据是否已经在 CPU Cache ⾥⾯了：

- 如果数据已经在Cache里面，先将数据更新到Cache里面，再写人到内存里面；
- 如果数据没在Cache里面，就直接把数据更新到内存里面。

![image-20211210134808530](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101348629.png)

​	写直达法很直观，也很简单，但是问题明显，⽆论数据在不在 Cache ⾥⾯，每次写操作都会写回到内存，这样写操作将会花费⼤量的时间，⽆疑性能会受到很⼤的影响。



##### 写回

​	既然写直达由于每次写操作都会把数据写回到内存，⽽导致影响性能，于是为了要减少数据写回内存的频 率，就出现了**写回（Write Back）**的方法。在写回机制中，**当发生写操作时，新数据仅仅被写人Cache Block里，只有当修改过的Cache Block【被替换】时才需要写到内存中**，减少了数据写回内存的频率，这样便可以提⾼系统的性能。

![未命名文件](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101404849.png)

具体流程为：

1. 如果当发生写操作时，数据已经在CPU Cache里的话，则把数据更新到CPU Cache里，同时标记CPU Cache里的这个Cache Block为脏（Dirty）的，这个脏的标记表示这个时候，我们CPU Cache里面的这个Cache Block的数据和内存是不一致的，这种情况是不用把数据写到内存里的。
2. 如果发生写操作时，数据所对应的 Cache Block ⾥存放的是「别的内存地址的数据」的话，就要检 查这个  Cache  Block ⾥的数据有没有被标记为脏的，如果是脏的话，我们就要把这个  Cache Block⾥的数据写回到内存，然后再把当前要写⼊的数据，写⼊到这个 Cache Block ⾥，同时也把它标记为 脏的；如果  Cache  Block  ⾥⾯的数据没有被标记为脏，则就直接将数据写⼊到这个  Cache Block⾥，然后再把这个  Cache  Block 标记为脏的就好了。

​	可以发现写回这个⽅法，在把数据写⼊到  Cache 的时候，只有在缓存不命中，同时数据对应的  Cache 中 的 Cache Block 为脏标记的情况下，才会将数据写到内存中，⽽在缓存命中的情况下，则在写⼊后 Cache 后，只需把该数据对应的  Cache  Block 标记为脏即可，⽽不⽤写到内存⾥。这样的好处是，如果我们⼤量的操作都能够命中缓存，那么⼤部分时间⾥ CPU 都不需要读写内存，⾃然性能相⽐写直达会⾼很多。



##### 缓存一致性问题

​	现在  CPU  都是多核的，由于  L1/L2  Cache 是多个核⼼各⾃独有的，那么会带来多核⼼的 **缓存一致性（Cache Coherence）**的问题，如果不能保证缓存一致性的问题，就可能造成结果错误。那缓存⼀致性的问题具体是怎么发⽣的呢？我们以⼀个含有两个核⼼的 CPU 作为例⼦看⼀看。 假设 A 号核⼼和 B 号核⼼同时运⾏两个线程，都操作共同的变量 i（初始值为 0  ）。

![image-20211210141407313](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101414419.png)

​	这时如果 A 号核⼼执⾏了 i++ 语句的时候，为了考虑性能，使⽤了我们前⾯所说的写回策略，先把值为1 的执⾏结果写⼊到 L1/L2 Cache 中，然后把 L1/L2 Cache 中对应的 Block 标记为脏的，这个时候数据其实没有被同步到内存中的，因为写回策略，只有在 A 号核⼼中的这个 Cache Block 要被替换的时候，数据才会写⼊到内存⾥。如果这时旁边的 B 号核⼼尝试从内存读取 i 变量的值，则读到的将会是错误的值，因为刚才 A 号核⼼更新i 值还没写⼊到内存中，内存中的值还依然是 0。**这个就是所谓的缓存一致性问题，A号核心和B号核心的缓存，在这个时候是不一致的，从而导致执行结果的错误。**

 ![image-20211210141656484](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101416565.png)

​	那么，要解决这⼀问题，就需要⼀种机制，来同步两个不同核⼼⾥⾯的缓存数据。要实现的这个机制的 话，要保证做到下⾯这  2 点：

1. 某个  CPU 核⼼⾥的  Cache 数据更新时，必须要传播到其他核⼼的  Cache，这个称为 **写传播（Write Propagation）**
2. 某个CPU核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的，这个称为 **事务的串形化（Transaction Serialization）**

​	第⼀点写传播很容易就理解，当某个核⼼在 Cache 更新了数据，就需要同步到其他核⼼的 Cache ⾥。⽽ 对于第⼆点事务事的串形化，我们举个例⼦来理解它。假设我们有⼀个含有 4 个核⼼的 CPU，这 4 个核⼼都操作共同的变量 i（初始值为 0 ）。A 号核⼼先把 i值变为 100，⽽此时同⼀时间，B 号核⼼先把 i 值变为 200，这⾥两个修改，都会「传播」到 C 和 D 号核⼼。那么问题就来了，C 号核⼼先收到了  A 号核⼼更新数据的事件，再收到  B 号核⼼更新数据的事件，因此  C号核⼼看到的变量  i 是先变成  100，后变成 200。⽽如果 D 号核⼼收到的事件是反过来的，则 D 号核⼼看到的是变量 i 先变成 200，再变成 100，虽然是做 到了写传播，但是各个  Cache ⾥⾯的数据还是不⼀致的。所以，我们要保证 C 号核⼼和 D 号核⼼都能看到相同**顺序的数据变化**，⽐如变量 i 都是先变成 100，再变 成  200，这样的过程就是事务的串形化。

 

![image-20211210142032941](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101420041.png)

要实现事务串形化，要做到2点：

1. CPU核心对于 Cache 中数据的操作，需要同步给其他  CPU 核⼼；
2. 要引入“锁”的概念，如果两个 CPU 核⼼⾥有相同数据的 Cache，那么对于这个 Cache 数据的更 新，只有拿到了「锁」，才能进⾏对应的数据更新。

那接下来我们看看，写传播和事务串形化具体是⽤什么技术实现的。



#### 总线嗅探

​	写传播的原则就是当某个 CPU 核⼼更新了 Cache 中的数据，要把该事件⼴播通知到其他核⼼。最常⻅实现的⽅式是 **总线嗅探（Bus Snooping）**。

​	我还是以前⾯的 i 变量例⼦来说明总线嗅探的⼯作机制，当 A 号 CPU 核⼼修改了 L1 Cache 中 i 变量的值，通过总线把这个事件⼴播通知给其他所有的核⼼，然后每个 CPU 核⼼都会监听总线上的⼴播事件，并检查是否有相同的数据在⾃⼰的 L1 Cache ⾥⾯，如果 B 号 CPU 核⼼的 L1 Cache 中有该数据，那么也需要把该数据更新到⾃⼰的 L1 Cache。

​	可以发现，总线嗅探⽅法很简单， CPU 需要每时每刻监听总线上的⼀切活动，但是不管别的核⼼的Cache 是否缓存相同的数据，都需要发出⼀个⼴播事件，这⽆疑会加重总线的负载。另外，总线嗅探只是保证了某个 CPU 核⼼的 Cache 更新数据这个事件能被其他 CPU 核⼼知道，但是并不能保证事务串形化。于是，有⼀个协议基于总线嗅探机制实现了事务串形化，也⽤状态机机制降低了总线带宽压⼒，这个协议就是 MESI 协议，这个协议就做到了 CPU 缓存⼀致性。

#### MESI协议

MESI 协议其实是 4 个状态单词的开头字⺟缩写，分别是：

- `Modified` ： 已修改
- `Exclusive`: 独占
- `Shared`：共享
- `Invalidated`: 已失效

这四个状态来标记Cache Line四个不同的状态。

​	「已修改」状态就是我们前⾯提到的脏标记，代表该 Cache Block 上的数据已经被更新过，但是还没有写到内存⾥。

​	「已失效」状态，表示的是这个 Cache Block ⾥的数据已经失效了，不可以读取该状态的数据。

​	「独占」和「共享」状态都代表 Cache Block ⾥的数据是⼲净的，也就是说，这个时候 Cache Block ⾥的数据和内存⾥⾯的数据是⼀致性的。「独占」和「共享」的差别在于，独占状态的时候，数据只存储在⼀个 CPU 核⼼的 Cache ⾥，⽽其他CPU 核⼼的 Cache 没有该数据。这个时候，如果要向独占的 Cache 写数据，就可以直接⾃由地写⼊，⽽不需要通知其他 CPU 核⼼，因为只有你这有这个数据，就不存在缓存⼀致性的问题了，于是就可以随便操作该数据。另外，在「独占」状态下的数据，如果有其他核⼼从内存读取了相同的数据到各⾃的 Cache ，那么这个时候，独占状态下的数据就会变成共享状态。那么，「共享」状态代表着相同的数据在多个 CPU 核⼼的 Cache ⾥都有，所以当我们要更新 Cache ⾥⾯的数据的时候，不能直接修改，⽽是要先向所有的其他 CPU 核⼼⼴播⼀个请求，要求先把其他核⼼的Cache 中对应的 Cache Line 标记为「⽆效」状态，然后再更新当前 Cache ⾥⾯的数据。

我们举个具体的例⼦来看看这四个状态的转换：

1.  当 A 号 CPU 核⼼从内存读取变量 i 的值，数据被缓存在 A 号 CPU 核⼼⾃⼰的 Cache ⾥⾯，此时其他 CPU 核⼼的 Cache 没有缓存该数据，于是标记 Cache Line 状态为「独占」，此时其 Cache 中的数据与内存是⼀致的；

2. 然后 B 号 CPU 核⼼也从内存读取了变量 i 的值，此时会发送消息给其他 CPU 核⼼，由于 A 号 CPU

核⼼已经缓存了该数据，所以会把数据返回给 B 号 CPU 核⼼。在这个时候， A 和 B 核⼼缓存了相同

的数据，Cache Line 的状态就会变成「共享」，并且其 Cache 中的数据与内存也是⼀致的；

3. 当 A 号 CPU 核⼼要修改 Cache 中 i 变量的值，发现数据对应的 Cache Line 的状态是共享状态，则要向所有的其他 CPU 核⼼⼴播⼀个请求，要求先把其他核⼼的 Cache 中对应的 Cache Line 标记为「⽆效」状态，然后 A 号 CPU 核⼼才更新 Cache ⾥⾯的数据，同时标记 Cache Line 为「已修改」状态，此时 Cache 中的数据就与内存不⼀致了。

4. 如果 A 号 CPU 核⼼「继续」修改 Cache 中 i 变量的值，由于此时的 Cache Line 是「已修改」状态，因此不需要给其他 CPU 核⼼发送消息，直接更新数据即可。

5. 如果 A 号 CPU 核⼼的 Cache ⾥的 i 变量对应的 Cache Line 要被「替换」，发现 Cache Line 状态是「已修改」状态，就会在替换前先把数据同步到内存。

​	所以，可以发现当 Cache Line 状态是「已修改」或者「独占」状态时，修改更新其数据不需要发送⼴播给其他 CPU 核⼼，这在⼀定程度上减少了总线带宽压⼒。事实上，整个 MESI 的状态可以⽤⼀个有限状态机来表示它的状态流转。还有⼀点，对于不同状态触发的事件操作，可能是来⾃本地 CPU 核⼼发出的⼴播事件，也可以是来⾃其他 CPU 核⼼通过总线发出的⼴播、事件。

下图即是 MESI 协议的状态图：

![image-20211210145407957](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101454067.png)

​	MESI 协议的四种状态之间的流转过程，我汇总成了下⾯的表格，你可以更详细的看到每个状态转换的原 因：

![image-20211210145442911](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101454040.png)

![image-20211210145453749](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101454890.png)



#### 总结

​	CPU 在读写数据的时候，都是在 CPU Cache 读写数据的，原因是 Cache 离 CPU 很近，读写性能相⽐内 存⾼出很多。对于 Cache ⾥没有缓存 CPU 所需要读取的数据的这种情况，CPU 则会从内存读取数据，并 将数据缓存到 Cache ⾥⾯，最后 CPU 再从 Cache  读取数据。⽽对于数据的写⼊，CPU 都会先写⼊到 Cache ⾥⾯，然后再在找个合适的时机写⼊到内存，那就有「写 直达」和「写回」这两种策略来保证  Cache  与内存的数据⼀致性：

- **写直达**： 只要有数据写⼊，都会直接把数据写⼊到内存⾥⾯，这种⽅式简单直观，但是性能就会受限 于内存的访问速度；
- **写回**： 对于已经缓存在 Cache 的数据的写⼊，只需要更新其数据就可以，不⽤写⼊到内存，只有在 需要把缓存⾥⾯的脏数据交换出去的时候，才把数据同步到内存⾥，这种⽅式在缓存命中率⾼的情 况，性能会更好；

​	当今 CPU 都是多核的，每个核⼼都有各⾃独⽴的 L1/L2 Cache，只有 L3 Cache 是多个核⼼之间共享的。 所以，我们要确保多核缓存是⼀致性的，否则会出现错误的结果。要想实现缓存⼀致性，关键是要满⾜  2 点：

1. **写传播**： 当某个  CPU  核⼼发⽣写⼊操作时，需要把该事件⼴播通知给其他核⼼；
2. **事务串形化**： ，这个很重要，只有保证了这个，才能保障我们的数据是真正⼀致的，我们的 程序在各个不同的核⼼上运⾏的结果也是⼀致的；

​	基于总线嗅探机制的  MESI 协议，就满⾜上⾯了这两点，因此它是保障缓存⼀致性的协议。MESI 协议，是已修改、独占、共享、已实现这四个状态的英⽂缩写的组合。整个 MSI 状态的变更，则是 根据来⾃本地 CPU 核⼼的请求，或者来⾃其他 CPU 核⼼通过总线传输过来的请求，从⽽构成⼀个流动的 状态机。另外，对于在「已修改」或者「独占」状态的 Cache Line，修改更新其数据不需要发送⼴播给其 他 CPU 核⼼。

### 1.5 CPU是如何执行任务的？

你清楚下⾯这⼏个问题吗？

- 有了内存，为什么还需要 CPU Cache？
- CPU 是怎么读写数据的？
- 如何让 CPU 能读取数据更快⼀些？
- CPU 伪共享是如何发⽣的？⼜该如何避免？
- CPU 是如何调度任务的？如果你的任务对响应要求很⾼，你希望它总是能被先调度，这该怎么办？
- ...

这篇，我们就来回答这些问题。

 ![image-20211210145957324](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101459407.png)



#### CPU如何读写数据的？

![现代CPU 的架构图](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101501975.png)

​	由上图的CPU架构图可知，⼀个 CPU ⾥通常会有多个 CPU 核⼼，⽐如上图中的 1 号和 2 号 CPU 核⼼，并且每个 CPU 核⼼都有⾃⼰的 L1 Cache 和 L2 Cache，⽽ L1 Cache 通常分为 dCache（数据缓存） 和 iCache（指令缓 存），L3 Cache 则是多个核⼼共享的，这就是  CPU  典型的缓存层次。

​	上⾯提到的都是 CPU 内部的 Cache，放眼外部的话，还会有内存和硬盘，这些存储设备共同构成了⾦字 塔存储层次。如下图所示：

![image-20211210150317762](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101503869.png)

​	从上图也可以看到，从上往下，存储设备的容量会越⼤，⽽访问速度会越慢。⾄于每个存储设备的访问延 时，你可以看下图的表格：

![image-20211210150338979](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101503062.png)

​	你可以看到， CPU 访问 L1 Cache 速度⽐访问内存快 100 倍，这就是为什么 CPU ⾥会有 L1~L3 Cache的原因，⽬的就是把  Cache  作为  CPU 与内存之间的缓存层，以减少对内存的访问频率。CPU 从内存中读取数据到 Cache 的时候，并不是⼀个字节⼀个字节读取，⽽是⼀块⼀块的⽅式来读取数 据的，这⼀块⼀块的数据被称为 CPU Line（缓存⾏），所以 **CPU Line 是 CPU 从内存读取数据到 Cache 的单位**。⾄于 CPU Line ⼤⼩，在 Linux 系统可以⽤下⾯的⽅式查看到，你可以看我服务器的 L1 Cache Line ⼤⼩ 是 64 字节，也就意味 L1 Cache ⼀次载⼊数据的⼤⼩是 64  字节。

![image-20211210150508675](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101505743.png)

​	那么对数组的加载， CPU 就会加载数组⾥⾯连续的多个数据到 Cache ⾥，因此我们应该按照物理内存地 址分布的顺序去访问元素，这样访问数组元素的时候，Cache 命中率就会很⾼，于是就能减少从内存读取 数据的频率，  从⽽可提⾼程序的性能。但是，在我们不使⽤数组，⽽是使⽤单独的变量的时候，则会有**Cache 伪共享**的问题，**Cache 伪共享**问 题上是⼀个性能杀⼿，我们应该要规避它。接下来，就来看看  Cache  伪共享是什么？⼜如何避免这个问题？

​	现在假设有⼀个双核⼼的  CPU，这两个  CPU  核⼼并⾏运⾏着两个不同的线程，它们同时从内存中读取两l个不同的数据，分别是类型为`long`的变量  A  和 B，这个两个数据的地址在物理内存上是**连续**的，如果Cahce Line 的⼤⼩是 64 字节，并且变量 A 在 Cahce Line   的开头位置，那么这两个数据是位于**同⼀个Cache Line** 中，⼜因为 CPU Line 是 CPU 从内存读取数据到 Cache   的单位，所以这两个数据会被同时读⼊到了两个 CPU 核⼼中各⾃ Cache  中。我们来思考⼀个问题，如果这两个不同核⼼的线程分别修改不同的数据，⽐如 1 号 CPU 核⼼的线程只修 改了 变量 A，或 2 号 CPU 核⼼的线程的线程只修改了变量   B，会发⽣什么呢？

![image-20211210150815513](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101508586.png)

##### 分析伪共享的问题

​	现在我们结合保证多核缓存⼀致的 MESI 协议，来说明这⼀整个的过程。

1. 最开始变量 A 和 B 都还不在 Cache ⾥⾯，假设 1 号核⼼绑定了线程 A，2 号核⼼绑定了线程 B，线程 A 只会读写变量 A，线程 B 只会读写变量  B。

   ![](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101511538.png)

2. 1 号核⼼读取变量 A，由于 CPU 从内存读取数据到 Cache 的单位是 Cache Line，也正好变量 A 和 变 量 B 的数据归属于同⼀个 Cache Line，所以 A 和 B 的数据都会被加载到 Cache，并将此 Cache Line 标 记为「独占」状态。

![image-20211210151203943](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101512018.png)

3. 接着，2 号核⼼开始从内存⾥读取变量 B，同样的也是读取 Cache Line ⼤⼩的数据到 Cache 中，此 Cache Line 中的数据也包含了变量 A 和 变量 B，此时 1 号和 2 号核⼼的 Cache Line 状态变为「共享」状 态。

![image-20211210151218627](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101512712.png)

4. 1 号核⼼需要修改变量 A，发现此  Cache Line 的状态是「共享」状态，所以先需要通过总线发送消息 给 2 号核⼼，通知 2 号核⼼把 Cache 中对应的 Cache Line 标记为「已失效」状态，然后 1 号核⼼对应的 Cache  Line  状态变成「已修改」状态，并且修改变量 A。

![image-20211210151301822](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101513896.png)

5. 之后，2 号核⼼需要修改变量 B，此时 2 号核⼼的 Cache 中对应的 Cache Line 是已失效状态，另外由 于 1 号核⼼的 Cache 也有此相同的数据，且状态为「已修改」状态，所以要先把 1 号核⼼的 Cache 对应 的 Cache Line 写回到内存，然后 2 号核⼼再从内存读取 Cache Line ⼤⼩的数据到 Cache 中，最后把变 量  B 修改到  2 号核⼼的  Cache 中，并将状态标记为「已修改」状态。

![image-20211210151331219](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101513320.png)

​	所以，可以发现如果 1 号和 2 号 CPU 核⼼这样持续交替的分别修改变量 A 和 B，就会重复 ④ 和 ⑤ 这两 个步骤，Cache 并没有起到缓存的效果，虽然变量 A 和 B 之间其实并没有任何的关系，但是因为同时归属 于⼀个 Cache Line ，这个 Cache Line 中的任意数据被修改后，都会相互影响，从⽽出现 ④ 和 ⑤ 这两个 步骤。因此，这种因为多个线程同时读写同⼀个  Cache Line 的不同变量时，⽽导致  CPU Cache  失效的现象称为 **伪共享（False Sharing）**。



##### 避免伪共享的方法

​	因此，对于多个线程共享的热点数据，即经常会修改的数据，应该避免这些数据刚好在同⼀个  Cache  Line中，否则就会出现为伪共享的问题。接下来，看看在实际项⽬中是⽤什么⽅式来避免伪共享的问题的。

​	在linux内核中存在`__cacheline_aligned_in_smp`宏定义，是用来解决伪共享的问题。





#### CPU如何选择线程的？



## 二、操作系统结构

### 2.1 内核

### 2.2 Linux设计

### 2.3 Windows设计

### 2.4 总结





## 三、内存管理

### 虚拟内存

​	**单片机的CPU是直接操作内存的物理地址**，如果有多个程序使用同一个物理地址，是无法同时运行多个程序的，因为相同位置的内容会被擦除。

![image-20211207190517552](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071905602.png)

> 操作系统是如何解决这个问题呢？

​	这里问题的关键是程序都引用了绝对物理地址，这是我们需要避免的。

​	我们可以把进程所使⽤的地址「隔离」开来，即让操作系统为每个进程分配独⽴的⼀套「**虚拟地址**」，⼈⼈都有，⼤家⾃⼰玩⾃⼰的地址就⾏，互不⼲涉。但是有个前提每个进程都不能访问物理地址，⾄于虚拟地址最终怎么落到物理内存⾥，对进程来说是透明的，操作系统已经把这些都安排的明明⽩⽩了。

![image-20211207190740455](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071907504.png)

​	**操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。**

​	如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运⾏的时候，写⼊的是不同的物理地址，这样就不会冲突了。

于是，这里就引出了两种地址的概念：

- **虚拟内存地址**（*Virtual Memory Address*）： 程序所使用的内存地址
- **物理内存地址**（*Physical Memory Address*）： 存在硬件⾥⾯的空间地址

​	操作系统引⼊了虚拟内存，进程持有的虚拟地址会通过 CPU 芯⽚中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存，如下图所示：

![image-20211207191117307](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071911358.png)

> 操作系统是如何管理虚拟地址和物理地址之间的关系？

​	主要有两种⽅式，分别是**内存分段和内存分页**。



### 内存分段

​	程序是由若⼲个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。**不同的段是有不同的属性的，所以就用分段（Segmentation）的形式把这些段分离出来**。



> 分段机制下，虚拟地址和物理地址是如何映射的？

​	分段机制下的虚拟地址由两部分组成，**段选择因子**和**段内偏移量**。

![image-20211207193423814](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071934876.png)

- **段选择因子**： 保存在段寄存器⾥⾯。段选择⼦⾥⾯最重要的是**段号**，⽤作段表的索引。**段表**⾥⾯保存的是这个**段的基地址、段的界限和特权等级**等
- **段内偏移量**：大小位于0与段界限之间。`段基地址 + 段内偏移量 = 物理内存地址`。

​	由于虚拟地址是通过**段表**与物理地址进⾏映射的，分段机制会把程序的虚拟地址分成 4 个段，每个段在段表中有⼀个项，在这⼀项找到段的基地址，再加上偏移量，于是就能找到物理内存中的地址，如下图：

![image-20211207194149422](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071941484.png)

​	如果要访问段 3 中偏移量 500 的虚拟地址，那么`物理地址 = 段3基地址 7000 + 偏移量 500 `

​	分段的办法很好，解决了程序本身不需要关⼼具体的物理内存地址的问题，但它也有⼀些不⾜之处：

1. 存在**内存碎片**问题
2. **内存交换的效率低**

接下来，说说为什么会有这两个问题。

> 分段为什么会产生内存碎片问题？

​	我们来看看这样⼀个例⼦。假设有 1G 的物理内存，⽤户执⾏了多个程序，其中：

- 游戏占⽤了 512MB 内存
- 浏览器占⽤了 128MB 内存
- ⾳乐占⽤了 256 MB 内存。

​	这个时候，如果我们关闭了浏览器，则空闲内存还有 1024 - 512 - 256 = 256MB。

​	如果这个 256MB 不是连续的，被分成了两段 128 MB 内存，这就会导致没有空间再打开⼀个 200MB 的程序。

![image-20211207194739589](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112071947659.png)

这⾥的内存碎⽚的问题共有两处地⽅：

- **外部内存碎片**： 产⽣了多个不连续的⼩物理内存，导致新的程序⽆法被装载；
- **内部内存碎片**： 程序所有的内存都被装载到了物理内存，但是这个程序有部分的内存可能并不是很常使⽤，这也会导致内存的浪费；

针对上⾯两种内存碎⽚的问题，解决的⽅式会有所不同。

解决外部内存碎⽚的问题就是**内存交换**。

​	可以把⾳乐程序占⽤的那 256MB 内存写到硬盘上，然后再从硬盘上读回来到内存⾥。不过再读回的时候，我们不能装载回原来的位置，⽽是紧紧跟着那已经被占⽤了的 512MB 内存后⾯。这样就能空缺出连续的 256MB 空间，于是新的 200MB 程序就可以装载进来。

​	这个内存交换空间，在 Linux 系统⾥，也就是我们常看到的 Swap 空间，这块空间是从硬盘划分出来的，⽤于内存与硬盘的空间交换。

> 再来看看，分段为什么会导致内存交换效率低？

​	对于多进程的系统来说，⽤分段的⽅式，内存碎⽚是很容易产⽣的，产⽣了内存碎⽚，那不得不重新Swap 内存区域，这个过程会产⽣性能瓶颈。

​	因为硬盘的访问速度要⽐内存慢太多了，每⼀次内存交换，我们都需要把⼀⼤段连续的内存数据写到硬盘上。

​	所以，**如果内存交换的时候，交换的是⼀个占内存空间很⼤的程序，这样整个机器都会显得卡顿。**

​	为了解决内存分段的内存碎⽚和内存交换效率低的问题，就出现了内存分⻚。

### 内存分页

​	分段的好处就是能产⽣连续的内存空间，但是会出现内存碎⽚和内存交换的空间太⼤的问题。

​	要解决这些问题，那么就要想出能少出现⼀些内存碎⽚的办法。另外，当需要进⾏内存交换的时候，让需要交换写⼊或者从磁盘装载的数据更少⼀点，这样就可以解决问题了。这个办法，也就是**内存分页**（*Paging*）。

> 什么是内存分页

​	**分页是把整个虚拟和物理内存空间切成⼀段段固定尺⼨的⼤⼩**。这样⼀个连续并且尺⼨固定的内存空间，我们叫**⻚**（*Page*）。在 Linux 下，每⼀⻚的⼤⼩为 4KB 。

> 内存分页是怎么联系虚拟地址和物理地址的？

虚拟地址与物理地址之间通过**页表**来映射，如下图：

![image-20211207200544350](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072005417.png)

​	⻚表是存储在内存⾥的，**内存管理单元** （*MMU*）就做将虚拟内存地址转换成物理地址的⼯作。

​	⽽当进程访问的虚拟地址在⻚表中查不到时，系统会产⽣⼀个**缺页异常**，进⼊系统内核空间分配物理内存、更新进程⻚表，最后再返回⽤户空间，恢复进程的运⾏。

> 分页是怎么解决分段的内存碎片、内存交换效率低的问题？

​	由于内存空间都是预先划分好的，也就不会像分段会产⽣间隙⾮常⼩的内存，这正是分段会产⽣内存碎⽚的原因。⽽**采⽤了分页，那么释放的内存都是以页为单位释放的，也就不会产生无法给进程使用的小内存。**

​	如果内存空间不够，操作系统会把其他正在运⾏的进程中的「最近没被使⽤」的内存⻚⾯给释放掉，也就是暂时写在硬盘上，称为**换出**（*Swap Out*）。⼀旦需要的时候，再加载进来，称为**换⼊**（*Swap In*）。所以，⼀次性写⼊磁盘的也只有少数的⼀个⻚或者⼏个⻚，不会花太多时间，**内存交换的效率就相对⽐较高**

![image-20211207202650936](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072026999.png)

​	更进⼀步地，分⻚的⽅式使得我们在加载程序的时候，不再需要⼀次性都把程序加载到物理内存中。我们完全可以在进⾏虚拟内存和物理内存的⻚之间的映射之后，并不真的把⻚加载到物理内存⾥，⽽是**只有在程序运⾏中，需要⽤到对应虚拟内存页里面的指令和数据时，再加载到物理内存⾥⾯去。**

> 分⻚机制下，虚拟地址和物理地址是如何映射的？

​	在分⻚机制下，虚拟地址分为两部分，**⻚号**和**⻚内偏移**。⻚号作为⻚表的索引，**⻚表**包含物理⻚每⻚所在**物理内存的基地址**，这个基地址与⻚内偏移的组合就形成了物理内存地址，⻅下图： 

![image-20211207202905602](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072029664.png)

总结⼀下，对于⼀个内存地址转换，其实就是这样三个步骤：

1. 把虚拟内存地址，切分成⻚号和偏移量；
2. 根据⻚号，从⻚表⾥⾯，查询对应的物理⻚号；
3. 直接拿物理⻚号，加上前⾯的偏移量，就得到了物理内存地址。

下⾯举个例⼦，虚拟内存中的⻚通过⻚表映射为了物理内存中的⻚，如下图：

![image-20211207203153944](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072031008.png)

这看起来似乎没什么⽑病，但是放到实际中操作系统，这种简单的分⻚是肯定是会有问题的。

> 简单的分⻚有什么缺陷吗？

​	有空间上的缺陷。

​	因为操作系统是可以同时运⾏⾮常多的进程的，那这不就意味着⻚表会⾮常的庞⼤。

​	在 32 位的环境下，虚拟地址空间共有 4GB，假设⼀个⻚的⼤⼩是 4KB（2^12），那么就需要⼤约 100 万 （2^20） 个⻚，每个「⻚表项」需要 4 个字节⼤⼩来存储，那么整个 4GB 空间的映射就需要有 4MB的内存来存储⻚表。

​	这 4MB ⼤⼩的⻚表，看起来也不是很⼤。但是要知道每个进程都是有⾃⼰的虚拟地址空间的，也就说都有⾃⼰的⻚表。

​	那么， 100 个进程的话，就需要 400MB 的内存来存储⻚表，这是⾮常⼤的内存了，更别说 64 位的环境了。

##### 多级页表

​	要解决上⾯的问题，就需要采⽤⼀种叫作**多级页表**（*Multi-Level Page Table*）的解决⽅案。

​	在前⾯我们知道了，对于单⻚表的实现⽅式，在 32 位和⻚⼤⼩ 4KB 的环境下，⼀个进程的⻚表需要装下 100 多万个「⻚表项」，并且每个⻚表项是占⽤ 4 字节⼤⼩的，于是相当于每个⻚表需占⽤ 4MB ⼤⼩的空间。

​	我们把这个 100 多万个「⻚表项」的单级⻚表再分⻚，将⻚表（⼀级⻚表）分为 1024 个⻚表（⼆级⻚表），每个表（⼆级⻚表）中包含 1024 个「⻚表项」，形成**⼆级分⻚**。如下图所示：

![image-20211207203623567](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072036629.png)

> 你可能会问，分了⼆级表，映射 4GB 地址空间就需要 4KB（⼀级⻚表）+ 4MB（⼆级⻚表）的内存，这样占⽤空间不是更⼤了吗？

​	当然如果 4GB 的虚拟地址全部都映射到了物理内存上的话，⼆级分⻚占⽤空间确实是更⼤了，但是，我们往往不会为⼀个进程分配那么多内存。

​	其实我们应该换个⻆度来看问题，还记得计算机组成原理⾥⾯⽆处不在的**局部性原理**么？

​	每个进程都有 4GB 的虚拟地址空间，⽽显然对于⼤多数程序来说，其使⽤到的空间远未达到 4GB，因为会存在部分对应的⻚表项都是空的，根本没有分配，对于已分配的⻚表项，如果存在最近⼀定时间未访问的⻚表，在物理内存紧张的情况下，操作系统会将⻚⾯换出到硬盘，也就是说不会占⽤物理内存。

​	如果使⽤了⼆级分⻚，⼀级⻚表就可以覆盖整个 4GB 虚拟地址空间，但**如果某个⼀级⻚表的⻚表项没有被⽤到，也就不需要创建这个⻚表项对应的⼆级⻚表了，即可以在需要时才创建⼆级⻚表**。做个简单的计算，假设只有 20% 的⼀级⻚表项被⽤到了，那么⻚表占⽤的内存空间就只有 4KB（⼀级⻚表） + 20% *4MB（⼆级⻚表）= 0.804MB ，这对⽐单级⻚表的 4MB 是不是⼀个巨⼤的节约？

​	那么为什么不分级的⻚表就做不到这样节约内存呢？我们从⻚表的性质来看，保存在内存中的⻚表承担的职责是将虚拟地址翻译成物理地址。假如虚拟地址在⻚表中找不到对应的⻚表项，计算机系统就不能⼯作了。所以**⻚表⼀定要覆盖全部虚拟地址空间，不分级的⻚表就需要有 100多万个⻚表项来映射，⽽⼆级分⻚则只需要** **1024** **个⻚表项**（此时⼀级⻚表覆盖到了全部虚拟地址空间，⼆级⻚表在需要时创建）。

​	我们把⼆级分⻚再推⼴到多级⻚表，就会发现⻚表占⽤的内存空间更少了，这⼀切都要归功于对局部性原理的充分应⽤。

对于 64 位的系统，两级分⻚肯定不够了，就变成了四级⽬录，分别是：

- **全局页目录项PGD**(Page Global Directory)
- **上层⻚⽬录项 PUD**（*Page Upper Directory*）；
- **中间⻚⽬录项 PMD**（*Page Middle Directory*）；
- **⻚表项 PTE**（*Page Table Entry*）；

![image-20211207205736792](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072057862.png)

##### TLB

​	多级⻚表虽然解决了空间上的问题，但是虚拟地址到物理地址的转换就多了⼏道转换的⼯序，这显然就降低了这俩地址转换的速度，也就是带来了时间上的开销。

​	程序是有局部性的，即在⼀段时间内，整个程序的执⾏仅限于程序中的某⼀部分。相应地，执⾏所访问的存储空间也局限于某个内存区域。

![image-20211207205955327](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072059387.png)

​	我们就可以利⽤这⼀特性，把最常访问的⼏个⻚表项存储到访问速度更快的硬件，于是计算机科学家们， 就在 CPU 芯⽚中，加⼊了⼀个专⻔存放程序最常访问的⻚表项的  Cache，这个  Cache 就是 TLB（*Translation  Lookaside  Buffer*） ，通常称为⻚表缓存、转址旁路缓存、快表等。

![image-20211207210112285](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072101340.png)

​	在  CPU 芯⽚⾥⾯，封装了内存管理单元（*Memory Management Unit*）芯⽚，它⽤来完成地址转换和  TLB的访问与交互。

​	有了 TLB 后，那么 CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的⻚表。 TLB  的命中率其实是很⾼的，因为程序最常访问的⻚就那么⼏个。

### 段页式内存管理

​	内存分段和内存分⻚并不是对⽴的，它们是可以组合起来在同⼀个系统中使⽤的，那么组合起来后，通常 称为**段⻚式内存管理**。

![image-20211207210237933](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072102002.png)

段⻚式内存管理实现的⽅式：

1. 先将程序划分为多个有逻辑意义的段，也就是前⾯提到的分段机制；
2. 接着再把每个段划分为多个⻚，也就是对分段划分出来的连续空间，再划分固定⼤⼩的⻚； 这样，地址结构就由**段号、段内页号和页内位移**三部分组成。

​	⽤于段⻚式地址变换的数据结构是每⼀个程序⼀张段表，每个段⼜建⽴⼀张⻚表，段表中的地址是⻚表的 起始地址，⽽⻚表中的地址则为某⻚的物理⻚号，如图所示：

![image-20211207210459681](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072104757.png)

段⻚式地址变换中要得到物理地址须经过三次内存访问：

1. 访问段表，得到⻚表起始地址；
2. 访问⻚表，得到物理⻚号
3. 将物理⻚号与⻚内位移组合，得到物理地址

可⽤软、硬件相结合的⽅法实现段⻚式地址变换，这样虽然增加了硬件成本和系统开销，但提⾼了内存的 利⽤率。



### Linux内存管理 

那么，Linux  操作系统采⽤了哪种⽅式来管理内存呢？

> 在回答这个问题前，我们得先看看 Intel 处理器的发展历史。

​	早期 Intel 的处理器从 80286 开始使⽤的是段式内存管理。但是很快发现，光有段式内存管理⽽没有⻚式 内存管理是不够的，这会使它的  X86  系列会失去市场的竞争⼒。因此，在不久以后的  80386 中就实现了对⻚式内存管理。也就是说，80386 除了完成并完善从 80286 开始的段式内存管理的同时还实现了⻚式内存 管理。

​	但是这个 80386 的⻚式内存管理设计时，没有绕开段式内存管理，⽽是建⽴在段式内存管理的基础上，这 就意味着，**⻚式内存管理的作⽤是在由段式内存管理所映射⽽成的地址上再加上⼀层地址映射。**

​	由于此时由段式内存管理映射⽽成的地址不再是“物理地址”了，Intel 就称之为“线性地址”（也称虚拟地 址）。于是，段式内存管理先将逻辑地址映射成线性地址，然后再由⻚式内存管理将线性地址映射成物理 地址。

![image-20211207213455124](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072134182.png)

这⾥说明下逻辑地址和线性地址：

- 程序所使⽤的地址，通常是没被段式内存管理映射的地址，称为逻辑地址；
- 通过段式内存管理映射的地址，称为线性地址，也叫虚拟地址； 逻辑地址是「段式内存管理」转换前的地址，线性地址则是「⻚式内存管理」转换前的地址。

> 了解完 Intel 处理器的发展历史后，我们再来说说 Linux 采⽤了什么⽅式管理内存？

​	**Linux内存主要采⽤的是⻚式内存管理，但同时也不可避免地涉及了段机制**。

​	这主要是上⾯ Intel 处理器发展历史导致的，因为 Intel X86 CPU ⼀律对程序中使⽤的地址先进⾏段式映 射，然后才能进⾏⻚式映射。既然  CPU 的硬件结构是这样，Linux 内核也只好服从  Intel  的选择。

​	但是事实上，Linux 内核所采取的办法是使段式映射的过程实际上不起什么作⽤。也就是说，“上有政策， 下有对策”，若惹不起就躲着⾛。

​	**Linux系统中的每个段都是从0地址开始的整个4GB虚拟空间（32位环境下），也就是所有的段的起始地址都一样的。这意味着，Linux系统的代码，包括操作系统本身的代码和应用程序代码，所面对的地址空间都是线性地址空间（虚拟地址），这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被用于访问控制和内存保护。**

> Linux   的虚拟地址空间是如何分布的？

​	在 Linux 操作系统中，虚拟地址空间的内部⼜被分为**内核空间和⽤户空间**两部分，不同位数的系统，地址 空间的范围也不同。⽐如最常⻅的  32 位和  64 位系统，如下所示：

![image-20211207214115875](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072141943.png)

通过这里可以看出： 

- 32位系统的内核空间占用1G，位于最高处，剩下的3G是用户空间
- 64位系统的内核空间和用户空间都是128T，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。

再来说说，内核空间与⽤户空间的区别：

- 进程在⽤户态时，只能访问⽤户空间内存；
- 只有进⼊内核态后，才可以访问内核空间的内存；

虽然每个进程都各⾃有独⽴的虚拟内存，但是**每个虚拟内存中的内核地址，其实关联的都是相同的物理内存**。这样，进程切换到内核态后，就可以很⽅便地访问内核空间内存。

![image-20211207214453133](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072144199.png)

接下来，进⼀步了解虚拟空间的划分情况，⽤户空间和内核空间划分的⽅式是不同的，内核空间的分布情 况就不多说了。

我们看看⽤户空间分布的情况，以  32 位系统为例，我画了⼀张图来表示它们的关系：

![image-20211207214518441](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112072145524.png)

通过这张图你可以看到，⽤户空间内存，从**低到⾼分别是  7  种不同的内存段：

- 程序文件段，包括二进制可执行代码
- 已初始化数据段，包括静态常量；包括未初始化的静态变量； 堆段，包括动态分配的内存，从低地址开始向上增⻓。
- ⽂件映射段，包括动态库、共享内存等，从低地址开始向上增⻓（[跟硬件和内核版本有关](http://lishiwen4.github.io/linux/linux-process-memory-location)）；
- 栈段，包括局部变量和函数调⽤的上下⽂等。栈的⼤⼩是固定的，⼀般是 8MB。当然系统也提供了参数，以便我们⾃定义⼤⼩；

在这  7  个内存段中，堆和⽂件映射段的内存是动态分配的。⽐如说，使⽤  C 标准库的`malloc()`或者`mmap()`，就可以分别在堆和文件映射段动态分配内存。

### 总结

​	为了在多进程环境下，使得进程之间的内存地址不受影响，相互隔离，于是操作系统就为每个进程独⽴分配⼀套**虚拟地址空间**，每个程序只关⼼⾃⼰的虚拟地址就可以，实际上⼤家的虚拟地址都是⼀样的，但分布到物理地址内存是不⼀样的。作为程序，也不⽤关⼼物理地址的事情。

​	每个进程都有⾃⼰的虚拟空间，⽽物理内存只有⼀个，所以当启⽤了⼤量的进程，物理内存必然会很紧张，于是操作系统会通过**内存交换**技术，把不常使⽤的内存暂时存放到硬盘（换出），在需要的时候再装载回物理内存（换⼊）。

​	那既然有了虚拟地址空间，那必然要把虚拟地址「映射」到物理地址，这个事情通常由操作系统来维护。那么对于虚拟地址与物理地址的映射关系，可以有**分段**和**分⻚**的⽅式，同时两者结合都是可以的。

​	内存分段是根据程序的逻辑⻆度，分成了栈段、堆段、数据段、代码段等，这样可以分离出不同属性的段，同时是⼀块连续的空间。但是每个段的⼤⼩都不是统⼀的，这就会导致内存碎⽚和内存交换效率低的问题。

​	于是，就出现了内存分⻚，把虚拟空间和物理空间分成⼤⼩固定的⻚，如在 Linux 系统中，每⼀⻚的⼤⼩为 4KB 。由于分了⻚后，就不会产⽣细⼩的内存碎⽚。同时在内存交换的时候，写⼊硬盘也就⼀个⻚或⼏个⻚，这就⼤⼤提⾼了内存交换的效率。

​	再来，为了解决简单分⻚产⽣的⻚表过⼤的问题，就有了**多级⻚表**，它解决了空间上的问题，但这就会导致 CPU 在寻址的过程中，需要有很多层表参与，加⼤了时间上的开销。于是根据程序的**局部性原理**，在CPU 芯⽚中加⼊了 **TLB**，负责缓存最近常被访问的⻚表项，⼤⼤提⾼了地址的转换速度。

​	**Linux系统主要采⽤了分⻚管理，但是由于Intel处理器的发展史，Linux系统⽆法避免分段管理**。于是Linux 就把所有段的基地址设为 0 ，也就意味着所有程序的地址空间都是线性地址空间（虚拟地址），相

当于屏蔽了 CPU 逻辑地址的概念，所以段只被⽤于访问控制和内存保护。

另外，Linxu 系统中虚拟空间分布可分为**⽤户态**和**内核态**两部分，其中⽤户态的分布：代码段、全局变量、

BSS、函数栈、堆内存、映射区。



## 四、进程和线程

### 4.1进程、线程基础知识

>  先来看看一则小故事

​	我们写好的⼀⾏⾏代码，为了让其⼯作起来，我们还得把它送进城（**进程**）⾥，那既然进了城⾥，那肯定

不能胡作⾮为了。

​	城⾥⼈有城⾥⼈的规矩，城中有个专⻔管辖你们的城管（**操作系统**），⼈家让你休息就休息，让你⼯作就

⼯作，毕竟摊位不多，每个⼈都要占这个摊位来⼯作，城⾥要⼯作的⼈多着去了。

​	所以城管为了公平起⻅，它使⽤⼀种策略（**调度**）⽅式，给每个⼈⼀个固定的⼯作时间（**时间⽚**），时间

到了就会通知你去休息⽽换另外⼀个⼈上场⼯作。

​	另外，在休息时候你也不能偷懒，要记住⼯作到哪了，不然下次到你⼯作了，你忘记⼯作到哪了，那还怎

么继续？

​	有的⼈，可能还进⼊了县城（**线程**）⼯作，这⾥相对轻松⼀些，在休息的时候，要记住的东⻄相对较少，

⽽且还能共享城⾥的资源。

#### 4.1.1 进程

##### 4.1.1.1 进程的概念

​	我们编写的代码只是⼀个存储在硬盘的静态⽂件，通过编译后就会⽣成⼆进制可执⾏⽂件，当我们运⾏这

个可执⾏⽂件后，它会被装载到内存中，接着 CPU 会执⾏程序中的每⼀条指令，那么这个**运⾏中的程序**， 就被称为**进程（Process）**。

​	现在我们考虑有⼀个会读取硬盘⽂件数据的程序被执⾏了，那么当运⾏到读取⽂件的指令时，就会去从硬

盘读取数据，但是硬盘的读写速度是⾮常慢的，那么在这个时候，如果 CPU 傻傻的等硬盘返回数据的话，

那 CPU 的利⽤率是⾮常低的。

​	做个类⽐，你去煮开⽔时，你会傻傻的等⽔壶烧开吗？很明显，⼩孩也不会傻等。我们可以在⽔壶烧开之

前去做其他事情。当⽔壶烧开了，我们⾃然就会听到“嘀嘀嘀”的声⾳，于是再把烧开的⽔倒⼊到⽔杯⾥就

好了。

​	所以，当进程要从硬盘读取数据时，CPU 不需要阻塞等待数据的返回，⽽是去执⾏另外的进程。当硬盘数

据返回时，CPU 会收到个**中断**，于是 CPU 再继续运⾏这个进程。

![image-20211208090455641](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112080905771.png)

​	这种**多个程序、交替执⾏**的思想，就有  CPU  管理多个进程的初步想法。

​	对于⼀个⽀持多进程的系统，CPU 会从⼀个进程快速切换⾄另⼀个进程，其间每个进程各运⾏⼏⼗或⼏百 个毫秒。

​	虽然单核的 CPU 在某⼀个瞬间，只能运⾏⼀个进程。但在 1 秒钟期间，它可能会运⾏多个进程，这样就 产⽣**并⾏的错觉**，实际上这是**并发**。

> 并行和并发有什么区别？

![image-20211208090710753](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112080907824.png)

> 进程和程序的关系的类比

​	到了晚饭时间，⼀对⼩情侣肚⼦都咕咕叫了，于是男⽣⻅机⾏事，就想给⼥⽣做晚饭，所以他就在⽹上找

了辣⼦鸡的菜谱，接着买了⼀些鸡⾁、辣椒、⾹料等材料，然后边看边学边做这道菜。

![image-20211208090829569](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112080908628.png)

​	突然，⼥⽣说她想喝可乐，那么男⽣只好把做菜的事情暂停⼀下，并在⼿机菜谱标记做到哪⼀个步骤，把

状态信息记录了下来。

​	然后男⽣听从⼥⽣的指令，跑去下楼买了⼀瓶冰可乐后，⼜回到厨房继续做菜。

​	**这体现了，CPU可以从一个进程（做菜）切换到另外一个进程（买可乐），在切换前必须要记录当前进程中运行的状态信息，以备下次切换回来的时候可以恢复运行**

​	所以，可以发现进程有着「**运⾏** **-** **暂停** **-** **运⾏**」的活动规律。

##### 4.1.1.2 进程的状态

​	在上⾯，我们知道了进程有着「运⾏ - 暂停 - 运⾏」的活动规律。⼀般说来，⼀个进程并不是⾃始⾄终连

续不停地运⾏的，它与并发执⾏中的其他进程的执⾏是相互制约的。

​	它有时处于运⾏状态，有时⼜由于某种原因⽽暂停运⾏处于等待状态，当使它暂停的原因消失后，它⼜进

⼊准备运⾏状态。

所以，**在⼀个进程的活动期间⾄少具备三种基本状态，即运⾏状态、就绪状态、阻塞状态。**

![image-20211208091944866](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112080919953.png)

上图中各个状态的意义：

- 运⾏状态（*Runing*）：该时刻进程占⽤ CPU；

- 就绪状态（*Ready*）：可运⾏，由于其他进程处于运⾏状态⽽暂时停⽌运⾏；

- 阻塞状态（*Blocked*）：该进程正在等待某⼀事件发⽣（如等待输⼊/输出操作的完成）⽽暂时停⽌运

  ⾏，这时，即使给它CPU控制权，它也⽆法运⾏；

当然，进程还有另外两个基本状态：

- 创建状态（*new*）：进程正在被创建时的状态；
- 结束状态（*Exit*）：进程正在从系统中消失时的状态；

于是，⼀个完整的进程状态的变迁如下图：

![image-20211208092212035](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112080922111.png)

再来详细说明⼀下进程的状态变迁：

- **NULL -> 创建状态**：⼀个新进程被创建时的第⼀个状态；

- **创建状态 -> 就绪状态**：当进程被创建完成并初始化后，⼀切就绪准备运⾏时，变为就绪状态，这个

  过程是很快的；

- **就绪态 -> 运⾏状态**：处于就绪状态的进程被操作系统的进程调度器选中后，就分配给 CPU 正式运⾏

  该进程；

- **运⾏状态 ->结束状态**：当进程已经运⾏完成或出错时，会被操作系统作结束状态处理；

- **运⾏状态 -> 就绪状态**：处于运⾏状态的进程在运⾏过程中，由于分配给它的运⾏时间⽚⽤完，操作

  系统会把该进程变为就绪态，接着从就绪态选中另外⼀个进程运⾏；

- **运⾏状态 -> 阻塞状态**：当进程请求某个事件且必须等待时，例如请求 I/O 事件；

- **阻塞状态 -> 就绪状态**：当进程要等待的事件完成时，它从阻塞状态变到就绪状态；

​	如果有⼤量处于阻塞状态的进程，进程可能会占⽤着物理内存空间，显然不是我们所希望的，毕竟物理内

存空间是有限的，被阻塞状态的进程占⽤着物理内存就⼀种浪费物理内存的⾏为。

​	所以，在虚拟内存管理的操作系统中，通常会把**阻塞状态的进程的物理内存空间换出到硬盘**，等需要再次

运⾏的时候，再从硬盘换⼊到物理内存。

![image-20211208092927822](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112080929901.png)

​	那么，就需要⼀个新的状态，来**描述进程没有占⽤实际的物理内存空间的情况，这个状态就是挂起状态**。 这跟阻塞状态是不⼀样，阻塞状态是等待某个事件的返回。

​	另外，挂起状态可以分为两种：

- **阻塞挂起状态**： 进程在外存（硬盘）并等待某个事件的出现；
- **就绪挂起状态**： 进程在外存（硬盘），但只要进⼊内存，即刻⽴刻运⾏； 这两种挂起状态加上前⾯的五种状态，就变成了七种状态变迁，⻅如下图：

![image-20211208093122504](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112080931585.png)

导致进程挂起的原因不只是因为进程所使⽤的内存空间不在物理内存，还包括如下情况：

- 通过 sleep 让进程间歇性挂起，其⼯作原理是设置⼀个定时器，到期后唤醒进程。

- ⽤户希望挂起⼀个程序的执⾏，⽐如在 Linux 中⽤ Ctrl+Z 挂起进程；

##### 4.1.1.3 进程的控制结构

​	在操作系统中，是⽤**进程控制块**（*process control block*，*PCB*）数据结构来描述进程的。

​	那 PCB 是什么呢？**PCB** **是进程存在的唯⼀标识**，这意味着⼀个进程的存在，必然会有⼀个 PCB，如果进程消失了，那么PCB 也会随之消失。

> PCB 具体包含什么信息?

**进程描述信息**：

- 进程标识符：标识各个进程，每个进程都有⼀个并且唯⼀的标识符；
- ⽤户标识符：进程归属的⽤户，⽤户标识符主要为共享和保护服务；

**进程控制和管理信息**：

- 进程当前状态，如 new、ready、running、waiting 或 blocked 等；
- 进程优先级：进程抢占 CPU 时的优先级；

**资源分配清单**：

- 有关内存地址空间或虚拟地址空间的信息，所打开⽂件的列表和所使⽤的 I/O 设备信息。

**CPU相关信息**：

- CPU 中各个寄存器的值，当进程被切换时，CPU 的状态信息都会被保存在相应的 PCB 中，以便进程

  重新执⾏时，能从断点处继续执⾏。

> 每个PCB是如何组织的？

通常是通过**链表**的⽅式进⾏组织，把具有**相同状态的进程链在⼀起，组成各种队列**。⽐如：

- 将所有处于就绪状态的进程链在⼀起，称为**就绪队列**；

- 把所有因等待某事件⽽处于等待状态的进程链在⼀起就组成各种**阻塞队列**；

- 另外，对于运⾏队列在单核 CPU 系统中则只有⼀个运⾏指针了，因为单核 CPU 在某个时间，只能运

  ⾏⼀个程序。

那么，就绪队列和阻塞队列链表的组织形式如下图：

![image-20211208095928396](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112080959657.png)

​	除了链接的组织⽅式，还有索引⽅式，它的⼯作原理：将同⼀状态的进程组织在⼀个索引表中，索引表项

指向相应的 PCB，不同状态对应不同的索引表。

​	⼀般会选择链表，因为可能⾯临进程创建，销毁等调度导致进程状态发⽣变化，所以链表能够更加灵活的

插⼊和删除。

##### 4.1.1.4 进程的控制

​	我们熟知了进程的状态变迁和进程的数据结构 PCB 后，再来看看进程的**创建、终⽌、阻塞、唤醒**的过程，

这些过程也就是进程的控制。

###### 01 创建进程

​	操作系统允许⼀个进程创建另⼀个进程，⽽且允许⼦进程继承⽗进程所拥有的资源，当⼦进程被终⽌时，

其在⽗进程处继承的资源应当还给⽗进程。同时，终⽌⽗进程时同时也会终⽌其所有的⼦进程。

​	注意：Linux 操作系统对于终⽌有⼦进程的⽗进程，会把⼦进程交给 1 号进程接管。本⽂所指出的进程终

⽌概念是宏观操作系统的⼀种观点，最后怎么实现当然是看具体的操作系统。

创建进程的过程如下：

1. 为新进程分配⼀个唯⼀的进程标识号，并申请⼀个空⽩的 PCB，PCB 是有限的，若申请失败则创建

失败

2. 为进程分配资源，此处如果资源不⾜，进程就会进⼊等待状态，以等待资源；
3. 初始化 PCB；
4. 如果进程的调度队列能够接纳新进程，那就将进程插⼊到就绪队列，等待被调度运⾏；

###### 02 终止进程

进程可以有 3 种终⽌⽅式：正常结束、异常结束以及外界⼲预（信号 kill 掉）。

终⽌进程的过程如下：

1. 查找需要终⽌的进程的 PCB；
2. 如果处于执⾏状态，则⽴即终⽌该进程的执⾏，然后将 CPU 资源分配给其他进程；
3. 如果其还有⼦进程，则应将其所有⼦进程终⽌；
4. 将该进程所拥有的全部资源都归还给⽗进程或操作系统；
5. 将其从 PCB 所在队列中删除；

###### 03 阻塞进程

​	当进程需要等待某⼀事件完成时，它可以调⽤阻塞语句把⾃⼰阻塞等待。⽽⼀旦被阻塞等待，它只能由另

⼀个进程唤醒。

阻塞进程的过程如下：

1. 找到将要被阻塞进程标识号对应的 PCB；
2. 如果该进程为运⾏状态，则保护其现场，将其状态转为阻塞状态，停⽌运⾏；
3. 将该 PCB 插⼊到阻塞队列中去；

###### 04 唤醒进程

​	进程由「运⾏」转变为「阻塞」状态是由于进程必须等待某⼀事件的完成，所以处于阻塞状态的进程是绝

对不可能叫醒⾃⼰的。

​	如果某进程正在等待 I/O 事件，需由别的进程发消息给它，则只有当该进程所期待的事件出现时，才由发

现者进程⽤唤醒语句叫醒它。

唤醒进程的过程如下：

1. 在该事件的阻塞队列中找到相应进程的 PCB；
2. 将其从阻塞队列中移出，并置其状态为就绪状态；
3. 把该 PCB 插⼊到就绪队列中，等待调度程序调度；

​	进程的阻塞和唤醒是⼀对功能相反的语句，如果某个进程调⽤了阻塞语句，则必有⼀个与之对应的唤醒语

句。



##### 4.1.1.5 进程的上下文切换

​	各个进程之间是共享 CPU 资源的，在不同的时候进程之间需要切换，让不同的进程可以在 CPU 执⾏，那

么这个**⼀个进程切换到另⼀个进程运⾏，称为进程的上下⽂切换**。

> 在详细说进程上下⽂切换前，我们先来看看 CPU 上下⽂切换

​	⼤多数操作系统都是多任务，通常⽀持⼤于 CPU 数量的任务同时运⾏。实际上，这些任务并不是同时运⾏的，只是因为系统在很短的时间内，让各个任务分别在 CPU 运⾏，于是就造成同时运⾏的错觉。

​	任务是交给 CPU 运⾏的，那么在每个任务运⾏前，CPU 需要知道任务从哪⾥加载，⼜从哪⾥开始运⾏。

所以，操作系统需要事先帮 CPU 设置好 **CPU** **寄存器和程序计数器**。

​	CPU 寄存器是 CPU 内部⼀个容量⼩，但是速度极快的内存（缓存）。我举个例⼦，寄存器像是你的⼝

袋，内存像你的书包，硬盘则是你家⾥的柜⼦，如果你的东⻄存放到⼝袋，那肯定是⽐你从书包或家⾥柜

⼦取出来要快的多。

​	程序计数器则是⽤来存储 CPU 正在执⾏的指令位置、或者即将执⾏的下⼀条指令位置。

​	所以说，CPU 寄存器和程序计数是 CPU 在运⾏任何任务前，所必须依赖的环境，这些环境就叫做 **CPU**

**上下⽂**。

​	既然知道了什么是 CPU 上下⽂，那理解 CPU 上下⽂切换就不难了。CPU 上下⽂切换就是先把前⼀个任务的 CPU 上下⽂（CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下⽂到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运⾏新任务。

​	**系统内核**会存储保持下来的上下⽂信息，当此任务再次被分配给 CPU 运⾏时，CPU 会重新加载这些上下⽂，这样就能保证任务原来的状态不受影响，让任务看起来还是连续运⾏。上⾯说到所谓的「任务」，主要包含进程、线程和中断。所以，可以根据任务的不同，把 CPU 上下⽂切换分成：**进程上下⽂切换、线程上下⽂切换和中断上下⽂切换**。

> 进程的上下⽂切换到底是切换什么呢？

​	进程是由内核管理和调度的，所以进程的切换只能发⽣在内核态。所以，**进程的上下⽂切换不仅包含了虚拟内存、栈、全局变量等⽤户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。**通常，会把交换的信息保存在进程的 PCB，当要运⾏另外⼀个进程的时候，我们需要从这个进程的 PCB取出上下⽂，然后恢复到 CPU 中，这使得这个进程可以继续执⾏，如下图所示：

![image-20211208101359534](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112081013720.png)

​	⼤家需要注意，进程的上下⽂开销是很关键的，我们希望它的开销越⼩越好，这样可以使得进程可以把更 多时间花费在执⾏程序上，⽽不是耗费在上下⽂切换。

> 发⽣进程上下⽂切换有哪些场景？

- 为了保证所有进程可以得到公平调度，CPU 时间被划分为⼀段段的时间⽚，这些时间⽚再被轮流分配 给各个进程。这样，当某个进程的时间⽚耗尽了，进程就从运⾏状态变为就绪状态，系统从就绪队列 选择另外⼀个进程运⾏；
- 进程在系统资源不⾜（⽐如内存不⾜)时，要等到资源满⾜后才可以运⾏，这个时候进程也会被挂 起，并由系统调度其他进程运⾏；
- 当进程通过睡眠函数  sleep  这样的⽅法将⾃⼰主动挂起时，⾃然也会重新调度；
- 当有优先级更⾼的进程运⾏时，为了保证⾼优先级进程的运⾏，当前进程会被挂起，由⾼优先级进程 来运⾏；
- 发⽣硬件中断时，CPU 上的进程会被中断挂起，转⽽执⾏内核中的中断服务程序； 以上，就是发⽣进程上下⽂切换的常⻅场景了。



#### 4.1.2 线程

​	在早期的操作系统中都是以进程作为独⽴运⾏的基本单位，直到后⾯，计算机科学家们⼜提出了更⼩的能 独⽴运⾏的基本单位，也就是 **线程**

##### 4.1.2.1 为什么使用线程？

​	我们举个例⼦，假设你要编写⼀个视频播放器软件，那么该软件功能的核⼼模块有三个：

- 从视频⽂件当中读取数据；

- 对读取的数据进⾏解压缩；
- 把解压缩后的视频数据播放出来；

对于单进程的实现⽅式，我想⼤家都会是以下这个⽅式：

![image-20211208101940369](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112081019589.png)

对于单进程的这种⽅式，存在以下问题：

- 播放出来的画⾯和声⾳会不连贯，因为当 CPU 能⼒不够强的时候， Read 的时候可能进程就等在这了，这样就会导致等半天才进⾏数据解压和播放；
- 各个函数之间不是并发执⾏，影响资源的使⽤效率；

那改进成多进程的⽅式：

![image-20211208102210394](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112081022519.png)

对于多进程的这种⽅式，依然会存在问题：

- 进程之间如何通信，共享数据？维护进程的系统开销较⼤，如创建进程时，分配资源、建⽴ PCB；终⽌进程时，回收资源、撤销PCB；
- 进程切换时，保存当前进程的状态信息；

那到底如何解决呢？需要有⼀种新的实体，满⾜以下特性：

- 实体之间可以并发运⾏；
- 实体之间共享相同的地址空间；

这个新的实体，就是**线程(Thread** **)**，线程之间可以并发运⾏且共享相同的地址空间。



##### 4.1.2.2 什么是线程？

> 什么是线程？

​	**线程是进程当中的⼀条执行流程。**同⼀个进程内多个线程之间可以共享代码段、数据段、打开的⽂件等资源，但每个线程各⾃都有⼀套独⽴的寄存器和栈，这样可以确保线程的控制流是相对独⽴的。

![image-20211208102559052](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112081025296.png)

> 线程的优缺点？

**线程的优点**：

- ⼀个进程中可以同时存在多个线程；
- 各个线程之间可以并发执⾏；
- 各个线程之间可以共享地址空间和⽂件等资源；

**线程的缺点**：

- 当进程中的⼀个线程崩溃时，会导致其所属进程的所有线程崩溃。

​	举个例⼦，对于游戏的⽤户设计，则不应该使⽤多线程的⽅式，否则⼀个⽤户挂了，会影响其他同个进程

的线程。

##### **4.1.2.3** 线程和进程的比较

线程与进程的⽐较如下：

- 进程是资源（包括内存、打开的⽂件等）分配的单位，线程是 CPU 调度的单位；
- 进程拥有⼀个完整的资源平台，⽽线程只独享必不可少的资源，如寄存器和栈；
- 线程同样具有就绪、阻塞、执⾏三种基本状态，同样具有状态之间的转换关系；
- 线程能减少并发执⾏的时间和空间开销；

对于，线程相⽐进程能减少开销，体现在：

- 线程的创建时间⽐进程快，因为进程在创建的过程中，还需要资源管理信息，⽐如内存管理信息、⽂

件管理信息，⽽线程在创建的过程中，不会涉及这些资源管理信息，⽽是共享它们；

- 线程的终⽌时间⽐进程快，因为线程释放的资源相⽐进程少很多；

- 同⼀个进程内的线程切换⽐进程切换快，因为线程具有相同的地址空间（虚拟内存共享），这意味着同⼀个进程的线程都具有同⼀个⻚表，那么在切换的时候不需要切换⻚表。⽽对于进程之间的切换，切换的时候要把⻚表给切换掉，⽽⻚表的切换过程开销是⽐较⼤的；

- 由于同⼀进程的各线程间共享内存和⽂件资源，那么在线程之间数据传递的时候，就不需要经过内核了，这就使得线程之间的数据交互效率更⾼了；所以，不管是时间效率，还是空间效率线程⽐进程都要⾼。



#####  4.1.2.4  线程的上下文切换

​	在前⾯我们知道了，线程与进程最⼤的区别在于：**线程是调度的基本单位，⽽进程则是资源拥有的基本单位**。所以，所谓操作系统的任务调度，实际上的调度对象是线程，⽽进程只是给线程提供了虚拟内存、全局变量等资源。对于线程和进程，我们可以这么理解：当进程只有⼀个线程时，可以认为进程就等于线程；当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源，这些资源在上下⽂切换时是不需要修改的；另外，线程也有⾃⼰的私有数据，⽐如栈和寄存器等，这些在上下⽂切换时也是需要保存的。

> 线程上下文切换的是什么？

​	这还得看线程是不是属于同⼀个进程：

- 当两个线程不是属于同⼀个进程，则切换的过程就跟进程上下⽂切换⼀样；

- **当两个线程是属于同⼀个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不**

**动，只需要切换线程的私有数据、寄存器等不共享的数据**；

所以，线程的上下⽂切换相⽐进程，开销要⼩很多。



##### **4.1.2.5** 线程的实现

主要有三种线程的实现⽅式：

- **用户线程（User Thread）**: 在⽤户空间实现的线程，不是由内核管理的线程，是由⽤户态的线程库来完成线程的管理；
- **内核线程（Kernel Thread）**：在内核中实现的线程，是由内核管理的线程；
- **轻量级进程（LightWeight Process）**: 在内核中来⽀持⽤户线程；

​	那么，这还需要考虑⼀个问题，⽤户线程和内核线程的对应关系: 

⾸先，第⼀种关系是**多对⼀**的关系，也就是多个⽤户线程对应同⼀个内核线程：

![image-20211211075902262](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110759446.png)

第⼆种是**⼀对⼀**的关系，也就是⼀个⽤户线程对应⼀个内核线程：

 ![image-20211211075935581](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110759671.png)

第三种是**多对多**的关系，也就是多个⽤户线程对应到多个内核线程：

![image-20211211080007131](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110800224.png)

> 用户线程如何理解？存在什么优势和缺陷？

​	⽤户线程是基于⽤户态的线程管理库来实现的，那么**线程控制块（Thread Control Block, TCB）**也是在 库⾥⾯来实现的，对于操作系统⽽⾔是看不到这个  TCB  的，它只能看到整个进程的  PCB。所以，**⽤户线程的整个线程管理和调度，操作系统是不直接参与的，⽽是由⽤户级线程库函数来完成线程的管理，包括线程的创建、终止、同步和调度等。**

 	⽤户级线程的模型，也就类似前⾯提到的**多对⼀**的关系，即多个⽤户线程对应同⼀个内核线程，如下图所 示：

![image-20211211080428230](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110804317.png)

⽤户线程的**优点**：

- 每个进程都需要有它私有的线程控制块（TCB）列表，⽤来跟踪记录它各个线程状态信息（PC、栈指 针、寄存器)，TCB  由⽤户级线程库函数来维护，可⽤于不⽀持线程技术的操作系统；

- ⽤户线程的切换也是由线程库函数来完成的，⽆需⽤户态与内核态的切换，所以速度特别快；

⽤户线程的**缺点**：

- 由于操作系统不参与线程的调度，如果⼀个线程发起了系统调⽤⽽阻塞，那进程所包含的⽤户线程都不能执⾏了。

- 当⼀个线程开始运⾏后，除⾮它主动地交出 CPU 的使⽤权，否则它所在的进程当中的其他线程⽆法 运⾏，因为⽤户态的线程没法打断当前运⾏中的线程，它没有这个特权，只有操作系统才有，但是⽤ 户线程不是由操作系统管理的。

- 由于时间⽚分配给进程，故与其他进程⽐，在多线程执⾏时，每个线程得到的时间⽚较少，执⾏会⽐ 较慢；

 以上，就是⽤户线程的优缺点了。

> 那内核线程如何理解？存在什么优势和缺陷？

​	**内核线程是由操作系统管理的，线程对应的TCB⾃然是放在操作系统⾥的，这样线程的创建、终⽌和管理都是由操作系统负责。**内核线程的模型，也就类似前⾯提到的**⼀对⼀**的关系，即⼀个⽤户线程对应⼀个内核线程，如下图所示：

![image-20211211080931224](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110809311.png)

内核线程的优点：

- 在一个进程当中，如果某个内核线程发起系统调⽤⽽被阻塞，并不会影响其他内核线程的运⾏；
- 分配给线程，多线程的进程获得更多的 CPU 运⾏时间； 

内核线程的缺点：

- 在支持内核线程的操作系统中，由内核来维护进程和线程的上下⽂信息，如  PCB  和  TCB；
- 线程的创建、终⽌和切换都是通过系统调⽤的⽅式来进⾏，因此对于系统来说，系统开销⽐较⼤； 以上，就是内核线程的优缺点了。

> 最后的轻量级进程如何理解？

​	**轻量级进程（Light-weight process，LWP）是内核⽀持的⽤户线程，⼀个进程可有⼀个或多个 LWP，每 个  LWP  是跟内核线程⼀对⼀映射的，也就是  LWP 都是由⼀个内核线程⽀持。**

​	另外，LWP  只能由内核管理并像普通进程⼀样被调度，Linux  内核是⽀持  LWP 的典型例⼦。在⼤多数系统中，LWP与普通进程的区别也在于它只有⼀个最⼩的执⾏上下⽂和调度程序所需的统计信 息。⼀般来说，⼀个进程代表程序的⼀个实例，⽽ LWP 代表程序的执⾏线程，因为⼀个执⾏线程不像进程 那样需要那么多状态信息，所以  LWP  也不带有这样的信息。

​	在  LWP  之上也是可以使⽤⽤户线程的，那么  LWP 与⽤户线程的对应关系就有三种：

- `1:1`: 即⼀个 LWP 对应  ⼀个⽤户线程；
- `N:1`: 即⼀个  LWP 对应多个⽤户线程；
- `M:N`: 即多个 LMP 对应多个⽤户线程；

接下来针对上⾯这三种对应关系说明它们优缺点。先看下图的LWP模型：

![image-20211211081629453](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110816548.png)

**1：1模式**

⼀个线程对应到⼀个  LWP  再对应到⼀个内核线程，如上图的进程 4，属于此模型。

- **优点**： 实现并⾏，当⼀个  LWP  阻塞，不会影响其他 LWP；
- **缺点**： 每⼀个⽤户线程，就产⽣⼀个内核线程，创建线程的开销较⼤。



**N:1模式**

​	多个⽤户线程对应⼀个 LWP 再对应⼀个内核线程，如上图的进程 2，线程管理是在⽤户空间完成的，此模 式中⽤户的线程对操作系统不可⻅。

- 优点：⽤户线程要开⼏个都没问题，且上下⽂切换发⽣⽤户空间，切换的效率较⾼；

- 缺点: ⼀个⽤户线程如果阻塞了，则整个进程都将会阻塞，另外在多核  CPU   中，是没办法充分利⽤

  CPU 的。

**M:N模式**

​	根据前⾯的两个模型混搭⼀起，就形成M:N模型，该模型提供了两级控制，⾸先多个⽤户线程对应到多

个  LWP，LWP  再⼀⼀对应到内核线程，如上图的进程 3。

- **优点**： 综合了前两种优点，⼤部分的线程上下⽂发⽣在⽤户空间，且多个线程⼜可以充分利⽤多核

  CPU 的资源。

**组合模式**

​	如上图的进程  5，此进程结合`1：1`模型和`M:N`模型。开发⼈员可以针对不同的应⽤特点调节内核线程的数⽬来达到物理并⾏性和逻辑并⾏性的最佳⽅案。



#### 4.1.3 调度

​	进程都希望⾃⼰能够占⽤  CPU 进⾏⼯作，那么这涉及到前⾯说过的进程上下⽂切换。⼀旦操作系统把进程切换到运⾏状态，也就意味着该进程占⽤着 CPU 在执⾏，但是当操作系统把进程切换到其他状态时，那就不能在  CPU 中执⾏了，于是操作系统会选择下⼀个要运⾏的进程。选择⼀个进程运⾏这⼀功能是在操作系统中完成的，通常称为**调度程序（scheduler）**。 那到底什么时候调度进程，或以什么原则来调度进程呢？

##### 调度时机

​	在进程的⽣命周期中，当进程从⼀个运⾏状态到另外⼀状态变化的时候，其实会触发⼀次调度。⽐如，以下状态的变化都会触发操作系统的调度：

- **就绪态 -> 运行态**：当进程被创建时，会进⼊到就绪队列，操作系统会从就绪队列选择⼀个进程运⾏；
- **运行态 -> 阻塞态**：当进程发⽣  I/O 事件⽽阻塞时，操作系统必须另外⼀个进程运⾏；
- **运行态 -> 结束态**：当进程退出结束后，操作系统得从就绪队列选择另外⼀个进程运⾏；

​	因为，这些状态变化的时候，操作系统需要考虑是否要让新的进程给  CPU  运⾏，或者是否让当前进程从CPU  上退出来⽽换另⼀个进程运⾏。另外，如果硬件时钟提供某个频率的周期性中断，那么可以根据如何处理时钟中断，把调度算法分为两类：

- **非抢占式调度算法**： 挑选⼀个进程，然后让该进程运⾏直到被阻塞，或者直到该进程退出，才会调⽤另 外⼀个进程，也就是说不会理时钟中断这个事情。
- **抢占式调度算法**： 挑选⼀个进程，然后让该进程只运⾏某段时间，如果在该时段结束时，该进程仍然在 运⾏时，则会把它挂起，接着调度程序从就绪队列挑选另外⼀个进程。这种抢占式调度处理，需要在 时间间隔的末端发⽣时钟中断，以便把 CPU 控制返回给调度程序进⾏调度，也就是常说的时间⽚机 制。



##### 调度原则

1. 如果运⾏的程序，发⽣了 I/O 事件的请求，那 CPU 使⽤率必然会很低，因为此时进程在阻塞等待硬盘的数据返回。这样的过程，势必会造成 CPU 突然的空闲。所以，**为了提⾼ CPU 利⽤率，在这种发送 I/O  事件致使  CPU  空闲的情况下，调度程序需要从就绪队列中选择⼀个进程来运⾏。**

2. 有的程序执⾏某个任务花费的时间会⽐较⻓，如果这个程序⼀直占⽤着   CPU，会造成系统吞吐量（CPU 在单位时间内完成的进程数量）的降低。所以，**要提⾼系统的吞吐率，调度程序要权衡⻓任务和短 任务进程的运⾏完成数量。**

3. 从进程开始到结束的过程中，实际上是包含两个时间，分别是进程运⾏时间和进程等待时间，这 两个时间总和就称为**周转时间**。进程的周转时间越⼩越好，**如果进程的等待时间很⻓⽽运⾏时间很短，那 周转时间就很⻓，这不是我们所期望的，调度程序应该避免这种情况发⽣。**

4. 处于就绪队列的进程，也不能等太久，当然希望这个等待的时间越短越好，这样可以使得进程更 快的在  CPU 中执⾏。所以，**就绪队列中进程的等待时间也是调度程序所需要考虑的原则。**

5. 对于⿏标、键盘这种交互式⽐较强的应⽤，我们当然希望它的响应时间越快越好，否则就会影响

   ⽤户体验了。所以，**对于交互式⽐较强的应⽤，响应时间也是调度程序需要考虑的原则**。

    

![image-20211211091034968](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110910064.png)

针对上⾯的五种调度原则，总结成如下：

- **CPU利用率**：调度程序应确保  CPU 是始终匆忙的状态，这可提⾼  CPU 的利⽤率；
- **系统吞吐量**：吞吐量表示的是单位时间内 CPU 完成进程的数量，⻓作业的进程会占⽤较⻓的 CPU 资 源，因此会降低吞吐量，相反，短作业的进程会提升系统吞吐量；
- **周转时间**：周转时间是进程运⾏和阻塞时间总和，⼀个进程的周转时间越⼩越好；
- **等待时间**：这个等待时间不是阻塞状态的时间，⽽是进程处于就绪队列的时间，等待的时间越⻓，⽤ 户越不满意；
- **响应时间**： ⽤户提交请求到系统第⼀次产⽣响应所花费的时间，在交互式系统中，响应时间是衡量调 度算法好坏的主要标准。



##### 调度算法

​	不同的调度算法适⽤的场景也是不同的。接下来，说说在**单核  CPU 系统**中常⻅的调度算法。

###### 01 先来先服务调度算法

![image-20211211091422934](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110914656.png)

​	最简单的⼀个调度算法，就是⾮抢占式的先来先服务（First  Come  First  Seved, FCFS）算法了。顾名思义，先来后到，**每次从就绪队列选择最先进⼊队列的进程，然后⼀直运⾏，直到进程退出或被阻 塞，才会继续从队列中选择第⼀个进程接着运⾏**。这似乎很公平，但是当⼀个⻓作业先运⾏了，那么后⾯的短作业等待的时间就会很⻓，不利于短作业。FCFS 对⻓作业有利，适⽤于  CPU 繁忙型作业的系统，⽽不适⽤于  I/O 繁忙型作业的系统。

###### 02 最短作业优先调度算法

![image-20211211091547854](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110915922.png)



​	**最短作业优先（Shortest Job First, SJF）调度算法**，同样也是顾名思义，它会优先选择运⾏时间最短的进 程来运⾏，这有助于提⾼系统的吞吐量。这显然对⻓作业不利，很容易造成⼀种极端现象。⽐如，⼀个⻓作业在就绪队列等待运⾏，⽽这个就绪队列有⾮常多的短作业，那么就会使得⻓作业不断的 往后推，周转时间变⻓，致使⻓作业⻓期不会被运⾏。

###### 03 高响应比优先调度算法

​	前⾯的「先来先服务调度算法」和「最短作业优先调度算法」都没有很好的权衡短作业和⻓作业。那么，**⾼响应⽐优先（Highest  Response  Ratio  Next, HRRN）调度算法**主要是权衡了短作业和⻓作业。**每次进⾏进程调度时，先计算「响应⽐优先级」，然后把「响应⽐优先级」最⾼的进程投⼊运⾏**。

>  响应⽐优先级」的计算公式：

![image-20211211091755529](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110917606.png)

从上⾯的公式，可以发现：

- 如果两个进程的「等待时间」相同时，「要求的服务时间」越短，「响应⽐」就越⾼，这样短作业的 进程容易被选中运⾏；
- 如果两个进程「要求的服务时间」相同时，「等待时间」越⻓，「响应⽐」就越⾼，这就兼顾到了⻓ 作业进程，因为进程的响应⽐可以随时间等待的增加⽽提⾼，当其等待时间⾜够⻓时，其响应⽐便可 以升到很⾼，从⽽获得运⾏的机会；

###### 04 时间轮转调度算法

![image-20211211091933872](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110919945.png)

**时间⽚轮转（Round  Robin,  RR）调度算法**：**每个进程被分配⼀个时间段，称为时间⽚（Quantum），即允许该进程在该时间段中运⾏。**

- 如果时间⽚⽤完，进程还在运⾏，那么将会把此进程从  CPU  释放出来，并把  CPU 分配给另外⼀个进程；
- 如果该进程在时间⽚结束前阻塞或结束，则 CPU ⽴即进⾏切换； 另外，时间⽚的⻓度就是⼀个很关键的点：
- 如果时间⽚设得太短会导致过多的进程上下⽂切换，降低了  CPU  效率；
- 如果设得太⻓⼜可能引起对短作业进程的响应时间变⻓。

一般来说，时间片设为20ms~50ms 通常是一个比较合理的折中值。

###### 05 最⾼优先级调度算法

​	前⾯的「时间⽚轮转算法」做了个假设，即让所有的进程同等重要，也不偏袒谁，⼤家的运⾏时间都⼀  样。但是，对于多⽤户计算机系统就有不同的看法了，它们希望调度是有优先级的，即希望调度程序能从就绪 队列中选择最⾼优先级的进程进⾏运⾏，这称为**最⾼优先级（Highest  Priority  First，HPF）调度算法**。

进程的优先级可以分为，静态优先级和动态优先级：

- **静态优先级**：创建进程时候，就已经确定了优先级了，然后整个运⾏时间优先级都不会变化；
- **动态优先级**： 根据进程的动态变化调整优先级，⽐如如果进程运⾏时间增加，则降低其优先级，如果 进程等待时间（就绪队列的等待时间）增加，则升⾼其优先级，也就是随着时间的推移增加等待进程 的优先级。

该算法也有两种处理优先级⾼的⽅法，⾮抢占式和抢占式：

- **非抢占式**：当就绪队列中出现优先级⾼的进程，运⾏完当前进程，再选择优先级⾼的进程。
- **抢占式**： 当就绪队列中出现优先级⾼的进程，当前进程挂起，调度优先级⾼的进程运⾏。 但是依然有缺点，可能会导致低优先级的进程永远不会运⾏。

###### 06 多级反馈队列调度算法

​	**多级反馈队列（Multilevel Feedback Queue）调度算法**是「时间⽚轮转算法」和「最⾼优先级算法」的 综合和发展。顾名思义：

- **多级**：表示有多个队列，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越短。
- **反馈**： 表示如果有新的进程加⼊优先级⾼的队列时，⽴刻停⽌当前正在运⾏的进程，转⽽去运⾏优 先级⾼的队列；

![image-20211211092956540](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110929622.png)

来看看，它是如何⼯作的：

- 设置了多个队列，赋予每个队列不同的优先级，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越 短；
- 新的进程会被放⼊到第⼀级队列的末尾，按先来先服务的原则排队等待被调度，如果在第⼀级队列规 定的时间⽚没运⾏完成，则将其转⼊到第⼆级队列的末尾，以此类推，直⾄完成；
- 当较⾼优先级的队列为空，才调度较低优先级的队列中的进程运⾏。如果进程运⾏时，有新进程进⼊ 较⾼优先级的队列，则停⽌当前运⾏的进程并将其移⼊到原队列末尾，接着让较⾼优先级的进程运⾏；

​	可以发现，对于短作业可能可以在第⼀级队列很快被处理完。对于⻓作业，如果在第⼀级队列处理不完， 可以移⼊下次队列等待被执⾏，虽然等待的时间变⻓了，但是运⾏时间也变更⻓了，所以该算法很好的**兼 顾了⻓短作业，同时有较好的响应时间**。

###### 举例说明

​	**办理业务的客户相当于进程，银⾏窗⼝⼯作⼈员相当于  CPU。**现在，假设这个银⾏只有⼀个窗⼝（单核  CPU ），那么⼯作⼈员⼀次只能处理⼀个业务。

![image-20211211093433092](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110934177.png)

​	那么最简单的处理⽅式，就是先来的先处理，后⾯来的就乖乖排队，这就是**先来先服务（FCFS）调度算 法**。但是万⼀先来的这位⽼哥是来贷款的，这⼀谈就好⼏个⼩时，⼀直占⽤着窗⼝，这样后⾯的⼈只能⼲ 等，或许后⾯的⼈只是想简单的取个钱，⼏分钟就能搞定，却因为前⾯⽼哥办⻓业务⽽要等⼏个⼩时，你 说⽓不⽓⼈？

 ![image-20211211093520063](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110935154.png)

​	有客户抱怨了，那我们就要改进，我们⼲脆优先给那些⼏分钟就能搞定的⼈办理业务，这就是**短作业优先（SJF）调度算法**。听起来不错，但是依然还是有个极端情况，万⼀办理短业务的⼈⾮常的多，这会导致⻓业务的⼈⼀直得不到服务，万⼀这个⻓业务是个⼤客户，那不就捡了芝麻丢了⻄⽠。

![image-20211211093634049](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110936141.png)

​	那就公平起⻅，现在窗⼝⼯作⼈员规定，每个⼈我只处理  10  分钟。如果  10 分钟之内处理完，就⻢上换下⼀个⼈。如果没处理完，依然换下⼀个⼈，但是客户⾃⼰得记住办理到哪个步骤了。这个也就是**时间⽚轮 转（RR）调度算法**。但是如果时间⽚设置过短，那么就会造成⼤量的上下⽂切换，增⼤了系统开销。如果 时间⽚过⻓，相当于退化成  FCFS 算法了。

![image-20211211093723530](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110937613.png)

​	既然公平也可能存在问题，那银⾏就对客户分等级，分为普通客户、VIP 客户、SVIP 客户。只要⾼优先级 的客户⼀来，就第⼀时间处理这个客户，这就是**最⾼优先级（HPF）调度算法**。但依然也会有极端的问 题，万⼀当天来的全是⾼级客户，那普通客户不是没有被服务的机会，不把普通客户当⼈是吗？那我们把 优先级改成动态的，如果客户办理业务时间增加，则降低其优先级，如果客户等待时间增加，则升⾼其优先级。

![image-20211211093759666](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110937747.png)

​	那有没有兼顾到公平和效率的⽅式呢？这⾥介绍⼀种算法，考虑的还算充分的，多级反馈队列（MFQ）调 度算法，它是时间⽚轮转算法和优先级算法的综合和发展。它的⼯作⽅式：

![image-20211211093831388](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110938472.png)

- 银行设置了多个排队（就绪）队列，每个队列都有不同的优先级，各个队列优先级从⾼到低，同时每 个队列执⾏时间⽚的⻓度也不同，优先级越⾼的时间⽚越短。
- 新客户（进程）来了，先进⼊第⼀级队列的末尾，按先来先服务原则排队等待被叫号（运⾏）。如果 时间⽚⽤完客户的业务还没办理完成，则让客户进⼊到下⼀级队列的末尾，以此类推，直⾄客户业务 办理完成。
- 当第⼀级队列没⼈排队时，就会叫号⼆级队列的客户。如果客户办理业务过程中，有新的客户加⼊到 较⾼优先级的队列，那么此时办理中的客户需要停⽌办理，回到原队列的末尾等待再次叫号，因为要 把窗⼝让给刚进⼊较⾼优先级队列的客户。

​	可以发现，对于要办理短业务的客户来说，可以很快的轮到并解决。对于要办理⻓业务的客户，⼀下⼦解 决不了，就可以放到下⼀个队列，虽然等待的时间稍微变⻓了，但是轮到⾃⼰的办理时间也变⻓了，也可 以接受，不会造成极端的现象，可以说是综合上⾯⼏种算法的优点。



### 4.2 进程间通信

![image-20211211094601116](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112110946742.png)

​	每个进程的⽤户地址空间都是独⽴的，⼀般⽽⾔是不能互相访问的，但内核空间是每个进程都共享的，所以进程之间要通信必须通过内核。Linux  内核提供了不少进程间通信的机制，我们来⼀起瞧瞧有哪些？

#### 4.2.1 管道

​	如果你学过 Linux 命令，那你肯定很熟悉「` | `」这个竖线。

```shell
$ ps auxf | grep mysql
```

​	上⾯命令⾏⾥的「 | 」竖线就是⼀个**管道**，它的功能是将前⼀个命令（ ps auxf ）的输出，作为后⼀个命令（ grep mysql ）的输⼊，从这功能描述，可以看出**管道传输数据是单向的**，如果想相互通信，我们需要创建两个管道才⾏。

​	同时，我们得知上⾯这种管道是没有名字，所以「 | 」表示的管道称为**匿名管道**，⽤完了就销毁。

管道还有另外⼀个类型是**命名管道**，也被叫做 FIFO ，因为数据是先进先出的传输⽅式。

在使⽤命名管道前，先需要通过 mkfifo 命令来创建，并且指定管道名字：

```shell
$ mkfifo myPipe
```

​	myPipe  就是这个管道的名称，基于  Linux  ⼀切皆⽂件的理念，所以管道也是以⽂件的⽅式存在，我们可以⽤  ls 看⼀下，这个⽂件的类型是  p，也就是  pipe（管道） 的意思：

```shell
# 查看管道文件
$ ls -l
prw-r--r--. 1 root	root	0 Jul 17 02:45 myPipe
```

​	接下来，我们往  myPipe 这个管道写⼊数据：

```shell
$ echo "hello" > myPipe	// 将数据写进管道
						// 停住了  ...
```

​	你操作了后，你会发现命令执⾏后就停在这了，这是因为管道⾥的内容没有被读取，只有当管道⾥的数据 被读完后，命令才可以正常退出。

​	于是，我们执⾏另外⼀个命令来读取这个管道⾥的数据：

```shell
$ cat < myPipe	// 读取管道⾥的数据
hello
```

​	可以看到，管道⾥的内容被读取出来了，并打印在了终端上，另外⼀⽅⾯，echo   那个命令也正常退出了。我们可以看出，管道这种通信⽅式效率低，不适合进程间频繁地交换数据。当然，它的好处，⾃然就是简 单，同时也我们很容易得知管道⾥的数据已经被另⼀个进程读取了。



#### 4.2.2 消息队列



#### 4.2.3 共享内存



#### 4.2.4 信息量



#### 4.2.5 信号



#### 4.2.6 Socket



### 4.3 多线程同步

#### 4.3.1 竞争与协作

##### 4.3.1.1 互斥的概念

##### 4.3.1.2 同步的概念

#### 4.3.2 互斥与同步的实现

##### 4.3.2.1 锁

##### 4.3.2.2 信息量

##### 4.3.2.3 生产者-消费者问题

#### 4.3.3 经典同步问题

##### 4.3.3.1 哲学假就餐问题

##### 4.3.3.2 读者-写者问题

### 4.4 死锁

#### 4.4.1 死锁的概念

#### 4.4.2 模拟死锁问题的产生

#### 4.4.3 利用工具排查死锁问题

#### 4.4.4 避免死锁问题的发生

#### 4.4.5 总结

​	简单来说，死锁问题的产⽣是由两个或者以上线程并⾏执⾏的时候，争夺资源⽽互相等待造成的。死锁只有同时满⾜**互斥**、**持有并等待**、**不可剥夺**、**环路**等待这四个条件的时候才会发⽣。所以要避免死锁问题，就是要破坏其中⼀个条件即可，最常⽤的⽅法就是使⽤**资源有序分配法**来破坏环路等待条件。

### 4.5 悲观所与乐观锁







## 五、调度算法

### 5.1 进程调度算法

### 5.2 页面置换算法

### 5.3 磁盘调度算法





## 六、文件系统



### 6.1 文件系统的组成

​	⽂件系统是操作系统中负责管理持久数据的⼦系统，说简单点，就是负责把⽤户的⽂件存到磁盘硬件中， 因为即使计算机断电了，磁盘⾥的数据并不会丢失，所以可以持久化的保存⽂件。⽂件系统的基本数据单位是⽂件，它的⽬的是对磁盘上的⽂件进⾏组织管理，那组织的⽅式不同，就会形 成不同的⽂件系统。

​	Linux 最经典的⼀句话是：「**⼀切皆⽂件**」，不仅普通的⽂件和⽬录，就连块设备、管道、socket 等，也 都是统⼀交给⽂件系统管理的。Linux ⽂件系统会为每个⽂件分配两个数据结构：**索引节点（index node）**和**⽬录项（directory entry）**，它们主要⽤来记录⽂件的元信息和⽬录层次结构。

- **索引节点**：也就是 inode，⽤来记录⽂件的元信息，⽐如 inode 编号、⽂件⼤⼩、访问权限、创建时 间、修改时间、数据在磁盘的位置等等。索引节点是⽂件的唯⼀标识，它们之间⼀⼀对应，也同样都会被存储在硬盘中，所以索引节点同样占⽤磁盘空间。
- **⽬录项**：也就是 dentry，⽤来记录⽂件的名字、索引节点指针以及与其他⽬录项的层级关联关系。多 个⽬录项关联起来，就会形成⽬录结构，但它与索引节点不同的是，⽬录项是由内核维护的⼀个数据 结构，不存放于磁盘，⽽是缓存在内存。

​	由于索引节点唯⼀标识⼀个⽂件，⽽⽬录项记录着⽂件的名，所以⽬录项和索引节点的关系是多对⼀，也 就是说，⼀个⽂件可以有多个别字。⽐如，硬链接的实现就是多个⽬录项中的索引节点指向同⼀个⽂件。注意，⽬录也是⽂件，也是⽤索引节点唯⼀标识，和普通⽂件不同的是，普通⽂件在磁盘⾥⾯保存的是⽂件数据，⽽⽬录⽂件在磁盘⾥⾯保存⼦⽬录或⽂件。

> 目录项和目录是一个东西吗？

​	虽然名字很相近，但是它们不是⼀个东⻄，⽬录是个⽂件，持久化存储在磁盘，⽽⽬录项是内核⼀个数据 结构，缓存在内存。如果查询⽬录频繁从磁盘读，效率会很低，所以内核会把已经读过的⽬录⽤⽬录项这个数据结构缓存在内 存，下次再次读到相同的⽬录时，只需从内存读就可以，⼤⼤提⾼了⽂件系统的效率。注意，⽬录项这个数据结构不只是表示⽬录，也是可以表示⽂件的。

> 那文件数据是如何存储在磁盘的呢？

​	磁盘读写的最小单位是扇区，扇区的大小只有512B大小，很明显，如果每次读写都以这么小的单位，那这读写的效率会非常低。所以，文件系统把多个扇区组成一个逻辑块，每次读写的最小单位就是逻辑块（数据块），Linux中的逻辑块大小`4KB`，也就是一次性读写8个扇区，这将大大提高了磁盘的读写的效率。

![image-20211210161113136](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101611234.png)

​	索引节点是存储在硬盘上的数据，那么为了加速⽂件的访问，通常会把索引节点加载到内存中。另外，磁盘进⾏格式化的时候，会被分成三个存储区域，分别是超级块、索引节点区和数据块区。

- **超级块**：用来存储文件系统的详细信息，比如块个数、块大小、空闲块等等。
- **索引节点区**：用来存储索引节点；
- **数据块区**：用来存储文件或目录数据

​	我们不可能把超级块和索引节点区全部加载到内存，这样内存肯定撑不住，所以只有当需要使⽤的时候， 才将其加载进内存，它们加载进内存的时机是不同的：

- **超级块**： 当文件系统挂载时进入内存
- **索引节点区**： 当文件被访问时进入内存



### 6.2 虚拟文件系统

​	⽂件系统的种类众多，⽽操作系统希望**对⽤户提供⼀个统⼀的接⼝**，于是在⽤户层与⽂件系统层引⼊了中 间层，这个中间层就称为虚拟⽂件系统（`Virtual  File System，VFS`）。VFS 定义了⼀组所有⽂件系统都⽀持的数据结构和标准接⼝，这样程序员不需要了解⽂件系统的⼯作原 理，只需要了解  VFS 提供的统⼀接⼝即可。

> 图：在 Linux ⽂件系统中，⽤户空间、系统调⽤、虚拟机⽂件系统、缓存、⽂件系统以及存储之间的关系 

![image-20211210161917723](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101619826.png)



Linux  ⽀持的⽂件系统也不少，根据存储位置的不同，可以把⽂件系统分为三类：

- **磁盘的文件系统**： 直接把数据存储在磁盘，⽐如  Ext  2/3/4、XFS 等都是这类⽂件系统。
- **内存的文件系统**： 这类⽂件系统的数据不是存储在硬盘的，⽽是占⽤内存空间，我们经常⽤到的`/proc`和`/sys`文件系统都属于这一类，读写这类文件，实际上是读写内核中相关的数据。
- **网络的文件系统**： 用来访问其他计算机主机数据的文件系统，比如NFS、SMB等等。

​	⽂件系统⾸先要先挂载到某个⽬录才可以正常使⽤，⽐如 Linux 系统在启动时，会把⽂件系统挂载到根⽬ 录。

### 6.3 文件的使用

​	我们从⽤户⻆度来看⽂件的话，就是我们要怎么使⽤⽂件？⾸先，我们得通过系统调⽤来打开⼀个⽂件。

![image-20211210162357681](https://gitee.com/ljcdzh/my_pic/raw/master/img/202112101623771.png)

```shell
fd = open(name, flag); #　打开文件
...
write(fd , ...) # 写数据
...
close(fd); # 关闭文件
```

上⾯简单的代码是读取⼀个⽂件的过程：

1. 首先用`open`系统调用打开文件，`open`的参数中包含文件的路径名和文件名
2. 使用`write`写数据，其中`write`使用`open`所返回的 **文件描述符**，并不使用文件名做参数
3. 使用完文件后，要用`close`系统调用关闭文件，避免资源的泄露。









### 6.4 文件存储



#### 6.4.1 连续空间存储



#### 6.4.2 非连续空间存储



### 6.5 空闲空间管理





#### 6.5.1 空闲表法



#### 6.5.2 空闲链表法



### 6.6 文件系统结构



### 6.7 目录的存储



#### 6.7.1 列表



#### 6.7.2 哈希表



### 6.8 软链接和硬链接



### 6.9 文件I/O



#### 6.9.1 缓冲与非缓冲I/O



#### 6.9.2 直接与非直接I/O



#### 6.9.3 阻塞与非阻塞IO VS 异步与同步I/O









## 七、设备管理

### 7.1 设备控制器

### 7.2 I/O控制方式

### 7.3 设备驱动程序

### 7.4 通用块层

### 7.5 储系统I/O软件分层

### 7.6 键盘敲入字母时，期间发生了什么？

## 八、网络系统

### 8.1 Linux系统时如何发网络包的？

#### 8.1.1 网络模型

#### 8.1.2 Linux网络协议栈

#### 8.1.3 Linux接收网络包流程

#### 8.1.4 Linux发送网络包流程

#### 8.1.5 总结

### 8.2 零拷贝

#### 8.2.1 为什么要有DMA技术？

#### 8.2.2 传统的文件传输有多糟糕？

#### 8.2.3 如何优化文件传输的性能？

#### 8.2.4 如何实现零拷贝？

#### 8.2.5 PageCache有什么作用？

#### 8.2.6 大文件传输用什么方式实现？

### 8.3 I/O多路复用：select/poll/epoll

#### 8.3.1 最基本的Socket模型

#### 8.3.2 如何服务更多的用户

#### 8.3.3 多进程模型

#### 8.3.4 多线程模型

#### 8.3.5 I/O多路复用

#### 8.3.6 select/poll

#### 8.3.7epoll

#### 8.3.8 总结

### 8.4 高性能网络模式：Reactor和Proactor

#### 8.4.1 演进

#### 8.4.2 Reactor

#### 8.4.3 Proactor

#### 8.4.4 总结

## 九、Linux命令

### 9.1如何查看网络的性能指标？

#### 9.1.1 性能指标有哪些？

#### 9.1.2 网络配置如何看？

#### 9.1.3 socket 信息如何查看？

#### 9.1.4 网络吞吐率和PPS如何查看？

#### 9.1.5 连通性和延时如何查看？

### 9.2 如何从日志分析PV、UV？

#### 9.2.1 别急着开始

#### 9.2.2 慎用cat

#### 9.2.3 PV分析

#### 9.2.4 PV分组

#### 9.2.5 UV分析

#### 9.2.6 UV分组

#### 9.2.7 终端分析

#### 9.2.8 分析TOP3的请求

